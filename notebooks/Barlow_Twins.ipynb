{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d4f8add",
   "metadata": {},
   "source": [
    "# BARLOW TWINS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b50098",
   "metadata": {},
   "source": [
    "## SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1245ff51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/zano/Documents/TESI/TESI/notebooks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zano/Documents/TESI/TESI/venv/lib/python3.12/site-packages/IPython/core/magics/osm.py:393: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n",
      "/home/zano/Documents/TESI/TESI/venv/lib/python3.12/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd ~/Documents/TESI/TESI/notebooks\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4afb9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu124\n",
      "1.4.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import tifffile\n",
    "import glob\n",
    "import random\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "print(torch.__version__)\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from configs.ConfigLoader import ConfigLoader\n",
    "from utils.train_functions import (\n",
    "# train_epoch,\n",
    "# val_epoch,\n",
    "# print_model_summary,\n",
    "# plot_cv_results,\n",
    "# train_epoch_mixUp,\n",
    "# print_layers,\n",
    "# oversample_minority,\n",
    "# undersample_majority,\n",
    "# freeze_layers_up_to,\n",
    "# freeze_layers_up_to_progressive_ft,\n",
    "train_epoch_vit,\n",
    "val_epoch_vit,\n",
    ")\n",
    "\n",
    "import utils.transformations_functions as tf\n",
    "# Removed redundant import: from configs.ConfigLoader import ConfigLoader\n",
    "\n",
    "from classes.ModelManager import ModelManager\n",
    "import monai\n",
    "print(monai.__version__)\n",
    "#import tifffile\n",
    "#from monai.networks.nets import DenseNet121\n",
    "# import torch.nn.functional as F\n",
    "# from monai.visualize.class_activation_maps import GradCAMpp,GradCAM  \n",
    "#kaggle = input(\"Are you on Kaggle? Enter 'T' for True or 'F' for False:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88eb2862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment settings: {'gdrive': False, 'linux': True, 'kaggle': False, 'ssl': True}\n"
     ]
    }
   ],
   "source": [
    "from utils.setup_functions import set_environment_flags\n",
    "# Example usage:\n",
    "environment_flags = set_environment_flags()\n",
    "kaggle,gdrive,linux = environment_flags[\"kaggle\"], environment_flags[\"gdrive\"], environment_flags[\"linux\"]\n",
    "from utils.reproducibility_functions import set_global_seed\n",
    "set_global_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ee332c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you are on linux\n",
      "Linux detected, setting tracking URI\n",
      "Final Tracking URI: /home/zano/Documents/TESI/mlruns\n",
      "Does the directory exist? True\n"
     ]
    }
   ],
   "source": [
    "# start mlflow ui\n",
    "from utils.mlflow_functions import *\n",
    "from utils.directory_functions import *\n",
    "\n",
    "tracking_uri = get_tracking_uri(gdrive,kaggle,linux)\n",
    "mlflow.set_tracking_uri(tracking_uri)\n",
    "start_mlflow_ui(tracking_uri) # start mlflow ui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7beaba6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 channels input\n",
      "you are in linux\n",
      "/home/zano/Documents/TESI/3c_MIP_new\n"
     ]
    }
   ],
   "source": [
    "num_input_channels = int(input(\"Enter the number of input channels (3 or 4): \"))\n",
    "from utils.directory_functions import get_data_and_base_directory\n",
    "data_dir, base_dir = get_data_and_base_directory(environment_flags[\"kaggle\"], environment_flags[\"gdrive\"], environment_flags[\"linux\"], num_input_channels=num_input_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc41c7cf",
   "metadata": {},
   "source": [
    "# DATA EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3924a547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1a5bba25ef14f599c8d7e437fe4c88b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Class Set:', index=1, options=('MSA vs Control', 'MSA vs PD', 'MSA-P vs MSA-C', 'MSA-P vâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "CLASS_NAME_SETS = {\n",
    "    \"MSA vs Control\": [\"MSA\", \"control\"],\n",
    "    \"MSA vs PD\": [\"MSA\", \"PD\"],\n",
    "    \"MSA-P vs MSA-C\": [\"MSA-P\", \"MSA-C\"],\n",
    "    \"MSA-P vs PD\": [\"MSA-P\", \"PD\"],\n",
    "    \"PD vs MSA-P vs MSA-C\": [\"PD\", \"MSA-P\", \"MSA-C\"]\n",
    "}\n",
    "\n",
    "dropdown = widgets.Dropdown(\n",
    "    options=list(CLASS_NAME_SETS.keys()),\n",
    "    value=\"MSA vs PD\",\n",
    "    description='Class Set:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "def on_dropdown_change(change):\n",
    "    \"\"\"\n",
    "    Update the class_names variable when the dropdown selection changes.\n",
    "    \"\"\"\n",
    "    global class_names\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        class_names = CLASS_NAME_SETS[change['new']]\n",
    "        print(f\"class_names set to: {class_names}\")\n",
    "\n",
    "\n",
    "class_names = CLASS_NAME_SETS[dropdown.value]\n",
    "\n",
    "dropdown.observe(on_dropdown_change)\n",
    "\n",
    "display(dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7269b40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in /home/zano/Documents/TESI/3c_MIP_new/ALL: 152\n",
      "Number of images in ALL folder: 152\n"
     ]
    }
   ],
   "source": [
    "## Paths of ALL images into a numpy array without labels used for SSL\n",
    "def from_tif_folder_to_np_paths_array(folder_path: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Load all .tif images from a folder into a numpy array.\n",
    "    \"\"\"\n",
    "    image_paths = glob.glob(os.path.join(folder_path, \"*.tif\"))\n",
    "    image_paths_np = np.array(image_paths)\n",
    "    print(f\"Number of images in {folder_path}: {len(image_paths)}\")\n",
    "    return image_paths_np\n",
    "\n",
    "all_images_folder_path = os.path.join(data_dir, \"ALL\")\n",
    "all_images_paths = from_tif_folder_to_np_paths_array(all_images_folder_path)\n",
    "print(\"Number of images in ALL folder:\", len(all_images_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d61bcd73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MSA', 'PD']\n",
      "Number of images in /home/zano/Documents/TESI/3c_MIP_new/CONTROL: 18\n",
      "Number of images in /home/zano/Documents/TESI/3c_MIP_new/CONTROL folder: 18\n"
     ]
    }
   ],
   "source": [
    "## Paths of ALL images into a numpy array without labels used for SSL\n",
    "print(class_names)\n",
    "if class_names == ['MSA-P', 'PD']:\n",
    "    ssl_images_folder_path = os.path.join(data_dir, \"CONTROL+MSA-C\")\n",
    "else:\n",
    "    ssl_images_folder_path = os.path.join(data_dir, \"CONTROL\")\n",
    "    \n",
    "\n",
    "ssl_images_paths_np = from_tif_folder_to_np_paths_array(ssl_images_folder_path)\n",
    "print(f\"Number of images in {ssl_images_folder_path} folder:\", len(ssl_images_paths_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ba51763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MSA': '/home/zano/Documents/TESI/3c_MIP_new/MSA', 'PD': '/home/zano/Documents/TESI/3c_MIP_new/PD'}\n",
      "Class directories:\n",
      "{'MSA': '/home/zano/Documents/TESI/3c_MIP_new/MSA', 'PD': '/home/zano/Documents/TESI/3c_MIP_new/PD'}\n",
      "MSA images (before filtering): 'gh' count: 83, 'vaso' count: 0\n",
      "Number of glandular images before filtering: 83\n",
      "Number of glandular images after filtering: 83\n",
      "PD images (before filtering): 'gh' count: 57, 'vaso' count: 0\n",
      "Number of glandular images before filtering: 57\n",
      "Number of glandular images after filtering: 57\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking image shapes:\n",
      "MSA image: MAX_4092.lif - 4092 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_4092.lif - 4092 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_4121.lif - 4121 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_4121.lif - 4121 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5358.lif - 5358 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5358.lif - 5358 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5435 gh.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5435 gh2.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5463 gh.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5717.lif - 5717 DL VIP r TH b Sinapto gr DAPI grey 63x z2 gh 2 pinhole 1 z 05.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5717.lif - 5717 DL VIP r TH b Sinapto gr DAPI grey 63x z2 gh pinhole 1 z 05.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5745 gh.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5745 gh2.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5753 gh.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5753 gh2.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5753 gh3.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5767.lif - 5767 DL VIP r TH b Sinapto gr DAPI grey 63x z2 gh pinhole 1 z 05.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5767.lif - 5767 DL VIP r TH b Sinapto gr DAPI grey 63x z2 gh2 pinhole 1 z 05.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5776 gh.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5776 gh2.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5878.lif - 5878 DL VIP r TH b Sinapto gr DAPI grey 63x z2 gh2 pinhole 1 z 05.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5878.lif - 5878 DL VIP r TH b Sinapto gr DAPIgrey 63x z2 gh pinhole 1 z 05.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5881 gh.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5881 gh2.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5904 gh.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5904 gh2.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5954 gh.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5954 gh2.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5969 gh.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5969 gh2.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5978 gh.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5992 gh.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5992 gh2.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5996 gh.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5996 gh2.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_6046 gh.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_6046 gh2.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_6053 gh.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_6053 gh2.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_6060 gh.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_6085 gh.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_6085 gh2.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_6179 gh.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_6179 gh2.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_6308.lif - 6308 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_6308.lif - 6308 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_6311.lif - 6311 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_6311.lif - 6311 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_6326.lif - 6326 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_6326.lif - 6326 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_6485.lif - 6485 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_6485.lif - 6485 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_6491.lif - 6491 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_6491.lif - 6491 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_6593.lif - 6593 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_6593.lif - 6593 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_6599.lif - 6599 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_6599.lif - 6599 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_6657.lif - 6657 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_6657.lif - 6657 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_6663.lif - 6663 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_6663.lif - 6663 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_7105.lif - 7105 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh  n2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_7105.lif - 7105 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_7120.lif - 7120 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_7120.lif - 7120 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_7132.lif - 7132 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh n2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_7132.lif - 7132 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_7144.lif - 7144 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_7144.lif - 7144 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_7179.lif - 7179 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh  n2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_7179.lif - 7179 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_7185.lif - 7185 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh  n2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_7185.lif - 7185 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_7191.lif - 7191 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh  n2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_7191.lif - 7191 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_7239.lif - 7239 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_7239.lif - 7239 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_7293.lif - 7293 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_7293.lif - 7293 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_7343.lif - 7343 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_7343.lif - 7343 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_7579.lif - 7579 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6008.lif - 6008 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh  n2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6008.lif - 6008 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6320 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6323.lif - 6323 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh  n2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6323.lif - 6323 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6337.lif - 6337 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh  n2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6337.lif - 6337 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6340.lif - 6340 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh  n2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6340.lif - 6340 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6351.lif - 6351 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh n2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6351.lif - 6351 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6363.lif - 6363 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6363.lif - 6363 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6366.lif - 6366 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh n2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6366.lif - 6366 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6375.lif - 6375 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6375.lif - 6375 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6383.lif - 6383 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6383.lif - 6383 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6424.lif - 6424 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6424.lif - 6424 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6427.lif - 6427 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6427.lif - 6427 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6459.lif - 6459 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh  n2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6459.lif - 6459 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6571.lif - 6571 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6571.lif - 6571 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6577.lif - 6577 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh  n2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6577.lif - 6577 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6616.lif - 6616 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh  n2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6616.lif - 6616 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6651.lif - 6651 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6651.lif - 6651 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6690.lif - 6690 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh  n2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6690.lif - 6690 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6696.lif - 6696 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh  n2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6696.lif - 6696 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6749.lif - 6749 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6749.lif - 6749 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6773.lif - 6773 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh  n2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6773.lif - 6773 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6791.lif - 6791 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6791.lif - 6791 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_7155.lif - 7155 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_7155.lif - 7155 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_7222.lif - 7222 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_7222.lif - 7222 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_7229.lif - 7229 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_7229.lif - 7229 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_7284.lif - 7284 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_7284.lif - 7284 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_7461.lif - 7461 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_7461.lif - 7461 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_7544.lif - 7544 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_7677.lif - 7677 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_7688.lif - 7688 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_7710.lif - 7710 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "\n",
      "Minority label for resampling purposes: 1\n",
      "\n",
      "Sample of image paths: ('/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_4092.lif - 4092 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif', '/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_4092.lif - 4092 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif', '/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_4121.lif - 4121 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif', '/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_4121.lif - 4121 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif', '/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5358.lif - 5358 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif')\n",
      "Total images found: 140\n",
      "\n",
      "Sample of image paths (NumPy): ['/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_4092.lif - 4092 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif'\n",
      " '/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_4092.lif - 4092 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif'\n",
      " '/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_4121.lif - 4121 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif'\n",
      " '/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_4121.lif - 4121 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif'\n",
      " '/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5358.lif - 5358 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif']\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary mapping each class to its directory\n",
    "class_dirs = {} # { \"class_name\": \"path/to/class_dir\", \"class_name2\": \"path/to/class_dir2\", ... }\n",
    "is_three_classes = (len(class_names) == 3)\n",
    "\n",
    "for class_name in class_names:\n",
    "    class_dirs[class_name] = os.path.join(data_dir, class_name)\n",
    "    \n",
    "print(class_dirs)\n",
    "if is_three_classes:\n",
    "    class2_name, class1_name, class0_name = class_names\n",
    "    class2_dir, class1_dir, class0_dir = class_dirs.values()\n",
    "else:\n",
    "    class1_name, class0_name = class_names\n",
    "    class1_dir, class0_dir = class_dirs.values()\n",
    "\n",
    "print(\"Class directories:\")\n",
    "print(class_dirs)\n",
    "\n",
    "# Dictionaries to store image paths and counts for each class\n",
    "images_paths_dict = {}\n",
    "counts_dict = {}\n",
    "\n",
    "# Loop over classes to process each folder\n",
    "for class_name in class_names:\n",
    "    class_dir = class_dirs[class_name]\n",
    "    image_paths = sorted(glob.glob(os.path.join(class_dir, \"*.tif\")))\n",
    "    \n",
    "    # Check if images were found; otherwise raise an error\n",
    "    if not image_paths:\n",
    "        raise FileNotFoundError(f\"No TIFF image file found in {class_dir}\")\n",
    "    \n",
    "    # Count occurrences of 'gh' and 'vaso' in the filenames (using .lower() for case insensitivity)\n",
    "    gh_count = sum('gh' in os.path.basename(path).lower() for path in image_paths)\n",
    "    vaso_count = sum('vaso' in os.path.basename(path).lower() for path in image_paths)\n",
    "    print(f\"{class_name} images (before filtering): 'gh' count: {gh_count}, 'vaso' count: {vaso_count}\")\n",
    "    \n",
    "    # Filter out images that contain 'vaso' (if needed)\n",
    "    from utils.data_extraction_functions import remove_non_gland_images\n",
    "    image_paths = remove_non_gland_images(image_paths)\n",
    "    # counts after filtering\n",
    "    gh_count_after = sum('gh' in os.path.basename(path).lower() for path in image_paths)\n",
    "    vaso_count_after = sum('vaso' in os.path.basename(path).lower() for path in image_paths)\n",
    "    # print(f\"After removing 'vaso', {class_name} images: 'gh' count: {gh_count_after}, 'vaso' count: {vaso_count_after}\")\n",
    "    \n",
    "    # Store the filtered image paths and counts for later use\n",
    "    images_paths_dict[class_name] = image_paths\n",
    "    counts_dict[class_name] = {\"gh_count\": gh_count_after, \"vaso_count\": vaso_count_after}\n",
    "\n",
    "# Visualize the number of 'gh' counts per class in a bar chart\n",
    "# def plot_counts_bar_chart(counts_dict, class_names):\n",
    "#     \"\"\"\n",
    "#     Plot a bar chart of counts for each class.\n",
    "#     \"\"\"\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.bar(class_names, [counts_dict[cn][\"gh_count\"] for cn in class_names], color='blue')\n",
    "#     plt.xlabel(\"Class\")\n",
    "#     plt.ylabel(\"Number of 'gh' occurrences\")\n",
    "#     plt.title(\"Number of 'gh' occurrences per class\")\n",
    "#     plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Number of 'gh' occurrences\")\n",
    "plt.title(\"Number of 'gh' occurrences per class\")\n",
    "bar_heights = [counts_dict[cn][\"gh_count\"] for cn in class_names]\n",
    "bar_colors = ['red', 'blue', 'lightblue']\n",
    "plt.bar(class_names, bar_heights, color=bar_colors)\n",
    "plt.show()\n",
    "\n",
    "# --- Debug: Check image shapes after initial loading ---\n",
    "print(\"\\nChecking image shapes:\")\n",
    "for class_name, image_paths in images_paths_dict.items():\n",
    "    for path in image_paths:\n",
    "        img = tifffile.imread(path)  # Read image as a numpy array\n",
    "        print(f\"{class_name} image: {os.path.basename(path)}  dtype: {img.dtype}, shape: {img.shape}\")\n",
    "\n",
    "# Combine image paths and labels for the three classes; \n",
    "# the label here is simply the index of the class in class_names (0, 1, 2)\n",
    "combined = [] # List to store tuples of (image_path, label)\n",
    "for label, class_name in enumerate(class_names):\n",
    "    for path in images_paths_dict[class_name]:\n",
    "        combined.append((path, label))\n",
    "# print(\"\\nSample of combined image paths and labels:\", combined[:5])\n",
    "# random.shuffle(combined)  # Shuffle the combined list to mix classes\n",
    "\n",
    "# Optionally, determine the minority label for resampling purposes\n",
    "counts = {label: len(images_paths_dict[class_name]) for label, class_name in enumerate(class_names)}\n",
    "minority_label = min(counts.keys(), key=lambda k: counts[k])\n",
    "print(f\"\\nMinority label for resampling purposes: {minority_label}\")\n",
    "\n",
    "# Unzip the combined list back into separate tuples (if needed)\n",
    "images_paths, labels = zip(*combined)\n",
    "print(\"\\nSample of image paths:\", images_paths[:5])\n",
    "print(\"Total images found:\", len(combined))\n",
    "\n",
    "# Optionally, convert to NumPy arrays (helpful for further processing or k-fold splitting)\n",
    "images_paths_np = np.array(images_paths)\n",
    "labels_np = np.array(labels)\n",
    "print(\"\\nSample of image paths (NumPy):\", images_paths_np[:5])\n",
    "print((labels_np))\n",
    "#print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5763ba96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 140\n",
      "Original label distribution: {0: 83, 1: 57}\n",
      "\n",
      "Aiming for a balanced test set with 57 samples per class.\n",
      "Total balanced test set size will be: 114\n",
      "Test set size: 114\n",
      "\n",
      "Test set distribution: {0: 57, 1: 57}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set counts and percentages:\n",
      "MSA: 57 images (50.0% of test set)\n",
      "PD: 57 images (50.0% of test set)\n"
     ]
    }
   ],
   "source": [
    "# --- Your Split Logic for 50/50 distribution in test set ---\n",
    "print(\"Original dataset size:\", len(images_paths_np))\n",
    "\n",
    "# Find unique labels and their counts in the original dataset\n",
    "unique_labels, counts = np.unique(labels_np, return_counts=True)\n",
    "original_distribution = dict(zip(unique_labels, counts))\n",
    "print(f\"Original label distribution: {original_distribution}\")\n",
    "\n",
    "# Determine the maximum possible size for a balanced test set per class\n",
    "# This is limited by the count of the smallest class\n",
    "if len(unique_labels) > 1:\n",
    "    min_class_count = min(counts)\n",
    "    # We want a balanced test set, so take 'min_class_count' samples from each class\n",
    "    test_samples_per_class = min_class_count\n",
    "    total_balanced_test_size = test_samples_per_class * len(unique_labels)\n",
    "\n",
    "    print(f\"\\nAiming for a balanced test set with {test_samples_per_class} samples per class.\")\n",
    "    print(f\"Total balanced test set size will be: {total_balanced_test_size}\")\n",
    "\n",
    "    test_indices = []\n",
    "    train_indices = []\n",
    "\n",
    "    # Iterate through each class to split\n",
    "    for label in unique_labels:\n",
    "        # Get the indices in the original array that correspond to the current class\n",
    "        # print ( labels_np == label) # returns a boolean array\n",
    "        class_indices = np.where(labels_np == label)[0]  #use the boolean array to get the indices where cond is true \n",
    "        # print(f\"\\nClass {label} indices: {class_indices}\") #retuns the indices of the class in the original array and the boolarray so we use [0] to get the indices\n",
    "\n",
    "        # Randomly select a fixed number of indices for the test set from this class\n",
    "        # Use np.random.choice with replace=False for sampling without replacement\n",
    "        # Set a random_state for reproducibility if needed\n",
    "        rng = np.random.default_rng(42) # Use new random generator recommended over np.random.seed\n",
    "        test_class_indices = rng.choice(\n",
    "            class_indices,\n",
    "            size=test_samples_per_class,\n",
    "            replace=False\n",
    "        )\n",
    "        test_indices.extend(test_class_indices)\n",
    "\n",
    "    # Convert lists of indices to NumPy arrays\n",
    "    test_indices = np.array(test_indices)\n",
    "    train_indices = np.array(train_indices)\n",
    "\n",
    "    # Shuffle the indices to mix up the classes in the final arrays (optional but good practice)\n",
    "    # rng.shuffle(test_indices)\n",
    "    # rng.shuffle(train_indices)\n",
    "\n",
    "    balanced_test_images_paths = images_paths_np[test_indices]\n",
    "    balanced_test_true_labels = labels_np[test_indices]\n",
    "    print(f\"Test set size: {len(balanced_test_images_paths)}\")\n",
    "\n",
    "    # Verify the test set distribution\n",
    "    test_unique_labels, test_counts = np.unique(balanced_test_true_labels, return_counts=True)\n",
    "    test_distribution = dict(zip(test_unique_labels, test_counts))\n",
    "    print(f\"\\nTest set distribution: {test_distribution}\")\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    # Use test_unique_labels and test_counts for the bar plot\n",
    "    labels_for_plot = [class_names[label] if 'class_names' in locals() else f\"Label {label}\" for label in test_unique_labels]\n",
    "    plt.bar(labels_for_plot, test_counts)\n",
    "    plt.xlabel(\"Class\")\n",
    "    plt.ylabel(\"Number of test images\")\n",
    "    plt.title(\"Test Set Label Distribution (Balanced)\")\n",
    "    plt.show()\n",
    "\n",
    "    # Print counts and percentages for the balanced test set\n",
    "    print(\"\\nTest set counts and percentages:\")\n",
    "    for label, count in zip(test_unique_labels, test_counts):\n",
    "         class_name = class_names[label] if 'class_names' in locals() else f\"Label {label}\"\n",
    "         print(f\"{class_name}: {count} images ({count/len(balanced_test_true_labels):.1%} of test set)\")\n",
    "\n",
    "else:\n",
    "    print(\"Cannot perform a balanced split with less than two unique classes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e782dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119 training images\n",
      "21 test images\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0 (MSA): 12 images\n",
      "Label 1 (PD): 9 images\n",
      "Label 0 (MSA) is: 0.5714285714285714\n",
      "Label 1 (PD) is: 0.42857142857142855\n"
     ]
    }
   ],
   "source": [
    "# NB the test set must be splitted BEFORE oversampling to avoid data leakage!\n",
    "# -------------------------------------------------------------------------\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#returns numpy arrays containing the paths to images and the labels\n",
    "train_images_paths, test_images_paths, train_true_labels, test_true_labels = train_test_split(\n",
    "    images_paths_np,\n",
    "    labels_np,\n",
    "    test_size= 0.15,\n",
    "    stratify=labels,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "test_images_paths_np = np.array(test_images_paths)\n",
    "test_true_labels_np = np.array(test_true_labels)\n",
    "# print(\"train images paths:\", train_images_paths)\n",
    "# print(\"true test labels:\", test_true_labels)\n",
    "# # For the cross-validation, we'll use train_images_paths and labels_temp\n",
    "train_images_paths_np = np.array(train_images_paths) #contains the images paths\n",
    "train_labels_np = np.array(train_true_labels) #contains the labels\n",
    "print(f\"{train_images_paths_np.shape[0]} training images\")\n",
    "print(f\"{len(test_images_paths)} test images\")\n",
    "#test_images_paths = [os.path.basename(path) for path in test_images_paths]\n",
    "# print(test_images_paths)\n",
    "print(type(train_images_paths))\n",
    "\n",
    "unique_labels, counts = np.unique(test_true_labels_np, return_counts=True)\n",
    "\n",
    "\n",
    "plt.bar([class_names[label] for label in unique_labels], counts)\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Number of test images\")\n",
    "plt.title(\"Test Set Label Distribution\")\n",
    "plt.show()\n",
    "\n",
    "for label, count in zip(unique_labels, counts):\n",
    "    print(f\"Label {label} ({class_names[label]}): {count} images\")\n",
    "\n",
    "for label, count in zip(unique_labels, counts):\n",
    "    print(f\"Label {label} ({class_names[label]}) is: {count/test_true_labels_np.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f60f714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "image_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "label",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "patient_id",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "82f9b96c-bf79-41b0-b81f-35743036e05a",
       "rows": [
        [
         "0",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_4092.lif - 4092 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif",
         "0",
         "4092"
        ],
        [
         "1",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_4092.lif - 4092 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif",
         "0",
         "4092"
        ],
        [
         "2",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_4121.lif - 4121 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif",
         "0",
         "4121"
        ],
        [
         "3",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_4121.lif - 4121 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif",
         "0",
         "4121"
        ],
        [
         "4",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5358.lif - 5358 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif",
         "0",
         "5358"
        ],
        [
         "5",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5358.lif - 5358 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh2.tif",
         "0",
         "5358"
        ],
        [
         "6",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5435 gh.tif.tif",
         "0",
         "5435"
        ],
        [
         "7",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5435 gh2.tif.tif",
         "0",
         "5435"
        ],
        [
         "8",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5463 gh.tif.tif",
         "0",
         "5463"
        ],
        [
         "9",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5717.lif - 5717 DL VIP r TH b Sinapto gr DAPI grey 63x z2 gh 2 pinhole 1 z 05.tif",
         "0",
         "5717"
        ],
        [
         "10",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5717.lif - 5717 DL VIP r TH b Sinapto gr DAPI grey 63x z2 gh pinhole 1 z 05.tif",
         "0",
         "5717"
        ],
        [
         "11",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5745 gh.tif.tif",
         "0",
         "5745"
        ],
        [
         "12",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5745 gh2.tif.tif",
         "0",
         "5745"
        ],
        [
         "13",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5753 gh.tif.tif",
         "0",
         "5753"
        ],
        [
         "14",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5753 gh2.tif.tif",
         "0",
         "5753"
        ],
        [
         "15",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5753 gh3.tif.tif",
         "0",
         "5753"
        ],
        [
         "16",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5767.lif - 5767 DL VIP r TH b Sinapto gr DAPI grey 63x z2 gh pinhole 1 z 05.tif",
         "0",
         "5767"
        ],
        [
         "17",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5767.lif - 5767 DL VIP r TH b Sinapto gr DAPI grey 63x z2 gh2 pinhole 1 z 05.tif",
         "0",
         "5767"
        ],
        [
         "18",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5776 gh.tif.tif",
         "0",
         "5776"
        ],
        [
         "19",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5776 gh2.tif.tif",
         "0",
         "5776"
        ],
        [
         "20",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5878.lif - 5878 DL VIP r TH b Sinapto gr DAPI grey 63x z2 gh2 pinhole 1 z 05.tif",
         "0",
         "5878"
        ],
        [
         "21",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5878.lif - 5878 DL VIP r TH b Sinapto gr DAPIgrey 63x z2 gh pinhole 1 z 05.tif",
         "0",
         "5878"
        ],
        [
         "22",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5881 gh.tif.tif",
         "0",
         "5881"
        ],
        [
         "23",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5881 gh2.tif.tif",
         "0",
         "5881"
        ],
        [
         "24",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5904 gh.tif.tif",
         "0",
         "5904"
        ],
        [
         "25",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5904 gh2.tif.tif",
         "0",
         "5904"
        ],
        [
         "26",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5954 gh.tif.tif",
         "0",
         "5954"
        ],
        [
         "27",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5954 gh2.tif.tif",
         "0",
         "5954"
        ],
        [
         "28",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5969 gh.tif.tif",
         "0",
         "5969"
        ],
        [
         "29",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5969 gh2.tif.tif",
         "0",
         "5969"
        ],
        [
         "30",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5978 gh.tif.tif",
         "0",
         "5978"
        ],
        [
         "31",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5992 gh.tif.tif",
         "0",
         "5992"
        ],
        [
         "32",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5992 gh2.tif.tif",
         "0",
         "5992"
        ],
        [
         "33",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5996 gh.tif.tif",
         "0",
         "5996"
        ],
        [
         "34",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5996 gh2.tif.tif",
         "0",
         "5996"
        ],
        [
         "35",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_6046 gh.tif.tif",
         "0",
         "6046"
        ],
        [
         "36",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_6046 gh2.tif.tif",
         "0",
         "6046"
        ],
        [
         "37",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_6053 gh.tif.tif",
         "0",
         "6053"
        ],
        [
         "38",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_6053 gh2.tif.tif",
         "0",
         "6053"
        ],
        [
         "39",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_6060 gh.tif.tif",
         "0",
         "6060"
        ],
        [
         "40",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_6085 gh.tif.tif",
         "0",
         "6085"
        ],
        [
         "41",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_6085 gh2.tif.tif",
         "0",
         "6085"
        ],
        [
         "42",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_6179 gh.tif.tif",
         "0",
         "6179"
        ],
        [
         "43",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_6179 gh2.tif.tif",
         "0",
         "6179"
        ],
        [
         "44",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_6308.lif - 6308 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif",
         "0",
         "6308"
        ],
        [
         "45",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_6308.lif - 6308 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif",
         "0",
         "6308"
        ],
        [
         "46",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_6311.lif - 6311 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif",
         "0",
         "6311"
        ],
        [
         "47",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_6311.lif - 6311 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif",
         "0",
         "6311"
        ],
        [
         "48",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_6326.lif - 6326 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif",
         "0",
         "6326"
        ],
        [
         "49",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_6326.lif - 6326 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif",
         "0",
         "6326"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 140
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>label</th>\n",
       "      <th>patient_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_4...</td>\n",
       "      <td>0</td>\n",
       "      <td>4092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_4...</td>\n",
       "      <td>0</td>\n",
       "      <td>4092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_4...</td>\n",
       "      <td>0</td>\n",
       "      <td>4121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_4...</td>\n",
       "      <td>0</td>\n",
       "      <td>4121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5...</td>\n",
       "      <td>0</td>\n",
       "      <td>5358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>/home/zano/Documents/TESI/3c_MIP_new/PD/MAX_74...</td>\n",
       "      <td>1</td>\n",
       "      <td>7461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>/home/zano/Documents/TESI/3c_MIP_new/PD/MAX_75...</td>\n",
       "      <td>1</td>\n",
       "      <td>7544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>/home/zano/Documents/TESI/3c_MIP_new/PD/MAX_76...</td>\n",
       "      <td>1</td>\n",
       "      <td>7677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>/home/zano/Documents/TESI/3c_MIP_new/PD/MAX_76...</td>\n",
       "      <td>1</td>\n",
       "      <td>7688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>/home/zano/Documents/TESI/3c_MIP_new/PD/MAX_77...</td>\n",
       "      <td>1</td>\n",
       "      <td>7710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            image_path  label patient_id\n",
       "0    /home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_4...      0       4092\n",
       "1    /home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_4...      0       4092\n",
       "2    /home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_4...      0       4121\n",
       "3    /home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_4...      0       4121\n",
       "4    /home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5...      0       5358\n",
       "..                                                 ...    ...        ...\n",
       "135  /home/zano/Documents/TESI/3c_MIP_new/PD/MAX_74...      1       7461\n",
       "136  /home/zano/Documents/TESI/3c_MIP_new/PD/MAX_75...      1       7544\n",
       "137  /home/zano/Documents/TESI/3c_MIP_new/PD/MAX_76...      1       7677\n",
       "138  /home/zano/Documents/TESI/3c_MIP_new/PD/MAX_76...      1       7688\n",
       "139  /home/zano/Documents/TESI/3c_MIP_new/PD/MAX_77...      1       7710\n",
       "\n",
       "[140 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique patient IDs: ['4092' '4121' '5358' '5435' '5463' '5717' '5745' '5753' '5767' '5776'\n",
      " '5878' '5881' '5904' '5954' '5969' '5978' '5992' '5996' '6008' '6046'\n",
      " '6053' '6060' '6085' '6179' '6308' '6311' '6320' '6323' '6326' '6337'\n",
      " '6340' '6351' '6363' '6366' '6375' '6383' '6424' '6427' '6459' '6485'\n",
      " '6491' '6571' '6577' '6593' '6599' '6616' '6651' '6657' '6663' '6690'\n",
      " '6696' '6749' '6773' '6791' '7105' '7120' '7132' '7144' '7155' '7179'\n",
      " '7185' '7191' '7222' '7229' '7239' '7284' '7293' '7343' '7461' '7544'\n",
      " '7579' '7677' '7688' '7710']\n",
      "Number of unique patients: 74\n",
      "Unique patient labels: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1\n",
      " 1 1 0 0 1 1 0 0 1 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 1 0 1 1 1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "patient_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "label",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "6b208a07-ae64-4a7e-a734-c2ad90bc5f20",
       "rows": [
        [
         "0",
         "4092",
         "0"
        ],
        [
         "1",
         "4121",
         "0"
        ],
        [
         "2",
         "5358",
         "0"
        ],
        [
         "3",
         "5435",
         "0"
        ],
        [
         "4",
         "5463",
         "0"
        ],
        [
         "5",
         "5717",
         "0"
        ],
        [
         "6",
         "5745",
         "0"
        ],
        [
         "7",
         "5753",
         "0"
        ],
        [
         "8",
         "5767",
         "0"
        ],
        [
         "9",
         "5776",
         "0"
        ],
        [
         "10",
         "5878",
         "0"
        ],
        [
         "11",
         "5881",
         "0"
        ],
        [
         "12",
         "5904",
         "0"
        ],
        [
         "13",
         "5954",
         "0"
        ],
        [
         "14",
         "5969",
         "0"
        ],
        [
         "15",
         "5978",
         "0"
        ],
        [
         "16",
         "5992",
         "0"
        ],
        [
         "17",
         "5996",
         "0"
        ],
        [
         "18",
         "6008",
         "1"
        ],
        [
         "19",
         "6046",
         "0"
        ],
        [
         "20",
         "6053",
         "0"
        ],
        [
         "21",
         "6060",
         "0"
        ],
        [
         "22",
         "6085",
         "0"
        ],
        [
         "23",
         "6179",
         "0"
        ],
        [
         "24",
         "6308",
         "0"
        ],
        [
         "25",
         "6311",
         "0"
        ],
        [
         "26",
         "6320",
         "1"
        ],
        [
         "27",
         "6323",
         "1"
        ],
        [
         "28",
         "6326",
         "0"
        ],
        [
         "29",
         "6337",
         "1"
        ],
        [
         "30",
         "6340",
         "1"
        ],
        [
         "31",
         "6351",
         "1"
        ],
        [
         "32",
         "6363",
         "1"
        ],
        [
         "33",
         "6366",
         "1"
        ],
        [
         "34",
         "6375",
         "1"
        ],
        [
         "35",
         "6383",
         "1"
        ],
        [
         "36",
         "6424",
         "1"
        ],
        [
         "37",
         "6427",
         "1"
        ],
        [
         "38",
         "6459",
         "1"
        ],
        [
         "39",
         "6485",
         "0"
        ],
        [
         "40",
         "6491",
         "0"
        ],
        [
         "41",
         "6571",
         "1"
        ],
        [
         "42",
         "6577",
         "1"
        ],
        [
         "43",
         "6593",
         "0"
        ],
        [
         "44",
         "6599",
         "0"
        ],
        [
         "45",
         "6616",
         "1"
        ],
        [
         "46",
         "6651",
         "1"
        ],
        [
         "47",
         "6657",
         "0"
        ],
        [
         "48",
         "6663",
         "0"
        ],
        [
         "49",
         "6690",
         "1"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 74
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4092</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4121</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5358</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5435</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5463</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>7544</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>7579</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>7677</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>7688</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>7710</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   patient_id  label\n",
       "0        4092      0\n",
       "1        4121      0\n",
       "2        5358      0\n",
       "3        5435      0\n",
       "4        5463      0\n",
       "..        ...    ...\n",
       "69       7544      1\n",
       "70       7579      0\n",
       "71       7677      1\n",
       "72       7688      1\n",
       "73       7710      1\n",
       "\n",
       "[74 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_patient_id(image_path):\n",
    "    # Example: parse from the file name\n",
    "    # In real code, you might have a different pattern\n",
    "    match = re.search(r'(\\d{4})', image_path)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        return \"UNKNOWN\"\n",
    "\n",
    "# Build a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"image_path\": images_paths_np,\n",
    "    \"label\": labels_np\n",
    "})\n",
    "\n",
    "df[\"patient_id\"] = df[\"image_path\"].apply(extract_patient_id)\n",
    "\n",
    "display(df)\n",
    "\n",
    "# Ensure everything is string or int\n",
    "df[\"patient_id\"] = df[\"patient_id\"].astype(str)\n",
    "\n",
    "# Now group by patient to get a single label per patient.\n",
    "# If every patient truly has exactly one label, we can just take .first()\n",
    "patient_label_df = df.groupby(\"patient_id\", as_index=False)[\"label\"].first()\n",
    "\n",
    "unique_pat_ids = patient_label_df[\"patient_id\"].values  # need these to stratify for patient\n",
    "print(f\"Unique patient IDs: {unique_pat_ids}\")\n",
    "print(f\"Number of unique patients: {len(unique_pat_ids)}\")\n",
    "pat_labels = patient_label_df[\"label\"].values\n",
    "print(f\"Unique patient labels: {pat_labels}\")\n",
    "\n",
    "patient_label_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22dda6a",
   "metadata": {},
   "source": [
    "## TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4624fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import I\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "# Import PyTorch schedulers\n",
    "from torch.optim.lr_scheduler import LambdaLR, CosineAnnealingLR, SequentialLR\n",
    "import torchvision\n",
    "\n",
    "# Lightly imports (make sure these are installed and correct)\n",
    "from lightly.data import LightlyDataset\n",
    "from lightly.transforms.byol_transform import BYOLTransform, BYOLView1Transform, BYOLView2Transform\n",
    "# Assuming BarlowTwinsProjectionHead and BarlowTwinsLoss are defined correctly elsewhere\n",
    "# from your_module import BarlowTwinsProjectionHead, BarlowTwinsLoss\n",
    "\n",
    "LEARNING_RATE = 3e-4 # Initial learning rate (peak after warmup)\n",
    "WEIGHT_DECAY = 1e-4\n",
    "MAX_EPOCHS = 250     # Or your desired number\n",
    "WARMUP_EPOCHS = 10   # Number of epochs for linear warmup\n",
    "BATCH_SIZE = 8      # Increase if GPU memory allows (e.g., 64, 128, 256)\n",
    "NUM_WORKERS = 4\n",
    "GRADIENT_CLIP_VAL = 1.0 # Max norm for gradient clipping\n",
    "INPUT_DIR = ssl_images_folder_path\n",
    "BARLOW_PROJECT_DIM = 2048 # Or your desired projection dim\n",
    "\n",
    "\n",
    "# --- Helper function for linear warmup lambda ---\n",
    "def linear_warmup_decay(warmup_steps):\n",
    "    \"\"\" Linear warmup for warmup_steps steps. \"\"\"\n",
    "    def fn(step):\n",
    "        if step < warmup_steps:\n",
    "            return float(step) / float(max(1, warmup_steps))\n",
    "        return 1.0 # Constant multiplier after warmup\n",
    "    return fn\n",
    "\n",
    "########################################################\n",
    "# 2) Barlow Twins LightningModule\n",
    "########################################################\n",
    "class BarlowTwins(pl.LightningModule):\n",
    "    def __init__(self, learning_rate, warmup_epochs, max_epochs):\n",
    "        \"\"\"\n",
    "        ResNet18 backbone -> 512-d features\n",
    "        Projection head -> (512 -> proj_dim -> proj_dim)\n",
    "        BarlowTwinsLoss for self-supervised training\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Save hyperparameters like learning_rate, warmup_epochs, max_epochs\n",
    "        # These are needed for scheduler setup in configure_optimizers\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Create ResNet18, initialized with ImageNet weights\n",
    "        resnet = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        resnet_output_dim = 512\n",
    "\n",
    "        # Barlow Twins head\n",
    "        self.projection_head = BarlowTwinsProjectionHead(\n",
    "            resnet_output_dim,\n",
    "            BARLOW_PROJECT_DIM,\n",
    "            BARLOW_PROJECT_DIM\n",
    "        )\n",
    "        self.criterion = BarlowTwinsLoss()\n",
    "\n",
    "        # Store steps_per_epoch, calculated in setup\n",
    "        self.steps_per_epoch = 0\n",
    "\n",
    "    # --- REVISED setup method ---\n",
    "    def setup(self, stage=None):\n",
    "        \"\"\"Calculate steps_per_epoch after dataloader is available.\"\"\"\n",
    "        if stage == 'fit' or stage is None:\n",
    "            # Check if trainer and dataloaders are available\n",
    "            if not self.trainer or not self.trainer.train_dataloader:\n",
    "                print(\"Warning: Trainer or train_dataloader not available in setup.\")\n",
    "                return\n",
    "\n",
    "            try:\n",
    "                # Get the length of the dataloader\n",
    "                self.steps_per_epoch = len(self.trainer.train_dataloader)\n",
    "                print(f\"Calculated steps per epoch in setup: {self.steps_per_epoch}\")\n",
    "\n",
    "            except TypeError:\n",
    "                # Handle cases where dataloader has no __len__ (e.g., IterableDataset)\n",
    "                # Estimate based on limit_train_batches if set\n",
    "                if self.trainer.limit_train_batches:\n",
    "                     if isinstance(self.trainer.limit_train_batches, int):\n",
    "                         self.steps_per_epoch = self.trainer.limit_train_batches\n",
    "                     elif isinstance(self.trainer.limit_train_batches, float):\n",
    "                          # Estimate requires dataset size, difficult to get reliably here\n",
    "                          self.steps_per_epoch = 500 # Fallback guess\n",
    "                          print(f\"Warning: Cannot determine length from dataloader and limit_train_batches is a float.\")\n",
    "                          print(f\"Using fallback estimate for steps_per_epoch: {self.steps_per_epoch}\")\n",
    "                     else:\n",
    "                          self.steps_per_epoch = 500 # Fallback guess\n",
    "                          print(f\"Warning: Unknown type for limit_train_batches.\")\n",
    "                          print(f\"Using fallback estimate for steps_per_epoch: {self.steps_per_epoch}\")\n",
    "                     print(f\"Using limit_train_batches to set steps_per_epoch: {self.steps_per_epoch}\")\n",
    "                else:\n",
    "                     # If no length and no limit, estimation is hard. Use a default with warning.\n",
    "                     self.steps_per_epoch = 500 # Fallback guess\n",
    "                     print(f\"Warning: Could not determine dataloader length (possibly IterableDataset without length).\")\n",
    "                     print(f\"Using fallback estimate for steps_per_epoch: {self.steps_per_epoch}\")\n",
    "                     print(f\"For accurate scheduler behaviour, ensure train dataloader has __len__ or set trainer's limit_train_batches.\")\n",
    "\n",
    "            if self.steps_per_epoch == 0:\n",
    "                 raise ValueError(\"Could not determine steps_per_epoch for LR scheduler. Aborting.\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Forward pass for one augmented view x. \"\"\"\n",
    "        x = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.projection_head(x)\n",
    "        return z\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\" Barlow Twins requires two augmented views (x0, x1). \"\"\"\n",
    "        # Adapt based on actual batch structure from LightlyDataset\n",
    "        if isinstance(batch, (list, tuple)) and len(batch) == 3:\n",
    "             (x0, x1), _, _ = batch # common case: (views), labels, fnames\n",
    "        elif isinstance(batch, (list, tuple)) and len(batch) == 2 and isinstance(batch[0], (list, tuple)):\n",
    "             (x0, x1), _ = batch # case: (views), labels\n",
    "        elif isinstance(batch, (list, tuple)) and len(batch) == 1 and isinstance(batch[0], (list, tuple)):\n",
    "             (x0, x1) = batch[0] # case: (views)\n",
    "        else:\n",
    "            # Attempt to handle dictionary batches if applicable\n",
    "            try:\n",
    "                 views = batch[0] # Assuming views are the first element if it's not a standard lightly format\n",
    "                 if isinstance(views, (list, tuple)) and len(views) == 2:\n",
    "                      x0, x1 = views\n",
    "                 else:\n",
    "                      raise TypeError(\"Could not extract two views from batch[0]\")\n",
    "            except (IndexError, KeyError, TypeError) as e:\n",
    "                 raise ValueError(f\"Unexpected batch format in training_step. Type: {type(batch)}, Content sample: {str(batch)[:100]}, Error: {e}\")\n",
    "\n",
    "\n",
    "        z0 = self.forward(x0)\n",
    "        z1 = self.forward(x1)\n",
    "        loss = self.criterion(z0, z1)\n",
    "\n",
    "        # Log loss and learning rate\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        optimizers = self.optimizers()\n",
    "        # If multiple optimizers, take the first one\n",
    "        optimizer = optimizers[0] if isinstance(optimizers, list) else optimizers\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        self.log('learning_rate', lr, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\" Use AdamW and a SequentialLR scheduler (Warmup + Cosine Decay). \"\"\"\n",
    "        optimizer = AdamW(self.parameters(),\n",
    "                          lr=self.hparams.learning_rate,\n",
    "                          weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "        # Ensure setup has run and calculated steps_per_epoch\n",
    "        if self.steps_per_epoch == 0:\n",
    "             # If setup didn't run or failed (e.g. Trainer(fast_dev_run=True))\n",
    "             # Use a reasonable fallback, but warn the user.\n",
    "             self.steps_per_epoch = 500\n",
    "             print(f\"Warning: steps_per_epoch was 0 in configure_optimizers. Using fallback: {self.steps_per_epoch}. \"\n",
    "                   \"This might happen with fast_dev_run or if setup failed.\")\n",
    "\n",
    "\n",
    "        warmup_epochs = getattr(self, \"warmup_epochs\", getattr(self.hparams, \"warmup_epochs\", WARMUP_EPOCHS))\n",
    "        max_epochs = getattr(self, \"max_epochs\", getattr(self.hparams, \"max_epochs\", MAX_EPOCHS))\n",
    "\n",
    "        warmup_steps = self.steps_per_epoch * warmup_epochs\n",
    "        # Adjust total_steps calculation based on max_epochs potentially changing\n",
    "        if self.trainer and hasattr(self.trainer, \"max_epochs\") and self.trainer.max_epochs is not None:\n",
    "            max_epochs = self.trainer.max_epochs\n",
    "        total_steps = self.steps_per_epoch * max_epochs\n",
    "        decay_steps = total_steps - warmup_steps\n",
    "\n",
    "        # Ensure decay_steps is not negative if warmup_epochs >= max_epochs\n",
    "        decay_steps = max(1, decay_steps)\n",
    "\n",
    "        print(f\"Optimizer: AdamW, Initial LR: {self.hparams.learning_rate}\")\n",
    "        print(f\"Scheduler: SequentialLR(Warmup + CosineAnnealingLR)\")\n",
    "        print(f\"  Total estimated steps: {total_steps}\")\n",
    "        print(f\"  Warmup steps: {warmup_steps}\")\n",
    "        print(f\"  Cosine decay steps: {decay_steps}\")\n",
    "\n",
    "\n",
    "        scheduler_warmup = LambdaLR(\n",
    "            optimizer,\n",
    "            lr_lambda=linear_warmup_decay(warmup_steps)\n",
    "        )\n",
    "        scheduler_cosine = CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=decay_steps,\n",
    "            eta_min=1e-7 # Slightly higher minimum LR\n",
    "        )\n",
    "        lr_scheduler = SequentialLR(\n",
    "            optimizer,\n",
    "            schedulers=[scheduler_warmup, scheduler_cosine],\n",
    "            milestones=[warmup_steps]\n",
    "        )\n",
    "\n",
    "        scheduler_config = {\n",
    "            \"scheduler\": lr_scheduler,\n",
    "            \"interval\": \"step\",\n",
    "            \"frequency\": 1,\n",
    "        }\n",
    "\n",
    "        return [optimizer], [scheduler_config]\n",
    "\n",
    "########################################################\n",
    "# 3) Main Function to Run Training (Using PyTorch Schedulers)\n",
    "########################################################\n",
    "class BarlowTwinsProjectionHead(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim, bias=False),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, out_dim, bias=False),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class BarlowTwinsLoss(nn.Module):\n",
    "     def __init__(self, lambda_param=5e-3):\n",
    "         super().__init__()\n",
    "         self.lambda_param = lambda_param\n",
    "     def forward(self, z_a: torch.Tensor, z_b: torch.Tensor) -> torch.Tensor:\n",
    "         z_a_norm = (z_a - z_a.mean(0)) / (z_a.std(0) + 1e-6) # Add epsilon for stability\n",
    "         z_b_norm = (z_b - z_b.mean(0)) / (z_b.std(0) + 1e-6) # Add epsilon for stability\n",
    "         N = z_a.size(0)\n",
    "         D = z_a.size(1)\n",
    "         c = torch.mm(z_a_norm.T, z_b_norm) / N\n",
    "         c_diff = (c - torch.eye(D, device=c.device)).pow(2)\n",
    "         c_diff[~torch.eye(D, dtype=bool)] *= self.lambda_param\n",
    "         loss = c_diff.sum()\n",
    "         return loss\n",
    "\n",
    "# 1) Define Transforms\n",
    "transform = BYOLTransform(\n",
    "    view_1_transform=BYOLView1Transform(input_size=256, gaussian_blur=0.1),\n",
    "    view_2_transform=BYOLView2Transform(input_size=256, gaussian_blur=0.1),\n",
    ")\n",
    "\n",
    "# Use torchvision.transforms.Compose, not monai.transforms.Compose\n",
    "from torchvision.transforms import Compose\n",
    "transform = Compose([transform])\n",
    "\n",
    "# Convert ndarray to list for LightlyDataset compatibility\n",
    "images_paths_list = ssl_images_paths_np.tolist()\n",
    "# 2) Create Dataset\n",
    "dataset = LightlyDataset(\n",
    "    input_dir=INPUT_DIR,\n",
    "    transform=transform,\n",
    "    )\n",
    "    # Optionally, you can pass labels if needed for debugging or logging\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "\n",
    "# 3) Create DataLoader\n",
    "# Consider using pin_memory=True if using GPU and not seeing slowdowns\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    persistent_workers=True if NUM_WORKERS > 0 else False,\n",
    "    # pin_memory=True # Optional\n",
    ")\n",
    "\n",
    "# 4) Create the Barlow Twins model instance\n",
    "model = BarlowTwins(learning_rate=LEARNING_RATE,\n",
    "                    warmup_epochs=WARMUP_EPOCHS,\n",
    "                    max_epochs=MAX_EPOCHS)\n",
    "\n",
    "# 5) Setup Callbacks\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, TQDMProgressBar\n",
    "\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "progress_bar = TQDMProgressBar(refresh_rate=20)\n",
    "\n",
    "# 6) Train with PyTorch Lightning Trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    devices=1,\n",
    "    precision=\"16-mixed\",\n",
    "    gradient_clip_val=GRADIENT_CLIP_VAL,\n",
    "    callbacks=[lr_monitor, progress_bar],\n",
    "    # logger= # Add logger if desired\n",
    "    # log_every_n_steps=50\n",
    ")\n",
    "\n",
    "print(\"Starting Barlow Twins pre-training (using PyTorch LR schedulers)...\")\n",
    "# The trainer automatically calls model.setup('fit') before starting training\n",
    "trainer.fit(model=model, train_dataloaders=dataloader)\n",
    "print(\"Pre-training finished.\")\n",
    "\n",
    "# 7) After training, save ONLY the backbone\n",
    "save_dir = \".\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "backbone_save_path = os.path.join(save_dir, \"barlow_backbone_stable_pytorch_sched.pth\")\n",
    "\n",
    "torch.save(model.backbone.state_dict(), backbone_save_path)\n",
    "print(f\"Stable Barlow Twins pretraining completed. Saved backbone to '{backbone_save_path}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a63cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# # Launch the training job in the background and return immediately\n",
    "# nohup python -u barlow_tw_training.py > train.log 2>&1 &\n",
    "# echo \"Training started in background (PID=$!)  â€“  logs âžœ train.log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fd0bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# head -n 50 train.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834d5c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# list any train.py process that is still running\n",
    "# pgrep -af \"python .*barlow_tw_training.py\" || echo \"No train.py process found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961db6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # lightning VERSION NOT WORKING --> IT CREATES EXPLODING GRADIENTS\n",
    "# import pytorch_lightning as pl\n",
    "# import torchvision\n",
    "# from torch import nn\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# # Lightly imports\n",
    "# from lightly.data import LightlyDataset\n",
    "# from lightly.loss import BarlowTwinsLoss\n",
    "# from lightly.models.modules import BarlowTwinsProjectionHead\n",
    "# from lightly.transforms.byol_transform import (\n",
    "#     BYOLTransform, BYOLView1Transform, BYOLView2Transform\n",
    "# )\n",
    "# ########################################################\n",
    "# # 1) Custom Callback to Print Epoch Loss\n",
    "# ########################################################\n",
    "# class PrintEpochLossCallback(pl.Callback): #inherits from pl.Callback\n",
    "#     \"\"\"\n",
    "#     Prints the 'train_loss' metric (averaged over the epoch)\n",
    "#     after each epoch ends, on a new console line.\n",
    "#     \"\"\"\n",
    "#     def on_train_epoch_end(self, trainer, pl_module):\n",
    "#         epoch_loss = trainer.callback_metrics.get(\"train_loss\")\n",
    "#         current_epoch = trainer.current_epoch\n",
    "#         if epoch_loss is not None:\n",
    "#             print(f\"[Epoch {current_epoch}] => train_loss: {epoch_loss:.4f}\")\n",
    "#         else:\n",
    "#             print(f\"[Epoch {current_epoch}] => train_loss not found!\")\n",
    "\n",
    "\n",
    "# ########################################################\n",
    "# # 2) Barlow Twins LightningModule\n",
    "# ########################################################\n",
    "# class BarlowTwins(pl.LightningModule):\n",
    "#     def __init__(self):\n",
    "#         \"\"\"\n",
    "#         ResNet18 backbone -> 512-d features\n",
    "#         Projection head -> (512 -> 2048 -> 2048)\n",
    "#         BarlowTwinsLoss for self-supervised training\n",
    "#         \"\"\"\n",
    "#         super().__init__()\n",
    "#         # Create ResNet18, remove the final FC layer\n",
    "#         resnet = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.IMAGENET1K_V1) #TODO start using imagenet weights\n",
    "#         # bnuild the backbone by removing the final FC layer from the resnet\n",
    "#         self.backbone = nn.Sequential(*list(resnet.children())[:-1])  # shape (B,512,1,1)\n",
    "\n",
    "#         # Barlow Twins head: 512 -> 2048 -> 2048\n",
    "#         self.projection_head = BarlowTwinsProjectionHead(\n",
    "#             512,  # input dimension from resnet18\n",
    "#             2048,  # hidden dim\n",
    "#             2048   # output dim\n",
    "#         )\n",
    "#         self.criterion = BarlowTwinsLoss()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         \"\"\"\n",
    "#         Forward pass for one augmented view x.\n",
    "#         Returns the projected embedding (z).\n",
    "#         \"\"\"\n",
    "#         # backbone output shape: (B,512,1,1)\n",
    "#         x = self.backbone(x).flatten(start_dim=1)  # shape: (B,512)\n",
    "#         z = self.projection_head(x)               # shape: (B,2048)\n",
    "#         return z\n",
    "\n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         \"\"\"\n",
    "#         Barlow Twins requires two augmented views (x0, x1) of the same image.\n",
    "#         We'll produce embeddings for each and compute the correlation loss.\n",
    "#         \"\"\"\n",
    "#         (x0, x1) = batch[0]  # BYOLTransform returns a tuple: (x_query, x_key)\n",
    "#         z0 = self.forward(x0)\n",
    "#         z1 = self.forward(x1)\n",
    "\n",
    "#         loss = self.criterion(z0, z1)\n",
    "\n",
    "#         # Log the epoch-level average of 'train_loss'\n",
    "#         self.log(\"train_loss\", loss, on_step=False, on_epoch=True)\n",
    "#         return loss\n",
    "\n",
    "#     def configure_optimizers(self):\n",
    "#         \"\"\"\n",
    "#         Simple SGD. Adjust the learning rate for your data scale.\n",
    "#         \"\"\"\n",
    "#         optimizer = torch.optim.SGD(self.parameters(), lr=0.06, momentum=0.9, weight_decay=1e-4)\n",
    "#         return optimizer\n",
    "\n",
    "\n",
    "# ########################################################\n",
    "# # 3) Main Function to Run Training\n",
    "# ########################################################\n",
    "# # 1) Create the Barlow Twins model\n",
    "# model = BarlowTwins()\n",
    "# encoder_type = \"barlow_twins\"  # or \"moco\" if using MoCo\n",
    "# # 2) Define your transforms\n",
    "# #    We'll use the BYOL transforms with 256Ã—256 crops. \n",
    "# #    You can tune gaussian_blur, color jitter, etc.\n",
    "# transform = BYOLTransform(\n",
    "#     view_1_transform=BYOLView1Transform(input_size=256, gaussian_blur=0.1),\n",
    "#     view_2_transform=BYOLView2Transform(input_size=256, gaussian_blur=0.1),\n",
    "# )\n",
    "\n",
    "# # 3) Create a dataset from your folder of unlabeled images\n",
    "# #    e.g., \"path/to/unlabeled\" must contain .png, .jpg, etc.\n",
    "# dataset = LightlyDataset(\n",
    "#     input_dir=ssl_images_folder_path,\n",
    "#     transform=transform\n",
    "# )\n",
    "\n",
    "# # 4) Create a DataLoader\n",
    "# #    Adjust batch_size to fit your GPU memory. \n",
    "# #    If your dataset is very small, you can do batch_size=8 or 16.\n",
    "# dataloader = DataLoader(\n",
    "#     dataset,\n",
    "#     batch_size=32,\n",
    "#     shuffle=True,\n",
    "#     drop_last=True,  # ensures pairs are well-defined\n",
    "#     num_workers=4,\n",
    "# )\n",
    "\n",
    "# # 5) Train with PyTorch Lightning\n",
    "# #    We disable the standard progress bar and print each epoch's loss ourselves.\n",
    "# trainer = pl.Trainer(\n",
    "#     max_epochs=250,\n",
    "#     accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "#     devices=1,\n",
    "#     enable_progress_bar=False,\n",
    "#     callbacks=[PrintEpochLossCallback()],\n",
    "# )\n",
    "# trainer.fit(model=model, train_dataloaders=dataloader)\n",
    "\n",
    "# # 6) After training, you can save the ONLY the backbone\n",
    "# torch.save(model.backbone.state_dict(),\"barlow_backbone.pth\")\n",
    "# print(\"Barlow Twins pretraining completed. Saved backbone to 'barlow_backbone.pth'.\")\n",
    "# # Load the saved model\n",
    "# resnet = torchvision.models.resnet18(weights=None)\n",
    "#         # bnuild the backbone by removing the final FC layer from the resnet\n",
    "# backbone = nn.Sequential(*list(resnet.children())[:-1])  # shape (B,512,1,1)\n",
    "# backbone.load_state_dict(torch.load(\"barlow_backbone.pth\"))\n",
    "\n",
    "# print(\"Checking pre-trained backbone weights...\")\n",
    "# try:\n",
    "#     # Recreate the backbone structure\n",
    "#     resnet = torchvision.models.resnet18(weights=None)\n",
    "#     backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "#     state_dict = torch.load(\"barlow_backbone.pth\", map_location=torch.device('cpu')) # Load to CPU\n",
    "#     backbone.load_state_dict(state_dict)\n",
    "\n",
    "#     found_nan_inf = False\n",
    "#     for name, param in backbone.named_parameters():\n",
    "#         if torch.isnan(param).any() or torch.isinf(param).any():\n",
    "#             print(f\"!!! NaN or Inf found in backbone parameter: {name} !!!\")\n",
    "#             found_nan_inf = True\n",
    "\n",
    "#     if not found_nan_inf:\n",
    "#         print(\"Pre-trained backbone weights seem OK (no NaNs/Infs numerically).\")\n",
    "#         # If weights are OK, check their magnitude\n",
    "#         max_abs_weight = 0.0\n",
    "#         for name, param in backbone.named_parameters():\n",
    "#              max_abs_weight = max(max_abs_weight, param.abs().max().item())\n",
    "#         print(f\"Maximum absolute weight value in backbone: {max_abs_weight}\") # If this is huge (e.g., >1000), it indicates instability.\n",
    "\n",
    "#     else:\n",
    "#         print(\"!!! PROBLEM: Pre-trained backbone contains NaN/Inf weights! File is corrupt. !!!\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error loading or checking backbone weights: {e}\")           \n",
    "# # print(\"Barlow Twins pretraining completed. Saved backbone to 'barlow_backbone.pth'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4961f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import torchvision, copy, torch.nn as nn\n",
    "\n",
    "# barlow_backbone = nn.Sequential(*list(torchvision.models.resnet18(weights=None).children())[:-1])\n",
    "# _ = barlow_backbone.load_state_dict(torch.load(\"barlow_backbone.pth\"))\n",
    "# barlow_backbone.eval()          # good practice; will be copied later\n",
    "\n",
    "\n",
    "# resnet = torchvision.models.resnet18(weights=None)\n",
    "#         # bnuild the backbone by removing the final FC layer from the resnet\n",
    "# backbone = nn.Sequential(*list(resnet.children())[:-1])  # shape (B,512,1,1)\n",
    "# backbone.load_state_dict(torch.load(\"barlow_backbone.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73bafdf",
   "metadata": {},
   "source": [
    "## loading pre-trained encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "980ed6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Creating ResNet-18 backbone architecture (weights=None)...\n",
      "   Architecture created.\n",
      "\n",
      "2. Loading state dictionary from: ./barlow_backbone_stable_pytorch_sched.pth\n",
      "   State dictionary loaded from file.\n",
      "\n",
      "3. Loading state dictionary into the backbone model instance...\n",
      "   Successfully loaded pre-trained weights into the backbone.\n"
     ]
    }
   ],
   "source": [
    "STABLE_BACKBONE_PATH = backbone_save_path\n",
    "FEATURE_DIM = 512 # Output dimension of ResNet-18 backbone\n",
    "\n",
    "# 1. Create the backbone architecture instance\n",
    "#   Initialize with weights=None because we are loading our specific pre-trained weights.\n",
    "print(\"1. Creating ResNet-18 backbone architecture (weights=None)...\")\n",
    "base_resnet = torchvision.models.resnet18(weights=None)\n",
    "# The architecture saved was the ResNet excluding the final fully connected layer\n",
    "pretrained_backbone = nn.Sequential(*list(base_resnet.children())[:-1])\n",
    "print(\"   Architecture created.\")\n",
    "\n",
    "# 2. Load the saved state dictionary\n",
    "print(f\"\\n2. Loading state dictionary from: {STABLE_BACKBONE_PATH}\")\n",
    "try:\n",
    "    # Load to CPU first for flexibility, we'll move it to the correct device later\n",
    "    state_dict = torch.load(STABLE_BACKBONE_PATH, map_location=torch.device('cpu'))\n",
    "    print(\"   State dictionary loaded from file.\")\n",
    "    # 3. Load the state_dict into the architecture instance\n",
    "    print(\"\\n3. Loading state dictionary into the backbone model instance...\")\n",
    "    pretrained_backbone.load_state_dict(state_dict)\n",
    "    print(\"   Successfully loaded pre-trained weights into the backbone.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"   ERROR: Weight file not found at '{STABLE_BACKBONE_PATH}'.\")\n",
    "    print(\"   Please ensure the file exists and the path is correct.\")\n",
    "    # Handle the error appropriately - maybe exit or raise\n",
    "    exit()\n",
    "\n",
    "torch.manual_seed(42)  # For reproducibility\n",
    "randomly_initialized_backbone = torchvision.models.resnet18(weights=None)\n",
    "randomly_initialized_backbone = nn.Sequential(*list(randomly_initialized_backbone.children())[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dcb6924e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded from /home/zano/Documents/TESI/TESI/configs/3c/base.yaml\n",
      "Configuration: {'data_splitting': {'random_seed': 42, 'val_set_size': 0.2, 'test_set_size': 0.1, 'num_folds': 6}, 'data_augmentation': {'resize_spatial_size': [256, 256], 'rand_flip_prob': 0.3, 'rand_flip_spatial_axes': [0, 1], 'rand_rotate90_prob': 0.3, 'rand_rotate90_max_k': 3, 'rand_gaussian_noise_prob': 0.4, 'rand_gaussian_noise_mean': 0.0, 'rand_gaussian_noise_std': 0.1}, 'data_loading': {'batch_size': 8, 'num_workers': 0}, 'model': {'model_name': 'base', 'spatial_dims': 2, 'in_channels': 3, 'out_channels': 2, 'dropout_prob': 0.1}, 'training': {'num_epochs': 50, 'early_stopping_patience': 17, 'mixup_alpha': 0, 'oversample': True, 'undersample': False, 'weighted_loss': False, 'fine_tuning': False, 'transfer_learning': True, 'freezed_layerIndex': None}, 'optimizer': {'learning_rate': '1e-4', 'optimizer_name': 'Adam', 'weight_decay': '2e-5'}, 'scheduler': {'scheduler_name': 'ReduceLROnPlateau', 'factor': 0.5, 'scheduler_patience': 9, 'threshold': '1e-4', 'min_lr': '1e-8'}}\n",
      "Configuration loaded from /home/zano/Documents/TESI/TESI/configs/3c/resnet18.yaml\n",
      "Configuration: {'data_splitting': {'random_seed': 42, 'val_set_size': 0.17, 'test_set_size': 0.1, 'num_folds': 6}, 'data_augmentation': {'resize_spatial_size': [256, 256], 'rand_flip_prob': 0.3, 'rand_flip_spatial_axes': [0, 1], 'rand_rotate90_prob': 0.3, 'rand_rotate90_max_k': 3, 'rand_gaussian_noise_prob': 0.4, 'rand_gaussian_noise_mean': 0.0, 'rand_gaussian_noise_std': 0.1, 'use_color_transforms': False}, 'data_loading': {'batch_size': 16, 'num_workers': 0}, 'model': {'model_name': 'Resnet18', 'spatial_dims': 2, 'in_channels': 3, 'out_channels': 2, 'dropout_prob': 0.1}, 'training': {'num_epochs': 200, 'early_stopping_patience': 60, 'mixup_alpha': 0, 'oversample': True, 'undersample': False, 'weighted_loss': False, 'fine_tuning': False, 'transfer_learning': False, 'freezed_layerIndex': None, 'pretrained': True}, 'optimizer': {'learning_rate': '2e-4', 'optimizer_name': 'AdamW', 'weight_decay': '2e-4', 'patience': 30}, 'scheduler': {'scheduler_name': 'ReduceLROnPlateau', 'factor': 0.5, 'scheduler_patience': 9, 'threshold': '1e-4', 'min_lr': '1e-8'}, 'num_epochs': None, 'freeze_layers': None}\n"
     ]
    }
   ],
   "source": [
    "from configs.ConfigLoader import ConfigLoader\n",
    "import utils.transformations_functions as tf\n",
    "from classes.ModelManager import ModelManager\n",
    "\n",
    "yaml_path = f\"/home/zano/Documents/TESI/TESI/configs/{num_input_channels}c/resnet18.yaml\"\n",
    "cfg = ConfigLoader(yaml_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "748b1a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MSA', 'PD']\n"
     ]
    }
   ],
   "source": [
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f92ca943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 2 unique classes.\n",
      "\n",
      "===== OUTER FOLD 1 / 6 =====\n",
      "Outer Train images: 115 | Outer Test images: 25\n",
      "--- Calculating normalization stats for Fold 1 Training Data ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-30 15:09:19,669] A new study created in memory with name: no-name-74fd2290-a5b7-4228-9679-672c3d626b82\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 stats: {'mean': [0.028778579086065292, 0.010931559838354588, 0.07891840487718582], 'std': [0.05348258465528488, 0.01670260727405548, 0.08365660905838013]}\n",
      "--- Generating data transforms for Fold 1 ---\n",
      "Using pretrained model: False\n",
      "Using pretrained model: False and supported by torchvision: <function is_supported_by_torchvision at 0x72cd7d419440> with color transforms: False\n",
      "the model is not supported by torchvision or is not pretrained\n",
      "Model Resnet18 not supported using custom transforms\n",
      "Applying fold-specific normalization with mean: [0.028778579086065292, 0.010931559838354588, 0.07891840487718582], std: [0.05348258465528488, 0.01670260727405548, 0.08365660905838013]\n",
      "Applying fold-specific normalization to validation data with mean: [0.028778579086065292, 0.010931559838354588, 0.07891840487718582], std: [0.05348258465528488, 0.01670260727405548, 0.08365660905838013]\n",
      "Transforms generated for Fold 1.\n",
      "--- Starting Hyperparameter Tuning for Fold 1 ---\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-30 15:09:28,737] Trial 0 finished with value: 0.67943175137043 and parameters: {'lr': 4.715696678089837e-05}. Best is trial 0 with value: 0.67943175137043.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-30 15:09:37,582] Trial 1 finished with value: 17.78900391422212 and parameters: {'lr': 0.0014886262201211794}. Best is trial 0 with value: 0.67943175137043.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-30 15:09:46,310] Trial 2 finished with value: 0.862353190779686 and parameters: {'lr': 0.0004014783718209777}. Best is trial 0 with value: 0.67943175137043.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Best LR from inner CV = 0.000047\n",
      "--- Starting Final Model Training for Fold 1 with LR=0.000047 ---\n",
      "X_train_es: (95,) | X_val_es: (20,)\n",
      "Early stopping split: Train images: 95, Validation images: 20\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "===========================\n",
      "Model Architecture:\n",
      "==================\n",
      "Total parameters: 11,177,538\n",
      "Trainable parameters: 11,177,538\n",
      "Non-trainable parameters: 0\n",
      "===========================\n",
      " Fold 1 Epoch 1/200: Tr L: 0.7788, Tr Acc: 0.5000, Val L: 0.5590, Val Acc: 0.6500, Val F1: 0.5333 lr: 0.000047\n",
      " Fold 1 Epoch 2/200: Tr L: 0.6596, Tr Acc: 0.6000, Val L: 0.7638, Val Acc: 0.4500, Val F1: 0.5926 lr: 0.000047\n",
      " Fold 1 Epoch 3/200: Tr L: 0.5889, Tr Acc: 0.6909, Val L: 0.6103, Val Acc: 0.7500, Val F1: 0.7368 lr: 0.000047\n",
      " Fold 1 Epoch 4/200: Tr L: 0.5498, Tr Acc: 0.7455, Val L: 0.5342, Val Acc: 0.8000, Val F1: 0.8000 lr: 0.000047\n",
      " Fold 1 Epoch 5/200: Tr L: 0.4823, Tr Acc: 0.7909, Val L: 0.6576, Val Acc: 0.6500, Val F1: 0.6957 lr: 0.000047\n",
      " Fold 1 Epoch 6/200: Tr L: 0.4299, Tr Acc: 0.8182, Val L: 0.4191, Val Acc: 0.7500, Val F1: 0.7368 lr: 0.000047\n",
      " Fold 1 Epoch 7/200: Tr L: 0.4300, Tr Acc: 0.8091, Val L: 0.3935, Val Acc: 0.7000, Val F1: 0.6667 lr: 0.000047\n",
      " Fold 1 Epoch 8/200: Tr L: 0.4611, Tr Acc: 0.8000, Val L: 0.6955, Val Acc: 0.7000, Val F1: 0.7273 lr: 0.000047\n",
      " Fold 1 Epoch 9/200: Tr L: 0.3379, Tr Acc: 0.8455, Val L: 0.4527, Val Acc: 0.7500, Val F1: 0.7368 lr: 0.000047\n",
      " Fold 1 Epoch 10/200: Tr L: 0.3698, Tr Acc: 0.8636, Val L: 1.8489, Val Acc: 0.5000, Val F1: 0.5833 lr: 0.000047\n",
      " Fold 1 Epoch 11/200: Tr L: 0.4568, Tr Acc: 0.8364, Val L: 0.4474, Val Acc: 0.7500, Val F1: 0.7059 lr: 0.000047\n",
      " Fold 1 Epoch 12/200: Tr L: 0.3705, Tr Acc: 0.8545, Val L: 0.6230, Val Acc: 0.7000, Val F1: 0.7273 lr: 0.000047\n",
      " Fold 1 Epoch 13/200: Tr L: 0.3686, Tr Acc: 0.8182, Val L: 0.4161, Val Acc: 0.7000, Val F1: 0.6667 lr: 0.000047\n",
      " Fold 1 Epoch 14/200: Tr L: 0.2813, Tr Acc: 0.8909, Val L: 0.4408, Val Acc: 0.7000, Val F1: 0.6667 lr: 0.000047\n",
      " Fold 1 Epoch 15/200: Tr L: 0.2719, Tr Acc: 0.8545, Val L: 0.3784, Val Acc: 0.7500, Val F1: 0.7059 lr: 0.000047\n",
      " Fold 1 Epoch 16/200: Tr L: 0.2055, Tr Acc: 0.9364, Val L: 1.2456, Val Acc: 0.6000, Val F1: 0.6364 lr: 0.000047\n",
      " Fold 1 Epoch 17/200: Tr L: 0.2557, Tr Acc: 0.8727, Val L: 0.4437, Val Acc: 0.7500, Val F1: 0.7059 lr: 0.000047\n",
      " Fold 1 Epoch 18/200: Tr L: 0.2649, Tr Acc: 0.8909, Val L: 0.4544, Val Acc: 0.7000, Val F1: 0.6667 lr: 0.000047\n",
      " Fold 1 Epoch 19/200: Tr L: 0.3789, Tr Acc: 0.8182, Val L: 4.1915, Val Acc: 0.4500, Val F1: 0.5926 lr: 0.000047\n",
      " Fold 1 Epoch 20/200: Tr L: 0.3278, Tr Acc: 0.8273, Val L: 0.4305, Val Acc: 0.7000, Val F1: 0.5714 lr: 0.000047\n",
      " Fold 1 Epoch 21/200: Tr L: 0.3045, Tr Acc: 0.8545, Val L: 0.8428, Val Acc: 0.5500, Val F1: 0.5714 lr: 0.000047\n",
      " Fold 1 Epoch 22/200: Tr L: 0.2308, Tr Acc: 0.9091, Val L: 1.1778, Val Acc: 0.6500, Val F1: 0.6957 lr: 0.000047\n",
      " Fold 1 Epoch 23/200: Tr L: 0.2339, Tr Acc: 0.9182, Val L: 0.5874, Val Acc: 0.7500, Val F1: 0.7619 lr: 0.000047\n",
      " Fold 1 Epoch 24/200: Tr L: 0.2370, Tr Acc: 0.9182, Val L: 0.5505, Val Acc: 0.8000, Val F1: 0.8000 lr: 0.000047\n",
      " Fold 1 Epoch 25/200: Tr L: 0.2983, Tr Acc: 0.8636, Val L: 0.6516, Val Acc: 0.8000, Val F1: 0.8000 lr: 0.000047\n",
      " Fold 1 Epoch 26/200: Tr L: 0.2672, Tr Acc: 0.8818, Val L: 0.5831, Val Acc: 0.7500, Val F1: 0.7368 lr: 0.000047\n",
      " Fold 1 Epoch 27/200: Tr L: 0.1526, Tr Acc: 0.9364, Val L: 1.7236, Val Acc: 0.6000, Val F1: 0.6667 lr: 0.000047\n",
      " Fold 1 Epoch 28/200: Tr L: 0.2090, Tr Acc: 0.9273, Val L: 1.5114, Val Acc: 0.5500, Val F1: 0.6400 lr: 0.000047\n",
      " Fold 1 Epoch 29/200: Tr L: 0.1547, Tr Acc: 0.9545, Val L: 0.5923, Val Acc: 0.6500, Val F1: 0.6667 lr: 0.000047\n",
      " Fold 1 Epoch 30/200: Tr L: 0.2607, Tr Acc: 0.8909, Val L: 0.6775, Val Acc: 0.6500, Val F1: 0.5333 lr: 0.000047\n",
      " Fold 1 Epoch 31/200: Tr L: 0.2293, Tr Acc: 0.8636, Val L: 4.3722, Val Acc: 0.5000, Val F1: 0.6154 lr: 0.000047\n",
      " Fold 1 Epoch 32/200: Tr L: 0.1797, Tr Acc: 0.9455, Val L: 1.4406, Val Acc: 0.6500, Val F1: 0.6957 lr: 0.000047\n",
      " Fold 1 Epoch 33/200: Tr L: 0.1623, Tr Acc: 0.9091, Val L: 0.5510, Val Acc: 0.7500, Val F1: 0.7368 lr: 0.000047\n",
      " Fold 1 Epoch 34/200: Tr L: 0.1075, Tr Acc: 0.9636, Val L: 0.5326, Val Acc: 0.7500, Val F1: 0.7368 lr: 0.000047\n",
      " Fold 1 Epoch 35/200: Tr L: 0.2699, Tr Acc: 0.8909, Val L: 0.6834, Val Acc: 0.7000, Val F1: 0.7000 lr: 0.000047\n",
      " Fold 1 Epoch 36/200: Tr L: 0.0947, Tr Acc: 0.9818, Val L: 2.0606, Val Acc: 0.5500, Val F1: 0.6400 lr: 0.000047\n",
      " Fold 1 Epoch 37/200: Tr L: 0.2102, Tr Acc: 0.9091, Val L: 4.8097, Val Acc: 0.5000, Val F1: 0.6154 lr: 0.000047\n",
      " Fold 1 Epoch 38/200: Tr L: 0.1632, Tr Acc: 0.9364, Val L: 1.1267, Val Acc: 0.7000, Val F1: 0.7000 lr: 0.000047\n",
      " Fold 1 Epoch 39/200: Tr L: 0.2580, Tr Acc: 0.9000, Val L: 3.7582, Val Acc: 0.6000, Val F1: 0.6667 lr: 0.000047\n",
      " Fold 1 Epoch 40/200: Tr L: 0.2471, Tr Acc: 0.9273, Val L: 1.9586, Val Acc: 0.7000, Val F1: 0.7273 lr: 0.000047\n",
      " Fold 1 Epoch 41/200: Tr L: 0.0908, Tr Acc: 0.9727, Val L: 0.5530, Val Acc: 0.7000, Val F1: 0.5000 lr: 0.000047\n",
      " Fold 1 Epoch 42/200: Tr L: 0.2460, Tr Acc: 0.9000, Val L: 0.6361, Val Acc: 0.6000, Val F1: 0.5000 lr: 0.000047\n",
      " Fold 1 Epoch 43/200: Tr L: 0.2746, Tr Acc: 0.9182, Val L: 3.0000, Val Acc: 0.5500, Val F1: 0.6087 lr: 0.000047\n",
      " Fold 1 Epoch 44/200: Tr L: 0.1821, Tr Acc: 0.9273, Val L: 1.0381, Val Acc: 0.6000, Val F1: 0.5556 lr: 0.000047\n",
      " Fold 1 Epoch 45/200: Tr L: 0.3195, Tr Acc: 0.9273, Val L: 1.8715, Val Acc: 0.6000, Val F1: 0.6000 lr: 0.000047\n",
      " Fold 1 Epoch 46/200: Tr L: 0.1940, Tr Acc: 0.9273, Val L: 0.7273, Val Acc: 0.6000, Val F1: 0.5000 lr: 0.000047\n",
      " Fold 1 Epoch 47/200: Tr L: 0.2493, Tr Acc: 0.9000, Val L: 1.3052, Val Acc: 0.6500, Val F1: 0.6667 lr: 0.000024\n",
      " Fold 1 Epoch 48/200: Tr L: 0.0917, Tr Acc: 0.9545, Val L: 3.4110, Val Acc: 0.5500, Val F1: 0.6400 lr: 0.000024\n",
      " Fold 1 Epoch 49/200: Tr L: 0.1237, Tr Acc: 0.9545, Val L: 2.5991, Val Acc: 0.5500, Val F1: 0.6087 lr: 0.000024\n",
      " Fold 1 Epoch 50/200: Tr L: 0.1010, Tr Acc: 0.9636, Val L: 1.4289, Val Acc: 0.5500, Val F1: 0.5714 lr: 0.000024\n",
      " Fold 1 Epoch 51/200: Tr L: 0.1931, Tr Acc: 0.9182, Val L: 0.9819, Val Acc: 0.6000, Val F1: 0.6000 lr: 0.000024\n",
      " Fold 1 Epoch 52/200: Tr L: 0.2442, Tr Acc: 0.9182, Val L: 2.0585, Val Acc: 0.6000, Val F1: 0.6364 lr: 0.000024\n",
      " Fold 1 Epoch 53/200: Tr L: 0.0828, Tr Acc: 0.9818, Val L: 1.9203, Val Acc: 0.6000, Val F1: 0.6364 lr: 0.000024\n",
      " Fold 1 Epoch 54/200: Tr L: 0.1658, Tr Acc: 0.9364, Val L: 1.1180, Val Acc: 0.6000, Val F1: 0.6000 lr: 0.000024\n",
      " Fold 1 Epoch 55/200: Tr L: 0.0888, Tr Acc: 0.9545, Val L: 0.6528, Val Acc: 0.5500, Val F1: 0.4000 lr: 0.000024\n",
      " Fold 1 Epoch 56/200: Tr L: 0.1870, Tr Acc: 0.9364, Val L: 1.6788, Val Acc: 0.6500, Val F1: 0.6667 lr: 0.000024\n",
      " Fold 1 Epoch 57/200: Tr L: 0.1328, Tr Acc: 0.9273, Val L: 3.5808, Val Acc: 0.5500, Val F1: 0.6400 lr: 0.000024\n",
      " Fold 1 Epoch 58/200: Tr L: 0.2245, Tr Acc: 0.8818, Val L: 3.3361, Val Acc: 0.5000, Val F1: 0.5833 lr: 0.000024\n",
      " Fold 1 Epoch 59/200: Tr L: 0.1545, Tr Acc: 0.9636, Val L: 1.8455, Val Acc: 0.6000, Val F1: 0.6000 lr: 0.000024\n",
      " Fold 1 Epoch 60/200: Tr L: 0.1695, Tr Acc: 0.9182, Val L: 2.2381, Val Acc: 0.5500, Val F1: 0.6087 lr: 0.000024\n",
      " Fold 1 Epoch 61/200: Tr L: 0.1888, Tr Acc: 0.9273, Val L: 2.3907, Val Acc: 0.6000, Val F1: 0.6667 lr: 0.000024\n",
      " Fold 1 Epoch 62/200: Tr L: 0.1198, Tr Acc: 0.9455, Val L: 2.4552, Val Acc: 0.6500, Val F1: 0.6957 lr: 0.000024\n",
      " Fold 1 Epoch 63/200: Tr L: 0.1357, Tr Acc: 0.9273, Val L: 1.0045, Val Acc: 0.7000, Val F1: 0.6667 lr: 0.000024\n",
      " Fold 1 Epoch 64/200: Tr L: 0.1516, Tr Acc: 0.9273, Val L: 0.9732, Val Acc: 0.5500, Val F1: 0.4000 lr: 0.000024\n",
      " Fold 1 Epoch 65/200: Tr L: 0.1021, Tr Acc: 0.9545, Val L: 2.0697, Val Acc: 0.7000, Val F1: 0.7273 lr: 0.000024\n",
      " Fold 1 Epoch 66/200: Tr L: 0.2654, Tr Acc: 0.9182, Val L: 3.1452, Val Acc: 0.5000, Val F1: 0.6154 lr: 0.000024\n",
      " Fold 1 Epoch 67/200: Tr L: 0.1902, Tr Acc: 0.9273, Val L: 2.0789, Val Acc: 0.6000, Val F1: 0.6364 lr: 0.000024\n",
      " Fold 1 Epoch 68/200: Tr L: 0.1051, Tr Acc: 0.9636, Val L: 1.2333, Val Acc: 0.7000, Val F1: 0.7000 lr: 0.000024\n",
      " Fold 1 Epoch 69/200: Tr L: 0.1085, Tr Acc: 0.9364, Val L: 2.6606, Val Acc: 0.5500, Val F1: 0.6087 lr: 0.000024\n",
      " Fold 1 Epoch 70/200: Tr L: 0.0939, Tr Acc: 0.9818, Val L: 3.9405, Val Acc: 0.5500, Val F1: 0.6400 lr: 0.000024\n",
      " Fold 1 Epoch 71/200: Tr L: 0.1862, Tr Acc: 0.9455, Val L: 2.3876, Val Acc: 0.6500, Val F1: 0.6957 lr: 0.000024\n",
      " Fold 1 Epoch 72/200: Tr L: 0.1244, Tr Acc: 0.9455, Val L: 1.5901, Val Acc: 0.6500, Val F1: 0.6316 lr: 0.000024\n",
      " Fold 1 Epoch 73/200: Tr L: 0.2246, Tr Acc: 0.9182, Val L: 1.9557, Val Acc: 0.7000, Val F1: 0.7273 lr: 0.000024\n",
      " Fold 1 Epoch 74/200: Tr L: 0.0863, Tr Acc: 0.9636, Val L: 2.2151, Val Acc: 0.7000, Val F1: 0.7273 lr: 0.000024\n",
      " Fold 1 Epoch 75/200: Tr L: 0.2204, Tr Acc: 0.9091, Val L: 2.2167, Val Acc: 0.7000, Val F1: 0.7000 lr: 0.000024\n",
      "Early stopping triggered at epoch 75 for fold 1\n",
      "--- Evaluating Fold 1 on Outer Test Set ---\n",
      "Warning: model_factory used without specific LR, using default/cfg LR: 0.001\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Test set class counts for fold 1: {0: 16, 1: 9}\n",
      "percentage of classes in test set: 0    0.64\n",
      "1    0.36\n",
      "Name: count, dtype: float64\n",
      " [FOLD 1 FINAL] Test Loss: 0.5910 | Test Acc: 0.8000 | test Balanced Acc: 0.7708 | test F1: 0.7059 | Test AUC: 1.0000\n",
      "model class name: SSLClassifierModule\n",
      "model class name: SSLClassifierModule\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-30 15:10:45,717] A new study created in memory with name: no-name-d44a4a1b-d0bd-4f38-b0e0-03e2f6cfc0a2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== OUTER FOLD 2 / 6 =====\n",
      "Outer Train images: 114 | Outer Test images: 26\n",
      "--- Calculating normalization stats for Fold 2 Training Data ---\n",
      "Fold 2 stats: {'mean': [0.028827723115682602, 0.010728755034506321, 0.07943334430456161], 'std': [0.055495839565992355, 0.015906041488051414, 0.08466564863920212]}\n",
      "--- Generating data transforms for Fold 2 ---\n",
      "Using pretrained model: False\n",
      "Using pretrained model: False and supported by torchvision: <function is_supported_by_torchvision at 0x72cd7d419440> with color transforms: False\n",
      "the model is not supported by torchvision or is not pretrained\n",
      "Model Resnet18 not supported using custom transforms\n",
      "Applying fold-specific normalization with mean: [0.028827723115682602, 0.010728755034506321, 0.07943334430456161], std: [0.055495839565992355, 0.015906041488051414, 0.08466564863920212]\n",
      "Applying fold-specific normalization to validation data with mean: [0.028827723115682602, 0.010728755034506321, 0.07943334430456161], std: [0.055495839565992355, 0.015906041488051414, 0.08466564863920212]\n",
      "Transforms generated for Fold 2.\n",
      "--- Starting Hyperparameter Tuning for Fold 2 ---\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-30 15:10:54,978] Trial 0 finished with value: 0.713635116815567 and parameters: {'lr': 4.715696678089837e-05}. Best is trial 0 with value: 0.713635116815567.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-30 15:11:04,304] Trial 1 finished with value: 5.643018376082182 and parameters: {'lr': 0.0014886262201211794}. Best is trial 0 with value: 0.713635116815567.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-30 15:11:13,791] Trial 2 finished with value: 0.7172489874064922 and parameters: {'lr': 0.0004014783718209777}. Best is trial 0 with value: 0.713635116815567.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Best LR from inner CV = 0.000047\n",
      "--- Starting Final Model Training for Fold 2 with LR=0.000047 ---\n",
      "X_train_es: (94,) | X_val_es: (20,)\n",
      "Early stopping split: Train images: 94, Validation images: 20\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "===========================\n",
      "Model Architecture:\n",
      "==================\n",
      "Total parameters: 11,177,538\n",
      "Trainable parameters: 11,177,538\n",
      "Non-trainable parameters: 0\n",
      "===========================\n",
      " Fold 2 Epoch 1/200: Tr L: 0.6915, Tr Acc: 0.5175, Val L: 1.1603, Val Acc: 0.4000, Val F1: 0.5714 lr: 0.000047\n",
      " Fold 2 Epoch 2/200: Tr L: 0.6174, Tr Acc: 0.6667, Val L: 0.6425, Val Acc: 0.4500, Val F1: 0.5217 lr: 0.000047\n",
      " Fold 2 Epoch 3/200: Tr L: 0.6025, Tr Acc: 0.6930, Val L: 0.7069, Val Acc: 0.5000, Val F1: 0.5833 lr: 0.000047\n",
      " Fold 2 Epoch 4/200: Tr L: 0.5821, Tr Acc: 0.6667, Val L: 0.4464, Val Acc: 0.6500, Val F1: 0.6316 lr: 0.000047\n",
      " Fold 2 Epoch 5/200: Tr L: 0.5112, Tr Acc: 0.7895, Val L: 0.4114, Val Acc: 0.6000, Val F1: 0.6000 lr: 0.000047\n",
      " Fold 2 Epoch 6/200: Tr L: 0.4848, Tr Acc: 0.7632, Val L: 0.3963, Val Acc: 0.6000, Val F1: 0.4286 lr: 0.000047\n",
      " Fold 2 Epoch 7/200: Tr L: 0.4429, Tr Acc: 0.7719, Val L: 0.4540, Val Acc: 0.6500, Val F1: 0.6667 lr: 0.000047\n",
      " Fold 2 Epoch 8/200: Tr L: 0.3568, Tr Acc: 0.8509, Val L: 0.4296, Val Acc: 0.6500, Val F1: 0.5882 lr: 0.000047\n",
      " Fold 2 Epoch 9/200: Tr L: 0.4397, Tr Acc: 0.7982, Val L: 0.5178, Val Acc: 0.7000, Val F1: 0.6667 lr: 0.000047\n",
      " Fold 2 Epoch 10/200: Tr L: 0.3548, Tr Acc: 0.8333, Val L: 0.6455, Val Acc: 0.5000, Val F1: 0.1667 lr: 0.000047\n",
      " Fold 2 Epoch 11/200: Tr L: 0.4002, Tr Acc: 0.8158, Val L: 0.9592, Val Acc: 0.6500, Val F1: 0.6957 lr: 0.000047\n",
      " Fold 2 Epoch 12/200: Tr L: 0.3262, Tr Acc: 0.8333, Val L: 0.5643, Val Acc: 0.7500, Val F1: 0.6154 lr: 0.000047\n",
      " Fold 2 Epoch 13/200: Tr L: 0.4685, Tr Acc: 0.8333, Val L: 0.7341, Val Acc: 0.6500, Val F1: 0.6316 lr: 0.000047\n",
      " Fold 2 Epoch 14/200: Tr L: 0.3318, Tr Acc: 0.8772, Val L: 0.9148, Val Acc: 0.7000, Val F1: 0.7000 lr: 0.000047\n",
      " Fold 2 Epoch 15/200: Tr L: 0.3353, Tr Acc: 0.8509, Val L: 0.6970, Val Acc: 0.5500, Val F1: 0.1818 lr: 0.000047\n",
      " Fold 2 Epoch 16/200: Tr L: 0.2693, Tr Acc: 0.8684, Val L: 0.5879, Val Acc: 0.6500, Val F1: 0.5882 lr: 0.000047\n",
      " Fold 2 Epoch 17/200: Tr L: 0.2416, Tr Acc: 0.8947, Val L: 0.9888, Val Acc: 0.6000, Val F1: 0.6364 lr: 0.000047\n",
      " Fold 2 Epoch 18/200: Tr L: 0.3367, Tr Acc: 0.8158, Val L: 0.8551, Val Acc: 0.7000, Val F1: 0.6667 lr: 0.000047\n",
      " Fold 2 Epoch 19/200: Tr L: 0.3263, Tr Acc: 0.8860, Val L: 0.5988, Val Acc: 0.6500, Val F1: 0.5333 lr: 0.000047\n",
      " Fold 2 Epoch 20/200: Tr L: 0.3291, Tr Acc: 0.8947, Val L: 0.6816, Val Acc: 0.6000, Val F1: 0.5556 lr: 0.000047\n",
      " Fold 2 Epoch 21/200: Tr L: 0.3103, Tr Acc: 0.9123, Val L: 0.9020, Val Acc: 0.6500, Val F1: 0.6667 lr: 0.000047\n",
      " Fold 2 Epoch 22/200: Tr L: 0.2288, Tr Acc: 0.9035, Val L: 0.7584, Val Acc: 0.6500, Val F1: 0.6316 lr: 0.000047\n",
      " Fold 2 Epoch 23/200: Tr L: 0.4053, Tr Acc: 0.9298, Val L: 0.6513, Val Acc: 0.7500, Val F1: 0.6154 lr: 0.000047\n",
      " Fold 2 Epoch 24/200: Tr L: 0.4785, Tr Acc: 0.9035, Val L: 0.8135, Val Acc: 0.6000, Val F1: 0.3333 lr: 0.000047\n",
      " Fold 2 Epoch 25/200: Tr L: 0.2739, Tr Acc: 0.8772, Val L: 1.2347, Val Acc: 0.6500, Val F1: 0.6316 lr: 0.000047\n",
      " Fold 2 Epoch 26/200: Tr L: 0.1351, Tr Acc: 0.9474, Val L: 0.8638, Val Acc: 0.5500, Val F1: 0.4706 lr: 0.000047\n",
      " Fold 2 Epoch 27/200: Tr L: 0.2658, Tr Acc: 0.9298, Val L: 1.0836, Val Acc: 0.6000, Val F1: 0.5556 lr: 0.000047\n",
      " Fold 2 Epoch 28/200: Tr L: 0.1700, Tr Acc: 0.8947, Val L: 0.9786, Val Acc: 0.6000, Val F1: 0.5000 lr: 0.000047\n",
      " Fold 2 Epoch 29/200: Tr L: 0.3135, Tr Acc: 0.9298, Val L: 1.1956, Val Acc: 0.6000, Val F1: 0.5000 lr: 0.000047\n",
      " Fold 2 Epoch 30/200: Tr L: 0.3572, Tr Acc: 0.9386, Val L: 1.4472, Val Acc: 0.6000, Val F1: 0.6000 lr: 0.000047\n",
      " Fold 2 Epoch 31/200: Tr L: 0.1443, Tr Acc: 0.9474, Val L: 0.9570, Val Acc: 0.6000, Val F1: 0.5000 lr: 0.000047\n",
      " Fold 2 Epoch 32/200: Tr L: 0.5614, Tr Acc: 0.8947, Val L: 1.0502, Val Acc: 0.6000, Val F1: 0.5000 lr: 0.000047\n",
      " Fold 2 Epoch 33/200: Tr L: 0.2714, Tr Acc: 0.9211, Val L: 2.6527, Val Acc: 0.4500, Val F1: 0.5600 lr: 0.000047\n",
      " Fold 2 Epoch 34/200: Tr L: 0.2952, Tr Acc: 0.9123, Val L: 0.7967, Val Acc: 0.7000, Val F1: 0.6250 lr: 0.000047\n",
      " Fold 2 Epoch 35/200: Tr L: 0.3838, Tr Acc: 0.9386, Val L: 1.4162, Val Acc: 0.6500, Val F1: 0.2222 lr: 0.000047\n",
      " Fold 2 Epoch 36/200: Tr L: 0.1001, Tr Acc: 0.9561, Val L: 1.5528, Val Acc: 0.7000, Val F1: 0.7000 lr: 0.000047\n",
      " Fold 2 Epoch 37/200: Tr L: 0.1656, Tr Acc: 0.9561, Val L: 0.6554, Val Acc: 0.8000, Val F1: 0.7500 lr: 0.000047\n",
      " Fold 2 Epoch 38/200: Tr L: 0.1053, Tr Acc: 0.9825, Val L: 0.7894, Val Acc: 0.7000, Val F1: 0.4000 lr: 0.000024\n",
      " Fold 2 Epoch 39/200: Tr L: 0.3565, Tr Acc: 0.9211, Val L: 0.7684, Val Acc: 0.7000, Val F1: 0.5714 lr: 0.000024\n",
      " Fold 2 Epoch 40/200: Tr L: 0.1402, Tr Acc: 0.9474, Val L: 0.9487, Val Acc: 0.7000, Val F1: 0.7000 lr: 0.000024\n",
      " Fold 2 Epoch 41/200: Tr L: 0.4263, Tr Acc: 0.9123, Val L: 1.2023, Val Acc: 0.7000, Val F1: 0.7000 lr: 0.000024\n",
      " Fold 2 Epoch 42/200: Tr L: 0.2307, Tr Acc: 0.8947, Val L: 0.8640, Val Acc: 0.7000, Val F1: 0.6667 lr: 0.000024\n",
      " Fold 2 Epoch 43/200: Tr L: 0.0790, Tr Acc: 0.9737, Val L: 0.7851, Val Acc: 0.7500, Val F1: 0.7059 lr: 0.000024\n",
      " Fold 2 Epoch 44/200: Tr L: 0.7841, Tr Acc: 0.9561, Val L: 0.7921, Val Acc: 0.7500, Val F1: 0.6667 lr: 0.000024\n",
      " Fold 2 Epoch 45/200: Tr L: 0.1972, Tr Acc: 0.9298, Val L: 0.9165, Val Acc: 0.7000, Val F1: 0.6667 lr: 0.000024\n",
      " Fold 2 Epoch 46/200: Tr L: 0.1911, Tr Acc: 0.9474, Val L: 0.8796, Val Acc: 0.6500, Val F1: 0.5882 lr: 0.000024\n",
      " Fold 2 Epoch 47/200: Tr L: 0.1750, Tr Acc: 0.9474, Val L: 1.2172, Val Acc: 0.6500, Val F1: 0.6316 lr: 0.000024\n",
      " Fold 2 Epoch 48/200: Tr L: 0.2057, Tr Acc: 0.9035, Val L: 0.8684, Val Acc: 0.6500, Val F1: 0.5882 lr: 0.000024\n",
      " Fold 2 Epoch 49/200: Tr L: 0.5229, Tr Acc: 0.9298, Val L: 1.0173, Val Acc: 0.6000, Val F1: 0.5556 lr: 0.000024\n",
      " Fold 2 Epoch 50/200: Tr L: 0.2413, Tr Acc: 0.9298, Val L: 1.0429, Val Acc: 0.6500, Val F1: 0.6316 lr: 0.000024\n",
      " Fold 2 Epoch 51/200: Tr L: 0.2692, Tr Acc: 0.9561, Val L: 1.4700, Val Acc: 0.7000, Val F1: 0.7000 lr: 0.000024\n",
      " Fold 2 Epoch 52/200: Tr L: 0.2724, Tr Acc: 0.8947, Val L: 1.7004, Val Acc: 0.6000, Val F1: 0.6364 lr: 0.000024\n",
      " Fold 2 Epoch 53/200: Tr L: 0.3273, Tr Acc: 0.8860, Val L: 1.0740, Val Acc: 0.6500, Val F1: 0.6316 lr: 0.000024\n",
      " Fold 2 Epoch 54/200: Tr L: 0.1306, Tr Acc: 0.9561, Val L: 0.9650, Val Acc: 0.7000, Val F1: 0.6667 lr: 0.000024\n",
      " Fold 2 Epoch 55/200: Tr L: 0.0351, Tr Acc: 0.9912, Val L: 0.8870, Val Acc: 0.6500, Val F1: 0.6316 lr: 0.000024\n",
      " Fold 2 Epoch 56/200: Tr L: 0.1009, Tr Acc: 0.9649, Val L: 0.9480, Val Acc: 0.6500, Val F1: 0.6316 lr: 0.000024\n",
      " Fold 2 Epoch 57/200: Tr L: 0.2062, Tr Acc: 0.9386, Val L: 0.8420, Val Acc: 0.7000, Val F1: 0.6667 lr: 0.000024\n",
      " Fold 2 Epoch 58/200: Tr L: 0.1420, Tr Acc: 0.9386, Val L: 0.9287, Val Acc: 0.6500, Val F1: 0.6316 lr: 0.000024\n",
      " Fold 2 Epoch 59/200: Tr L: 0.1855, Tr Acc: 0.9561, Val L: 1.3241, Val Acc: 0.6500, Val F1: 0.5882 lr: 0.000024\n",
      " Fold 2 Epoch 60/200: Tr L: 0.3140, Tr Acc: 0.9035, Val L: 0.9911, Val Acc: 0.6500, Val F1: 0.5882 lr: 0.000024\n",
      " Fold 2 Epoch 61/200: Tr L: 0.0804, Tr Acc: 0.9649, Val L: 0.8527, Val Acc: 0.6500, Val F1: 0.5882 lr: 0.000024\n",
      " Fold 2 Epoch 62/200: Tr L: 0.0963, Tr Acc: 0.9561, Val L: 1.1811, Val Acc: 0.7000, Val F1: 0.6667 lr: 0.000024\n",
      " Fold 2 Epoch 63/200: Tr L: 0.5491, Tr Acc: 0.9386, Val L: 1.0840, Val Acc: 0.7000, Val F1: 0.6667 lr: 0.000024\n",
      " Fold 2 Epoch 64/200: Tr L: 0.2445, Tr Acc: 0.8684, Val L: 0.9045, Val Acc: 0.5500, Val F1: 0.3077 lr: 0.000024\n",
      " Fold 2 Epoch 65/200: Tr L: 0.0742, Tr Acc: 0.9386, Val L: 0.9276, Val Acc: 0.7000, Val F1: 0.6667 lr: 0.000024\n",
      " Fold 2 Epoch 66/200: Tr L: 0.4871, Tr Acc: 0.9386, Val L: 1.2810, Val Acc: 0.7000, Val F1: 0.6667 lr: 0.000024\n",
      "Early stopping triggered at epoch 66 for fold 2\n",
      "--- Evaluating Fold 2 on Outer Test Set ---\n",
      "Warning: model_factory used without specific LR, using default/cfg LR: 0.001\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Test set class counts for fold 2: {0: 14, 1: 12}\n",
      "percentage of classes in test set: 0    0.538462\n",
      "1    0.461538\n",
      "Name: count, dtype: float64\n",
      " [FOLD 2 FINAL] Test Loss: 0.7470 | Test Acc: 0.5385 | test Balanced Acc: 0.5119 | test F1: 0.2500 | Test AUC: 1.0000\n",
      "model class name: SSLClassifierModule\n",
      "model class name: SSLClassifierModule\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-30 15:12:07,220] A new study created in memory with name: no-name-5add1990-93c9-44f9-b160-f685c2ecf588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== OUTER FOLD 3 / 6 =====\n",
      "Outer Train images: 117 | Outer Test images: 23\n",
      "--- Calculating normalization stats for Fold 3 Training Data ---\n",
      "Fold 3 stats: {'mean': [0.029878893867135048, 0.011145910248160362, 0.08080650866031647], 'std': [0.05596392974257469, 0.01636389084160328, 0.08576004952192307]}\n",
      "--- Generating data transforms for Fold 3 ---\n",
      "Using pretrained model: False\n",
      "Using pretrained model: False and supported by torchvision: <function is_supported_by_torchvision at 0x72cd7d419440> with color transforms: False\n",
      "the model is not supported by torchvision or is not pretrained\n",
      "Model Resnet18 not supported using custom transforms\n",
      "Applying fold-specific normalization with mean: [0.029878893867135048, 0.011145910248160362, 0.08080650866031647], std: [0.05596392974257469, 0.01636389084160328, 0.08576004952192307]\n",
      "Applying fold-specific normalization to validation data with mean: [0.029878893867135048, 0.011145910248160362, 0.08080650866031647], std: [0.05596392974257469, 0.01636389084160328, 0.08576004952192307]\n",
      "Transforms generated for Fold 3.\n",
      "--- Starting Hyperparameter Tuning for Fold 3 ---\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-30 15:12:16,266] Trial 0 finished with value: 0.6928838863968849 and parameters: {'lr': 4.715696678089837e-05}. Best is trial 0 with value: 0.6928838863968849.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-30 15:12:25,581] Trial 1 finished with value: 5.366214253401267 and parameters: {'lr': 0.0014886262201211794}. Best is trial 0 with value: 0.6928838863968849.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-30 15:12:35,397] Trial 2 finished with value: 1.2447441685944796 and parameters: {'lr': 0.0004014783718209777}. Best is trial 0 with value: 0.6928838863968849.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Best LR from inner CV = 0.000047\n",
      "--- Starting Final Model Training for Fold 3 with LR=0.000047 ---\n",
      "X_train_es: (97,) | X_val_es: (20,)\n",
      "Early stopping split: Train images: 97, Validation images: 20\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "===========================\n",
      "Model Architecture:\n",
      "==================\n",
      "Total parameters: 11,177,538\n",
      "Trainable parameters: 11,177,538\n",
      "Non-trainable parameters: 0\n",
      "===========================\n",
      " Fold 3 Epoch 1/200: Tr L: 0.7553, Tr Acc: 0.5000, Val L: 0.7653, Val Acc: 0.4500, Val F1: 0.5926 lr: 0.000047\n",
      " Fold 3 Epoch 2/200: Tr L: 0.7100, Tr Acc: 0.5614, Val L: 0.6675, Val Acc: 0.6000, Val F1: 0.0000 lr: 0.000047\n",
      " Fold 3 Epoch 3/200: Tr L: 0.6644, Tr Acc: 0.6053, Val L: 0.6802, Val Acc: 0.6000, Val F1: 0.6000 lr: 0.000047\n",
      " Fold 3 Epoch 4/200: Tr L: 0.6282, Tr Acc: 0.7193, Val L: 0.6466, Val Acc: 0.6000, Val F1: 0.5000 lr: 0.000047\n",
      " Fold 3 Epoch 5/200: Tr L: 0.5569, Tr Acc: 0.7544, Val L: 0.6254, Val Acc: 0.5000, Val F1: 0.2857 lr: 0.000047\n",
      " Fold 3 Epoch 6/200: Tr L: 0.5093, Tr Acc: 0.7982, Val L: 0.6381, Val Acc: 0.5000, Val F1: 0.3750 lr: 0.000047\n",
      " Fold 3 Epoch 7/200: Tr L: 0.4430, Tr Acc: 0.7982, Val L: 0.6389, Val Acc: 0.6000, Val F1: 0.5556 lr: 0.000047\n",
      " Fold 3 Epoch 8/200: Tr L: 0.4500, Tr Acc: 0.7719, Val L: 0.6522, Val Acc: 0.5500, Val F1: 0.4000 lr: 0.000047\n",
      " Fold 3 Epoch 9/200: Tr L: 0.4215, Tr Acc: 0.8158, Val L: 0.4762, Val Acc: 0.7500, Val F1: 0.7619 lr: 0.000047\n",
      " Fold 3 Epoch 10/200: Tr L: 0.4420, Tr Acc: 0.8421, Val L: 0.4477, Val Acc: 0.6500, Val F1: 0.6316 lr: 0.000047\n",
      " Fold 3 Epoch 11/200: Tr L: 0.4460, Tr Acc: 0.7895, Val L: 0.6185, Val Acc: 0.6500, Val F1: 0.6316 lr: 0.000047\n",
      " Fold 3 Epoch 12/200: Tr L: 0.3560, Tr Acc: 0.8596, Val L: 0.6441, Val Acc: 0.6000, Val F1: 0.5000 lr: 0.000047\n",
      " Fold 3 Epoch 13/200: Tr L: 0.4024, Tr Acc: 0.8070, Val L: 0.5228, Val Acc: 0.7500, Val F1: 0.7619 lr: 0.000047\n",
      " Fold 3 Epoch 14/200: Tr L: 0.2898, Tr Acc: 0.9035, Val L: 0.5091, Val Acc: 0.8000, Val F1: 0.7778 lr: 0.000047\n",
      " Fold 3 Epoch 15/200: Tr L: 0.4184, Tr Acc: 0.8070, Val L: 0.7630, Val Acc: 0.6500, Val F1: 0.5882 lr: 0.000047\n",
      " Fold 3 Epoch 16/200: Tr L: 0.3900, Tr Acc: 0.8860, Val L: 0.7313, Val Acc: 0.5500, Val F1: 0.4000 lr: 0.000047\n",
      " Fold 3 Epoch 17/200: Tr L: 0.4035, Tr Acc: 0.8772, Val L: 0.8998, Val Acc: 0.5500, Val F1: 0.5263 lr: 0.000047\n",
      " Fold 3 Epoch 18/200: Tr L: 0.4235, Tr Acc: 0.8246, Val L: 0.6880, Val Acc: 0.6500, Val F1: 0.6316 lr: 0.000047\n",
      " Fold 3 Epoch 19/200: Tr L: 0.3437, Tr Acc: 0.8684, Val L: 0.7247, Val Acc: 0.6000, Val F1: 0.5556 lr: 0.000047\n",
      " Fold 3 Epoch 20/200: Tr L: 0.3368, Tr Acc: 0.9035, Val L: 0.7803, Val Acc: 0.6500, Val F1: 0.4615 lr: 0.000047\n",
      " Fold 3 Epoch 21/200: Tr L: 0.2990, Tr Acc: 0.8947, Val L: 0.7472, Val Acc: 0.7000, Val F1: 0.7273 lr: 0.000047\n",
      " Fold 3 Epoch 22/200: Tr L: 0.1940, Tr Acc: 0.9298, Val L: 0.6701, Val Acc: 0.6500, Val F1: 0.6667 lr: 0.000047\n",
      " Fold 3 Epoch 23/200: Tr L: 0.3513, Tr Acc: 0.8421, Val L: 1.0880, Val Acc: 0.6500, Val F1: 0.3636 lr: 0.000047\n",
      " Fold 3 Epoch 24/200: Tr L: 0.2445, Tr Acc: 0.8596, Val L: 0.9979, Val Acc: 0.7000, Val F1: 0.6667 lr: 0.000047\n",
      " Fold 3 Epoch 25/200: Tr L: 0.5359, Tr Acc: 0.8246, Val L: 1.1800, Val Acc: 0.6000, Val F1: 0.5556 lr: 0.000047\n",
      " Fold 3 Epoch 26/200: Tr L: 0.2118, Tr Acc: 0.9035, Val L: 0.9290, Val Acc: 0.6500, Val F1: 0.3636 lr: 0.000047\n",
      " Fold 3 Epoch 27/200: Tr L: 0.3291, Tr Acc: 0.9123, Val L: 1.7775, Val Acc: 0.5500, Val F1: 0.6087 lr: 0.000047\n",
      " Fold 3 Epoch 28/200: Tr L: 0.2322, Tr Acc: 0.8947, Val L: 1.5911, Val Acc: 0.6500, Val F1: 0.2222 lr: 0.000047\n",
      " Fold 3 Epoch 29/200: Tr L: 0.4333, Tr Acc: 0.8421, Val L: 0.9847, Val Acc: 0.7000, Val F1: 0.6667 lr: 0.000047\n",
      " Fold 3 Epoch 30/200: Tr L: 0.2597, Tr Acc: 0.8947, Val L: 1.4678, Val Acc: 0.6000, Val F1: 0.6364 lr: 0.000047\n",
      " Fold 3 Epoch 31/200: Tr L: 0.2655, Tr Acc: 0.8509, Val L: 0.6703, Val Acc: 0.6500, Val F1: 0.5333 lr: 0.000047\n",
      " Fold 3 Epoch 32/200: Tr L: 0.3891, Tr Acc: 0.8860, Val L: 0.7115, Val Acc: 0.6000, Val F1: 0.5556 lr: 0.000047\n",
      " Fold 3 Epoch 33/200: Tr L: 0.3019, Tr Acc: 0.8860, Val L: 0.9144, Val Acc: 0.6500, Val F1: 0.5333 lr: 0.000047\n",
      " Fold 3 Epoch 34/200: Tr L: 0.1284, Tr Acc: 0.9298, Val L: 1.1778, Val Acc: 0.6500, Val F1: 0.2222 lr: 0.000047\n",
      " Fold 3 Epoch 35/200: Tr L: 0.2302, Tr Acc: 0.9211, Val L: 0.8375, Val Acc: 0.7000, Val F1: 0.7000 lr: 0.000047\n",
      " Fold 3 Epoch 36/200: Tr L: 0.1133, Tr Acc: 0.9561, Val L: 1.0346, Val Acc: 0.6500, Val F1: 0.5882 lr: 0.000047\n",
      " Fold 3 Epoch 37/200: Tr L: 0.2661, Tr Acc: 0.9211, Val L: 0.9530, Val Acc: 0.7000, Val F1: 0.7273 lr: 0.000047\n",
      " Fold 3 Epoch 38/200: Tr L: 0.1720, Tr Acc: 0.9298, Val L: 0.8537, Val Acc: 0.7500, Val F1: 0.6667 lr: 0.000047\n",
      " Fold 3 Epoch 39/200: Tr L: 0.1882, Tr Acc: 0.9298, Val L: 0.8510, Val Acc: 0.7000, Val F1: 0.6250 lr: 0.000047\n",
      " Fold 3 Epoch 40/200: Tr L: 0.0764, Tr Acc: 0.9649, Val L: 0.6472, Val Acc: 0.8000, Val F1: 0.8000 lr: 0.000047\n",
      " Fold 3 Epoch 41/200: Tr L: 0.2699, Tr Acc: 0.9123, Val L: 0.7327, Val Acc: 0.7500, Val F1: 0.6667 lr: 0.000047\n",
      " Fold 3 Epoch 42/200: Tr L: 0.2052, Tr Acc: 0.9386, Val L: 1.2578, Val Acc: 0.7000, Val F1: 0.5000 lr: 0.000024\n",
      " Fold 3 Epoch 43/200: Tr L: 0.0871, Tr Acc: 0.9737, Val L: 1.0409, Val Acc: 0.6000, Val F1: 0.5000 lr: 0.000024\n",
      " Fold 3 Epoch 44/200: Tr L: 0.5061, Tr Acc: 0.9211, Val L: 0.7932, Val Acc: 0.6500, Val F1: 0.5882 lr: 0.000024\n",
      " Fold 3 Epoch 45/200: Tr L: 0.1615, Tr Acc: 0.9561, Val L: 0.5941, Val Acc: 0.7500, Val F1: 0.7059 lr: 0.000024\n",
      " Fold 3 Epoch 46/200: Tr L: 0.3695, Tr Acc: 0.9386, Val L: 0.6823, Val Acc: 0.8000, Val F1: 0.8000 lr: 0.000024\n",
      " Fold 3 Epoch 47/200: Tr L: 0.2092, Tr Acc: 0.9474, Val L: 1.1959, Val Acc: 0.7000, Val F1: 0.6250 lr: 0.000024\n",
      " Fold 3 Epoch 48/200: Tr L: 0.1285, Tr Acc: 0.9298, Val L: 1.2482, Val Acc: 0.7000, Val F1: 0.5000 lr: 0.000024\n",
      " Fold 3 Epoch 49/200: Tr L: 0.3320, Tr Acc: 0.9386, Val L: 1.1768, Val Acc: 0.7000, Val F1: 0.6250 lr: 0.000024\n",
      " Fold 3 Epoch 50/200: Tr L: 0.2473, Tr Acc: 0.9561, Val L: 0.7018, Val Acc: 0.7000, Val F1: 0.6250 lr: 0.000024\n",
      " Fold 3 Epoch 51/200: Tr L: 0.1843, Tr Acc: 0.9561, Val L: 0.6380, Val Acc: 0.7500, Val F1: 0.7059 lr: 0.000024\n",
      " Fold 3 Epoch 52/200: Tr L: 0.5364, Tr Acc: 0.9035, Val L: 0.5091, Val Acc: 0.7500, Val F1: 0.7059 lr: 0.000024\n",
      " Fold 3 Epoch 53/200: Tr L: 0.3185, Tr Acc: 0.9298, Val L: 0.6463, Val Acc: 0.7500, Val F1: 0.7619 lr: 0.000024\n",
      " Fold 3 Epoch 54/200: Tr L: 0.2528, Tr Acc: 0.9386, Val L: 0.5683, Val Acc: 0.7500, Val F1: 0.7059 lr: 0.000024\n",
      " Fold 3 Epoch 55/200: Tr L: 0.1049, Tr Acc: 0.9649, Val L: 0.6698, Val Acc: 0.6500, Val F1: 0.5882 lr: 0.000024\n",
      " Fold 3 Epoch 56/200: Tr L: 0.1082, Tr Acc: 0.9561, Val L: 0.8248, Val Acc: 0.6500, Val F1: 0.5333 lr: 0.000024\n",
      " Fold 3 Epoch 57/200: Tr L: 0.2458, Tr Acc: 0.9298, Val L: 0.8905, Val Acc: 0.7000, Val F1: 0.7000 lr: 0.000024\n",
      " Fold 3 Epoch 58/200: Tr L: 0.1280, Tr Acc: 0.9561, Val L: 1.0980, Val Acc: 0.6500, Val F1: 0.6316 lr: 0.000024\n",
      " Fold 3 Epoch 59/200: Tr L: 0.2583, Tr Acc: 0.9298, Val L: 0.6617, Val Acc: 0.7500, Val F1: 0.7368 lr: 0.000024\n",
      " Fold 3 Epoch 60/200: Tr L: 0.3253, Tr Acc: 0.9386, Val L: 0.6869, Val Acc: 0.8000, Val F1: 0.8000 lr: 0.000024\n",
      " Fold 3 Epoch 61/200: Tr L: 0.0826, Tr Acc: 0.9649, Val L: 0.6789, Val Acc: 0.7000, Val F1: 0.5714 lr: 0.000024\n",
      " Fold 3 Epoch 62/200: Tr L: 0.5531, Tr Acc: 0.9386, Val L: 1.3491, Val Acc: 0.6000, Val F1: 0.3333 lr: 0.000024\n",
      " Fold 3 Epoch 63/200: Tr L: 0.7026, Tr Acc: 0.8684, Val L: 1.4963, Val Acc: 0.6000, Val F1: 0.4286 lr: 0.000024\n",
      " Fold 3 Epoch 64/200: Tr L: 0.1408, Tr Acc: 0.9561, Val L: 1.5469, Val Acc: 0.6000, Val F1: 0.4286 lr: 0.000024\n",
      " Fold 3 Epoch 65/200: Tr L: 0.2030, Tr Acc: 0.9035, Val L: 1.3334, Val Acc: 0.6000, Val F1: 0.3333 lr: 0.000024\n",
      " Fold 3 Epoch 66/200: Tr L: 0.2863, Tr Acc: 0.9298, Val L: 0.7738, Val Acc: 0.7000, Val F1: 0.6667 lr: 0.000024\n",
      " Fold 3 Epoch 67/200: Tr L: 0.1552, Tr Acc: 0.9386, Val L: 0.6329, Val Acc: 0.7500, Val F1: 0.7368 lr: 0.000024\n",
      " Fold 3 Epoch 68/200: Tr L: 0.2394, Tr Acc: 0.9649, Val L: 0.6018, Val Acc: 0.8000, Val F1: 0.8000 lr: 0.000024\n",
      " Fold 3 Epoch 69/200: Tr L: 0.0906, Tr Acc: 0.9737, Val L: 0.7481, Val Acc: 0.7500, Val F1: 0.6667 lr: 0.000024\n",
      " Fold 3 Epoch 70/200: Tr L: 0.1227, Tr Acc: 0.9386, Val L: 1.2299, Val Acc: 0.6500, Val F1: 0.4615 lr: 0.000024\n",
      "Early stopping triggered at epoch 70 for fold 3\n",
      "--- Evaluating Fold 3 on Outer Test Set ---\n",
      "Warning: model_factory used without specific LR, using default/cfg LR: 0.001\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Test set class counts for fold 3: {0: 14, 1: 9}\n",
      "percentage of classes in test set: 0    0.608696\n",
      "1    0.391304\n",
      "Name: count, dtype: float64\n",
      " [FOLD 3 FINAL] Test Loss: 0.4310 | Test Acc: 0.6957 | test Balanced Acc: 0.6905 | test F1: 0.6316 | Test AUC: 1.0000\n",
      "model class name: SSLClassifierModule\n",
      "model class name: SSLClassifierModule\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-30 15:13:31,523] A new study created in memory with name: no-name-befb644a-d891-4fa7-9250-f51ef68091e0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== OUTER FOLD 4 / 6 =====\n",
      "Outer Train images: 117 | Outer Test images: 23\n",
      "--- Calculating normalization stats for Fold 4 Training Data ---\n",
      "Fold 4 stats: {'mean': [0.030319206416606903, 0.011444846168160439, 0.07839563488960266], 'std': [0.05663726478815079, 0.016863830387592316, 0.08801180124282837]}\n",
      "--- Generating data transforms for Fold 4 ---\n",
      "Using pretrained model: False\n",
      "Using pretrained model: False and supported by torchvision: <function is_supported_by_torchvision at 0x72cd7d419440> with color transforms: False\n",
      "the model is not supported by torchvision or is not pretrained\n",
      "Model Resnet18 not supported using custom transforms\n",
      "Applying fold-specific normalization with mean: [0.030319206416606903, 0.011444846168160439, 0.07839563488960266], std: [0.05663726478815079, 0.016863830387592316, 0.08801180124282837]\n",
      "Applying fold-specific normalization to validation data with mean: [0.030319206416606903, 0.011444846168160439, 0.07839563488960266], std: [0.05663726478815079, 0.016863830387592316, 0.08801180124282837]\n",
      "Transforms generated for Fold 4.\n",
      "--- Starting Hyperparameter Tuning for Fold 4 ---\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-30 15:13:40,375] Trial 0 finished with value: 0.6784340068697929 and parameters: {'lr': 4.715696678089837e-05}. Best is trial 0 with value: 0.6784340068697929.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-30 15:13:49,096] Trial 1 finished with value: 10.987973356968723 and parameters: {'lr': 0.0014886262201211794}. Best is trial 0 with value: 0.6784340068697929.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-30 15:13:57,775] Trial 2 finished with value: 0.7190725393593311 and parameters: {'lr': 0.0004014783718209777}. Best is trial 0 with value: 0.6784340068697929.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Best LR from inner CV = 0.000047\n",
      "--- Starting Final Model Training for Fold 4 with LR=0.000047 ---\n",
      "X_train_es: (97,) | X_val_es: (20,)\n",
      "Early stopping split: Train images: 97, Validation images: 20\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "===========================\n",
      "Model Architecture:\n",
      "==================\n",
      "Total parameters: 11,177,538\n",
      "Trainable parameters: 11,177,538\n",
      "Non-trainable parameters: 0\n",
      "===========================\n",
      " Fold 4 Epoch 1/200: Tr L: 0.7270, Tr Acc: 0.5175, Val L: 0.7228, Val Acc: 0.5000, Val F1: 0.6154 lr: 0.000047\n",
      " Fold 4 Epoch 2/200: Tr L: 0.6365, Tr Acc: 0.7018, Val L: 0.5850, Val Acc: 0.7500, Val F1: 0.6667 lr: 0.000047\n",
      " Fold 4 Epoch 3/200: Tr L: 0.6591, Tr Acc: 0.6754, Val L: 0.5987, Val Acc: 0.6000, Val F1: 0.6667 lr: 0.000047\n",
      " Fold 4 Epoch 4/200: Tr L: 0.5715, Tr Acc: 0.6842, Val L: 0.5321, Val Acc: 0.7500, Val F1: 0.6667 lr: 0.000047\n",
      " Fold 4 Epoch 5/200: Tr L: 0.5259, Tr Acc: 0.7456, Val L: 0.5268, Val Acc: 0.7000, Val F1: 0.5714 lr: 0.000047\n",
      " Fold 4 Epoch 6/200: Tr L: 0.5049, Tr Acc: 0.7807, Val L: 0.4949, Val Acc: 0.8000, Val F1: 0.7500 lr: 0.000047\n",
      " Fold 4 Epoch 7/200: Tr L: 0.5170, Tr Acc: 0.7544, Val L: 0.3290, Val Acc: 0.9000, Val F1: 0.8889 lr: 0.000047\n",
      " Fold 4 Epoch 8/200: Tr L: 0.4122, Tr Acc: 0.7982, Val L: 0.2790, Val Acc: 0.9000, Val F1: 0.8889 lr: 0.000047\n",
      " Fold 4 Epoch 9/200: Tr L: 0.3538, Tr Acc: 0.8246, Val L: 0.2830, Val Acc: 0.9000, Val F1: 0.8889 lr: 0.000047\n",
      " Fold 4 Epoch 10/200: Tr L: 0.3015, Tr Acc: 0.8772, Val L: 0.4297, Val Acc: 0.7500, Val F1: 0.6667 lr: 0.000047\n",
      " Fold 4 Epoch 11/200: Tr L: 0.3321, Tr Acc: 0.7895, Val L: 0.6144, Val Acc: 0.8000, Val F1: 0.7143 lr: 0.000047\n",
      " Fold 4 Epoch 12/200: Tr L: 0.3914, Tr Acc: 0.8596, Val L: 0.2527, Val Acc: 0.9000, Val F1: 0.8889 lr: 0.000047\n",
      " Fold 4 Epoch 13/200: Tr L: 0.3598, Tr Acc: 0.8772, Val L: 0.2888, Val Acc: 0.9000, Val F1: 0.8889 lr: 0.000047\n",
      " Fold 4 Epoch 14/200: Tr L: 0.3236, Tr Acc: 0.8772, Val L: 0.3171, Val Acc: 0.9000, Val F1: 0.8889 lr: 0.000047\n",
      " Fold 4 Epoch 15/200: Tr L: 0.3676, Tr Acc: 0.8333, Val L: 0.3879, Val Acc: 0.8000, Val F1: 0.7778 lr: 0.000047\n",
      " Fold 4 Epoch 16/200: Tr L: 0.3083, Tr Acc: 0.8947, Val L: 0.4248, Val Acc: 0.7500, Val F1: 0.6154 lr: 0.000047\n",
      " Fold 4 Epoch 17/200: Tr L: 0.2626, Tr Acc: 0.8860, Val L: 0.3650, Val Acc: 0.8000, Val F1: 0.7500 lr: 0.000047\n",
      " Fold 4 Epoch 18/200: Tr L: 0.2732, Tr Acc: 0.9035, Val L: 0.4731, Val Acc: 0.8000, Val F1: 0.7500 lr: 0.000047\n",
      " Fold 4 Epoch 19/200: Tr L: 0.3781, Tr Acc: 0.8596, Val L: 0.2900, Val Acc: 0.8500, Val F1: 0.8235 lr: 0.000047\n",
      " Fold 4 Epoch 20/200: Tr L: 0.2996, Tr Acc: 0.8947, Val L: 1.3945, Val Acc: 0.5500, Val F1: 0.0000 lr: 0.000047\n",
      " Fold 4 Epoch 21/200: Tr L: 0.2926, Tr Acc: 0.8947, Val L: 0.2751, Val Acc: 0.8500, Val F1: 0.8421 lr: 0.000047\n",
      " Fold 4 Epoch 22/200: Tr L: 0.2244, Tr Acc: 0.9298, Val L: 0.3530, Val Acc: 0.8500, Val F1: 0.8000 lr: 0.000047\n",
      " Fold 4 Epoch 23/200: Tr L: 0.3127, Tr Acc: 0.8684, Val L: 0.4222, Val Acc: 0.8000, Val F1: 0.7143 lr: 0.000047\n",
      " Fold 4 Epoch 24/200: Tr L: 0.1832, Tr Acc: 0.9123, Val L: 0.2734, Val Acc: 0.9500, Val F1: 0.9412 lr: 0.000047\n",
      " Fold 4 Epoch 25/200: Tr L: 0.2790, Tr Acc: 0.9386, Val L: 0.3714, Val Acc: 0.8500, Val F1: 0.8421 lr: 0.000047\n",
      " Fold 4 Epoch 26/200: Tr L: 0.1751, Tr Acc: 0.9298, Val L: 0.2633, Val Acc: 0.8500, Val F1: 0.8235 lr: 0.000047\n",
      " Fold 4 Epoch 27/200: Tr L: 0.2816, Tr Acc: 0.9123, Val L: 0.2571, Val Acc: 0.9500, Val F1: 0.9412 lr: 0.000047\n",
      " Fold 4 Epoch 28/200: Tr L: 0.1719, Tr Acc: 0.9298, Val L: 0.5327, Val Acc: 0.8000, Val F1: 0.7143 lr: 0.000047\n",
      " Fold 4 Epoch 29/200: Tr L: 0.4090, Tr Acc: 0.9123, Val L: 0.5344, Val Acc: 0.7500, Val F1: 0.6154 lr: 0.000047\n",
      " Fold 4 Epoch 30/200: Tr L: 0.4112, Tr Acc: 0.9386, Val L: 0.2596, Val Acc: 0.9000, Val F1: 0.8889 lr: 0.000047\n",
      " Fold 4 Epoch 31/200: Tr L: 0.1543, Tr Acc: 0.9649, Val L: 0.7065, Val Acc: 0.8000, Val F1: 0.8000 lr: 0.000047\n",
      " Fold 4 Epoch 32/200: Tr L: 0.5518, Tr Acc: 0.8509, Val L: 0.6875, Val Acc: 0.6500, Val F1: 0.3636 lr: 0.000047\n",
      " Fold 4 Epoch 33/200: Tr L: 0.2264, Tr Acc: 0.9474, Val L: 1.6346, Val Acc: 0.5500, Val F1: 0.0000 lr: 0.000047\n",
      " Fold 4 Epoch 34/200: Tr L: 0.2174, Tr Acc: 0.9386, Val L: 0.2776, Val Acc: 0.9500, Val F1: 0.9412 lr: 0.000047\n",
      " Fold 4 Epoch 35/200: Tr L: 0.2502, Tr Acc: 0.9298, Val L: 0.2103, Val Acc: 0.9000, Val F1: 0.8750 lr: 0.000047\n",
      " Fold 4 Epoch 36/200: Tr L: 0.0855, Tr Acc: 0.9561, Val L: 1.9070, Val Acc: 0.5500, Val F1: 0.6400 lr: 0.000047\n",
      " Fold 4 Epoch 37/200: Tr L: 0.2758, Tr Acc: 0.8860, Val L: 0.7008, Val Acc: 0.6000, Val F1: 0.2000 lr: 0.000047\n",
      " Fold 4 Epoch 38/200: Tr L: 0.2858, Tr Acc: 0.9386, Val L: 0.3496, Val Acc: 0.8500, Val F1: 0.8000 lr: 0.000047\n",
      " Fold 4 Epoch 39/200: Tr L: 0.2040, Tr Acc: 0.9386, Val L: 0.4459, Val Acc: 0.8500, Val F1: 0.8421 lr: 0.000047\n",
      " Fold 4 Epoch 40/200: Tr L: 0.1165, Tr Acc: 0.9386, Val L: 0.4791, Val Acc: 0.7000, Val F1: 0.5714 lr: 0.000047\n",
      " Fold 4 Epoch 41/200: Tr L: 0.3419, Tr Acc: 0.9386, Val L: 0.2561, Val Acc: 0.8500, Val F1: 0.8421 lr: 0.000047\n",
      " Fold 4 Epoch 42/200: Tr L: 0.1794, Tr Acc: 0.9035, Val L: 0.2969, Val Acc: 0.8500, Val F1: 0.8421 lr: 0.000047\n",
      " Fold 4 Epoch 43/200: Tr L: 0.1484, Tr Acc: 0.9474, Val L: 0.2957, Val Acc: 0.9000, Val F1: 0.8750 lr: 0.000047\n",
      " Fold 4 Epoch 44/200: Tr L: 0.1496, Tr Acc: 0.9474, Val L: 0.3152, Val Acc: 0.9000, Val F1: 0.8750 lr: 0.000047\n",
      " Fold 4 Epoch 45/200: Tr L: 0.1224, Tr Acc: 0.9649, Val L: 0.6239, Val Acc: 0.7500, Val F1: 0.6154 lr: 0.000047\n",
      " Fold 4 Epoch 46/200: Tr L: 0.3898, Tr Acc: 0.9298, Val L: 0.5624, Val Acc: 0.7500, Val F1: 0.7619 lr: 0.000047\n",
      " Fold 4 Epoch 47/200: Tr L: 0.3396, Tr Acc: 0.8772, Val L: 0.4400, Val Acc: 0.8000, Val F1: 0.7143 lr: 0.000047\n",
      " Fold 4 Epoch 48/200: Tr L: 0.2356, Tr Acc: 0.8772, Val L: 0.5283, Val Acc: 0.8500, Val F1: 0.8000 lr: 0.000047\n",
      " Fold 4 Epoch 49/200: Tr L: 0.4102, Tr Acc: 0.9211, Val L: 1.8697, Val Acc: 0.7000, Val F1: 0.7273 lr: 0.000047\n",
      " Fold 4 Epoch 50/200: Tr L: 0.3518, Tr Acc: 0.8860, Val L: 0.3655, Val Acc: 0.8000, Val F1: 0.7143 lr: 0.000047\n",
      " Fold 4 Epoch 51/200: Tr L: 0.2472, Tr Acc: 0.9211, Val L: 1.8574, Val Acc: 0.6000, Val F1: 0.0000 lr: 0.000047\n",
      " Fold 4 Epoch 52/200: Tr L: 0.4078, Tr Acc: 0.8684, Val L: 0.7169, Val Acc: 0.7500, Val F1: 0.7619 lr: 0.000047\n",
      " Fold 4 Epoch 53/200: Tr L: 0.4763, Tr Acc: 0.8158, Val L: 0.5791, Val Acc: 0.8000, Val F1: 0.7778 lr: 0.000047\n",
      " Fold 4 Epoch 54/200: Tr L: 0.5751, Tr Acc: 0.9035, Val L: 0.7244, Val Acc: 0.7500, Val F1: 0.5455 lr: 0.000047\n",
      " Fold 4 Epoch 55/200: Tr L: 0.2746, Tr Acc: 0.8947, Val L: 0.4156, Val Acc: 0.8000, Val F1: 0.8000 lr: 0.000047\n",
      " Fold 4 Epoch 56/200: Tr L: 0.1432, Tr Acc: 0.9649, Val L: 0.4709, Val Acc: 0.8500, Val F1: 0.8421 lr: 0.000047\n",
      " Fold 4 Epoch 57/200: Tr L: 0.4885, Tr Acc: 0.8947, Val L: 0.3085, Val Acc: 0.8500, Val F1: 0.8421 lr: 0.000047\n",
      " Fold 4 Epoch 58/200: Tr L: 0.1971, Tr Acc: 0.9123, Val L: 0.2625, Val Acc: 0.9000, Val F1: 0.8889 lr: 0.000047\n",
      " Fold 4 Epoch 59/200: Tr L: 0.2780, Tr Acc: 0.8947, Val L: 0.3665, Val Acc: 0.8500, Val F1: 0.8421 lr: 0.000047\n",
      " Fold 4 Epoch 60/200: Tr L: 0.2765, Tr Acc: 0.9474, Val L: 0.3941, Val Acc: 0.8500, Val F1: 0.8421 lr: 0.000047\n",
      " Fold 4 Epoch 61/200: Tr L: 0.1176, Tr Acc: 0.9386, Val L: 0.3188, Val Acc: 0.8000, Val F1: 0.7778 lr: 0.000047\n",
      " Fold 4 Epoch 62/200: Tr L: 0.2886, Tr Acc: 0.9649, Val L: 0.5093, Val Acc: 0.7000, Val F1: 0.5714 lr: 0.000047\n",
      " Fold 4 Epoch 63/200: Tr L: 0.5610, Tr Acc: 0.8947, Val L: 0.3316, Val Acc: 0.8500, Val F1: 0.8421 lr: 0.000047\n",
      " Fold 4 Epoch 64/200: Tr L: 0.0871, Tr Acc: 0.9561, Val L: 0.3303, Val Acc: 0.8000, Val F1: 0.7500 lr: 0.000047\n",
      " Fold 4 Epoch 65/200: Tr L: 0.1978, Tr Acc: 0.9211, Val L: 0.7911, Val Acc: 0.7000, Val F1: 0.5000 lr: 0.000047\n",
      " Fold 4 Epoch 66/200: Tr L: 0.2204, Tr Acc: 0.9474, Val L: 0.3089, Val Acc: 0.7500, Val F1: 0.7368 lr: 0.000047\n",
      " Fold 4 Epoch 67/200: Tr L: 0.3831, Tr Acc: 0.9474, Val L: 0.4409, Val Acc: 0.8000, Val F1: 0.8000 lr: 0.000024\n",
      " Fold 4 Epoch 68/200: Tr L: 0.2449, Tr Acc: 0.9649, Val L: 0.3151, Val Acc: 0.8500, Val F1: 0.8421 lr: 0.000024\n",
      " Fold 4 Epoch 69/200: Tr L: 0.1071, Tr Acc: 0.9561, Val L: 0.2175, Val Acc: 0.9000, Val F1: 0.8750 lr: 0.000024\n",
      " Fold 4 Epoch 70/200: Tr L: 0.1469, Tr Acc: 0.9561, Val L: 0.2135, Val Acc: 0.8500, Val F1: 0.8000 lr: 0.000024\n",
      " Fold 4 Epoch 71/200: Tr L: 0.4047, Tr Acc: 0.9211, Val L: 0.3264, Val Acc: 0.8500, Val F1: 0.8421 lr: 0.000024\n",
      " Fold 4 Epoch 72/200: Tr L: 0.1788, Tr Acc: 0.9561, Val L: 0.2922, Val Acc: 0.8500, Val F1: 0.8421 lr: 0.000024\n",
      " Fold 4 Epoch 73/200: Tr L: 0.3337, Tr Acc: 0.9298, Val L: 0.3532, Val Acc: 0.8000, Val F1: 0.7500 lr: 0.000024\n",
      " Fold 4 Epoch 74/200: Tr L: 0.3293, Tr Acc: 0.9035, Val L: 0.3928, Val Acc: 0.9000, Val F1: 0.8750 lr: 0.000024\n",
      " Fold 4 Epoch 75/200: Tr L: 0.1008, Tr Acc: 0.9561, Val L: 0.3106, Val Acc: 0.9000, Val F1: 0.8889 lr: 0.000024\n",
      " Fold 4 Epoch 76/200: Tr L: 0.2098, Tr Acc: 0.9474, Val L: 0.2955, Val Acc: 0.8500, Val F1: 0.8421 lr: 0.000024\n",
      " Fold 4 Epoch 77/200: Tr L: 0.1673, Tr Acc: 0.9211, Val L: 0.5453, Val Acc: 0.7000, Val F1: 0.5714 lr: 0.000024\n",
      " Fold 4 Epoch 78/200: Tr L: 0.0565, Tr Acc: 0.9649, Val L: 0.3999, Val Acc: 0.8500, Val F1: 0.8421 lr: 0.000024\n",
      " Fold 4 Epoch 79/200: Tr L: 0.2608, Tr Acc: 0.9211, Val L: 0.3430, Val Acc: 0.9500, Val F1: 0.9412 lr: 0.000024\n",
      " Fold 4 Epoch 80/200: Tr L: 0.0648, Tr Acc: 0.9825, Val L: 0.3034, Val Acc: 0.9000, Val F1: 0.8889 lr: 0.000024\n",
      " Fold 4 Epoch 81/200: Tr L: 0.2663, Tr Acc: 0.9737, Val L: 0.3439, Val Acc: 0.8500, Val F1: 0.8235 lr: 0.000024\n",
      " Fold 4 Epoch 82/200: Tr L: 0.6207, Tr Acc: 0.8772, Val L: 0.3826, Val Acc: 0.9000, Val F1: 0.8889 lr: 0.000024\n",
      " Fold 4 Epoch 83/200: Tr L: 0.1088, Tr Acc: 0.9474, Val L: 0.4605, Val Acc: 0.8500, Val F1: 0.8421 lr: 0.000024\n",
      " Fold 4 Epoch 84/200: Tr L: 0.2582, Tr Acc: 0.9386, Val L: 0.8457, Val Acc: 0.7000, Val F1: 0.5000 lr: 0.000024\n",
      " Fold 4 Epoch 85/200: Tr L: 0.2164, Tr Acc: 0.9825, Val L: 2.0443, Val Acc: 0.5500, Val F1: 0.1818 lr: 0.000024\n",
      " Fold 4 Epoch 86/200: Tr L: 0.1621, Tr Acc: 0.9035, Val L: 1.0472, Val Acc: 0.7000, Val F1: 0.5714 lr: 0.000024\n",
      " Fold 4 Epoch 87/200: Tr L: 0.1398, Tr Acc: 0.9561, Val L: 0.6244, Val Acc: 0.8500, Val F1: 0.8421 lr: 0.000024\n",
      " Fold 4 Epoch 88/200: Tr L: 0.2097, Tr Acc: 0.9298, Val L: 0.5435, Val Acc: 0.8500, Val F1: 0.8421 lr: 0.000024\n",
      " Fold 4 Epoch 89/200: Tr L: 0.2246, Tr Acc: 0.9386, Val L: 0.3310, Val Acc: 0.8500, Val F1: 0.8421 lr: 0.000024\n",
      " Fold 4 Epoch 90/200: Tr L: 0.3439, Tr Acc: 0.9298, Val L: 0.6320, Val Acc: 0.7000, Val F1: 0.5714 lr: 0.000024\n",
      " Fold 4 Epoch 91/200: Tr L: 0.5020, Tr Acc: 0.9386, Val L: 0.9552, Val Acc: 0.7000, Val F1: 0.5714 lr: 0.000024\n",
      " Fold 4 Epoch 92/200: Tr L: 0.4937, Tr Acc: 0.8947, Val L: 0.5132, Val Acc: 0.9000, Val F1: 0.8889 lr: 0.000024\n",
      " Fold 4 Epoch 93/200: Tr L: 0.2713, Tr Acc: 0.9298, Val L: 0.7657, Val Acc: 0.8000, Val F1: 0.8000 lr: 0.000024\n",
      " Fold 4 Epoch 94/200: Tr L: 0.5865, Tr Acc: 0.9474, Val L: 0.6680, Val Acc: 0.8000, Val F1: 0.8000 lr: 0.000024\n",
      " Fold 4 Epoch 95/200: Tr L: 0.2839, Tr Acc: 0.8860, Val L: 0.3405, Val Acc: 0.7500, Val F1: 0.7368 lr: 0.000024\n",
      "Early stopping triggered at epoch 95 for fold 4\n",
      "--- Evaluating Fold 4 on Outer Test Set ---\n",
      "Warning: model_factory used without specific LR, using default/cfg LR: 0.001\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Test set class counts for fold 4: {0: 14, 1: 9}\n",
      "percentage of classes in test set: 0    0.608696\n",
      "1    0.391304\n",
      "Name: count, dtype: float64\n",
      " [FOLD 4 FINAL] Test Loss: 0.7026 | Test Acc: 0.6957 | test Balanced Acc: 0.6905 | test F1: 0.6316 | Test AUC: 1.0000\n",
      "model class name: SSLClassifierModule\n",
      "model class name: SSLClassifierModule\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-30 15:15:11,064] A new study created in memory with name: no-name-6e366f6d-c9aa-493e-8256-48792ab7b462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== OUTER FOLD 5 / 6 =====\n",
      "Outer Train images: 119 | Outer Test images: 21\n",
      "--- Calculating normalization stats for Fold 5 Training Data ---\n",
      "Fold 5 stats: {'mean': [0.029889598488807678, 0.01130258571356535, 0.0780547708272934], 'std': [0.05666561797261238, 0.016906680539250374, 0.08665356785058975]}\n",
      "--- Generating data transforms for Fold 5 ---\n",
      "Using pretrained model: False\n",
      "Using pretrained model: False and supported by torchvision: <function is_supported_by_torchvision at 0x72cd7d419440> with color transforms: False\n",
      "the model is not supported by torchvision or is not pretrained\n",
      "Model Resnet18 not supported using custom transforms\n",
      "Applying fold-specific normalization with mean: [0.029889598488807678, 0.01130258571356535, 0.0780547708272934], std: [0.05666561797261238, 0.016906680539250374, 0.08665356785058975]\n",
      "Applying fold-specific normalization to validation data with mean: [0.029889598488807678, 0.01130258571356535, 0.0780547708272934], std: [0.05666561797261238, 0.016906680539250374, 0.08665356785058975]\n",
      "Transforms generated for Fold 5.\n",
      "--- Starting Hyperparameter Tuning for Fold 5 ---\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-30 15:15:19,913] Trial 0 finished with value: 0.6741330102086067 and parameters: {'lr': 4.715696678089837e-05}. Best is trial 0 with value: 0.6741330102086067.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-30 15:15:28,745] Trial 1 finished with value: 8.943761593298177 and parameters: {'lr': 0.0014886262201211794}. Best is trial 0 with value: 0.6741330102086067.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-30 15:15:37,683] Trial 2 finished with value: 0.6799462605267763 and parameters: {'lr': 0.0004014783718209777}. Best is trial 0 with value: 0.6741330102086067.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Best LR from inner CV = 0.000047\n",
      "--- Starting Final Model Training for Fold 5 with LR=0.000047 ---\n",
      "X_train_es: (98,) | X_val_es: (21,)\n",
      "Early stopping split: Train images: 98, Validation images: 21\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "===========================\n",
      "Model Architecture:\n",
      "==================\n",
      "Total parameters: 11,177,538\n",
      "Trainable parameters: 11,177,538\n",
      "Non-trainable parameters: 0\n",
      "===========================\n",
      " Fold 5 Epoch 1/200: Tr L: 0.6980, Tr Acc: 0.5690, Val L: 0.6583, Val Acc: 0.6667, Val F1: 0.4615 lr: 0.000047\n",
      " Fold 5 Epoch 2/200: Tr L: 0.7394, Tr Acc: 0.6379, Val L: 0.9465, Val Acc: 0.4762, Val F1: 0.5926 lr: 0.000047\n",
      " Fold 5 Epoch 3/200: Tr L: 0.5285, Tr Acc: 0.7500, Val L: 0.9712, Val Acc: 0.4762, Val F1: 0.5926 lr: 0.000047\n",
      " Fold 5 Epoch 4/200: Tr L: 0.5892, Tr Acc: 0.7500, Val L: 0.6996, Val Acc: 0.6667, Val F1: 0.6316 lr: 0.000047\n",
      " Fold 5 Epoch 5/200: Tr L: 0.5319, Tr Acc: 0.7586, Val L: 0.6692, Val Acc: 0.5238, Val F1: 0.5455 lr: 0.000047\n",
      " Fold 5 Epoch 6/200: Tr L: 0.3690, Tr Acc: 0.8793, Val L: 0.6024, Val Acc: 0.6667, Val F1: 0.5333 lr: 0.000047\n",
      " Fold 5 Epoch 7/200: Tr L: 0.3859, Tr Acc: 0.7845, Val L: 0.5932, Val Acc: 0.5238, Val F1: 0.3750 lr: 0.000047\n",
      " Fold 5 Epoch 8/200: Tr L: 0.3117, Tr Acc: 0.8707, Val L: 0.6128, Val Acc: 0.7619, Val F1: 0.7059 lr: 0.000047\n",
      " Fold 5 Epoch 9/200: Tr L: 0.3366, Tr Acc: 0.8879, Val L: 0.7848, Val Acc: 0.7143, Val F1: 0.7273 lr: 0.000047\n",
      " Fold 5 Epoch 10/200: Tr L: 0.4478, Tr Acc: 0.8103, Val L: 0.5999, Val Acc: 0.5714, Val F1: 0.4000 lr: 0.000047\n",
      " Fold 5 Epoch 11/200: Tr L: 0.2502, Tr Acc: 0.8793, Val L: 0.6431, Val Acc: 0.6667, Val F1: 0.6667 lr: 0.000047\n",
      " Fold 5 Epoch 12/200: Tr L: 0.3073, Tr Acc: 0.8707, Val L: 0.7340, Val Acc: 0.7143, Val F1: 0.7273 lr: 0.000047\n",
      " Fold 5 Epoch 13/200: Tr L: 0.3743, Tr Acc: 0.8879, Val L: 0.5460, Val Acc: 0.5714, Val F1: 0.3077 lr: 0.000047\n",
      " Fold 5 Epoch 14/200: Tr L: 0.3245, Tr Acc: 0.8621, Val L: 0.5925, Val Acc: 0.7619, Val F1: 0.6154 lr: 0.000047\n",
      " Fold 5 Epoch 15/200: Tr L: 0.2028, Tr Acc: 0.9310, Val L: 0.5707, Val Acc: 0.6190, Val F1: 0.6364 lr: 0.000047\n",
      " Fold 5 Epoch 16/200: Tr L: 0.2296, Tr Acc: 0.9052, Val L: 0.6512, Val Acc: 0.7143, Val F1: 0.7000 lr: 0.000047\n",
      " Fold 5 Epoch 17/200: Tr L: 0.2531, Tr Acc: 0.9052, Val L: 0.7724, Val Acc: 0.6667, Val F1: 0.6957 lr: 0.000047\n",
      " Fold 5 Epoch 18/200: Tr L: 0.4460, Tr Acc: 0.8879, Val L: 0.5255, Val Acc: 0.7619, Val F1: 0.7059 lr: 0.000047\n",
      " Fold 5 Epoch 19/200: Tr L: 0.2116, Tr Acc: 0.9052, Val L: 0.8199, Val Acc: 0.7143, Val F1: 0.7273 lr: 0.000047\n",
      " Fold 5 Epoch 20/200: Tr L: 0.2167, Tr Acc: 0.9052, Val L: 0.4921, Val Acc: 0.7619, Val F1: 0.6154 lr: 0.000047\n",
      " Fold 5 Epoch 21/200: Tr L: 0.2556, Tr Acc: 0.9310, Val L: 0.4944, Val Acc: 0.6667, Val F1: 0.6316 lr: 0.000047\n",
      " Fold 5 Epoch 22/200: Tr L: 0.1490, Tr Acc: 0.9483, Val L: 2.1121, Val Acc: 0.6190, Val F1: 0.6667 lr: 0.000047\n",
      " Fold 5 Epoch 23/200: Tr L: 0.2255, Tr Acc: 0.9138, Val L: 0.7593, Val Acc: 0.7143, Val F1: 0.7273 lr: 0.000047\n",
      " Fold 5 Epoch 24/200: Tr L: 0.1577, Tr Acc: 0.9310, Val L: 0.7011, Val Acc: 0.7619, Val F1: 0.5455 lr: 0.000047\n",
      " Fold 5 Epoch 25/200: Tr L: 0.3914, Tr Acc: 0.9052, Val L: 0.8961, Val Acc: 0.7143, Val F1: 0.5000 lr: 0.000047\n",
      " Fold 5 Epoch 26/200: Tr L: 0.2840, Tr Acc: 0.9138, Val L: 1.1937, Val Acc: 0.6667, Val F1: 0.6957 lr: 0.000047\n",
      " Fold 5 Epoch 27/200: Tr L: 0.2239, Tr Acc: 0.9052, Val L: 0.6491, Val Acc: 0.7619, Val F1: 0.7368 lr: 0.000047\n",
      " Fold 5 Epoch 28/200: Tr L: 0.0679, Tr Acc: 0.9741, Val L: 0.6751, Val Acc: 0.7143, Val F1: 0.6667 lr: 0.000047\n",
      " Fold 5 Epoch 29/200: Tr L: 0.1905, Tr Acc: 0.9310, Val L: 1.2768, Val Acc: 0.7143, Val F1: 0.7273 lr: 0.000047\n",
      " Fold 5 Epoch 30/200: Tr L: 0.6032, Tr Acc: 0.8276, Val L: 0.5620, Val Acc: 0.8095, Val F1: 0.8000 lr: 0.000047\n",
      " Fold 5 Epoch 31/200: Tr L: 0.0940, Tr Acc: 0.9655, Val L: 0.5478, Val Acc: 0.7143, Val F1: 0.6250 lr: 0.000047\n",
      " Fold 5 Epoch 32/200: Tr L: 0.2972, Tr Acc: 0.9138, Val L: 1.0108, Val Acc: 0.6667, Val F1: 0.6957 lr: 0.000047\n",
      " Fold 5 Epoch 33/200: Tr L: 0.1929, Tr Acc: 0.9310, Val L: 1.5552, Val Acc: 0.6667, Val F1: 0.6957 lr: 0.000047\n",
      " Fold 5 Epoch 34/200: Tr L: 0.1201, Tr Acc: 0.9483, Val L: 0.5419, Val Acc: 0.7143, Val F1: 0.7273 lr: 0.000047\n",
      " Fold 5 Epoch 35/200: Tr L: 0.5547, Tr Acc: 0.8879, Val L: 0.6559, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000047\n",
      " Fold 5 Epoch 36/200: Tr L: 0.1526, Tr Acc: 0.9397, Val L: 0.5083, Val Acc: 0.8095, Val F1: 0.8000 lr: 0.000047\n",
      " Fold 5 Epoch 37/200: Tr L: 0.3236, Tr Acc: 0.9310, Val L: 1.1437, Val Acc: 0.7619, Val F1: 0.5455 lr: 0.000047\n",
      " Fold 5 Epoch 38/200: Tr L: 0.1826, Tr Acc: 0.9483, Val L: 0.8107, Val Acc: 0.6667, Val F1: 0.6957 lr: 0.000047\n",
      " Fold 5 Epoch 39/200: Tr L: 0.3072, Tr Acc: 0.8879, Val L: 0.6858, Val Acc: 0.7143, Val F1: 0.7000 lr: 0.000047\n",
      " Fold 5 Epoch 40/200: Tr L: 0.2554, Tr Acc: 0.9052, Val L: 0.6627, Val Acc: 0.7619, Val F1: 0.5455 lr: 0.000047\n",
      " Fold 5 Epoch 41/200: Tr L: 0.1575, Tr Acc: 0.9397, Val L: 0.7127, Val Acc: 0.6667, Val F1: 0.5333 lr: 0.000047\n",
      " Fold 5 Epoch 42/200: Tr L: 0.1916, Tr Acc: 0.9569, Val L: 0.9175, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000047\n",
      " Fold 5 Epoch 43/200: Tr L: 0.1706, Tr Acc: 0.9310, Val L: 1.1013, Val Acc: 0.7143, Val F1: 0.7273 lr: 0.000047\n",
      " Fold 5 Epoch 44/200: Tr L: 0.3674, Tr Acc: 0.9310, Val L: 0.7855, Val Acc: 0.7143, Val F1: 0.4000 lr: 0.000047\n",
      " Fold 5 Epoch 45/200: Tr L: 0.0698, Tr Acc: 0.9741, Val L: 0.6012, Val Acc: 0.7143, Val F1: 0.7000 lr: 0.000047\n",
      " Fold 5 Epoch 46/200: Tr L: 0.4448, Tr Acc: 0.9397, Val L: 1.2007, Val Acc: 0.6667, Val F1: 0.6957 lr: 0.000047\n",
      " Fold 5 Epoch 47/200: Tr L: 0.4109, Tr Acc: 0.9310, Val L: 0.7377, Val Acc: 0.6667, Val F1: 0.6667 lr: 0.000047\n",
      " Fold 5 Epoch 48/200: Tr L: 0.3304, Tr Acc: 0.9310, Val L: 0.9613, Val Acc: 0.6667, Val F1: 0.4615 lr: 0.000047\n",
      " Fold 5 Epoch 49/200: Tr L: 0.1714, Tr Acc: 0.9569, Val L: 1.7519, Val Acc: 0.6667, Val F1: 0.6957 lr: 0.000047\n",
      " Fold 5 Epoch 50/200: Tr L: 0.1956, Tr Acc: 0.9310, Val L: 0.7011, Val Acc: 0.7143, Val F1: 0.7273 lr: 0.000047\n",
      " Fold 5 Epoch 51/200: Tr L: 0.0849, Tr Acc: 0.9569, Val L: 0.6780, Val Acc: 0.7143, Val F1: 0.5000 lr: 0.000047\n",
      " Fold 5 Epoch 52/200: Tr L: 0.1305, Tr Acc: 0.9483, Val L: 0.7241, Val Acc: 0.7143, Val F1: 0.7000 lr: 0.000024\n",
      " Fold 5 Epoch 53/200: Tr L: 0.0277, Tr Acc: 0.9914, Val L: 1.0747, Val Acc: 0.7143, Val F1: 0.7273 lr: 0.000024\n",
      " Fold 5 Epoch 54/200: Tr L: 0.1085, Tr Acc: 0.9310, Val L: 1.1627, Val Acc: 0.7143, Val F1: 0.7273 lr: 0.000024\n",
      " Fold 5 Epoch 55/200: Tr L: 0.3097, Tr Acc: 0.9483, Val L: 0.5776, Val Acc: 0.7143, Val F1: 0.7000 lr: 0.000024\n",
      " Fold 5 Epoch 56/200: Tr L: 0.2634, Tr Acc: 0.9224, Val L: 0.6078, Val Acc: 0.7619, Val F1: 0.7368 lr: 0.000024\n",
      " Fold 5 Epoch 57/200: Tr L: 0.1038, Tr Acc: 0.9655, Val L: 0.6952, Val Acc: 0.7143, Val F1: 0.7000 lr: 0.000024\n",
      " Fold 5 Epoch 58/200: Tr L: 0.2891, Tr Acc: 0.9569, Val L: 0.8794, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000024\n",
      " Fold 5 Epoch 59/200: Tr L: 0.1237, Tr Acc: 0.9569, Val L: 0.6241, Val Acc: 0.7143, Val F1: 0.7000 lr: 0.000024\n",
      " Fold 5 Epoch 60/200: Tr L: 0.0629, Tr Acc: 0.9914, Val L: 0.5084, Val Acc: 0.8095, Val F1: 0.7778 lr: 0.000024\n",
      " Fold 5 Epoch 61/200: Tr L: 0.1406, Tr Acc: 0.9741, Val L: 0.5573, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000024\n",
      " Fold 5 Epoch 62/200: Tr L: 0.1580, Tr Acc: 0.9310, Val L: 0.4010, Val Acc: 0.8571, Val F1: 0.8235 lr: 0.000024\n",
      " Fold 5 Epoch 63/200: Tr L: 0.1080, Tr Acc: 0.9828, Val L: 0.4565, Val Acc: 0.8571, Val F1: 0.8235 lr: 0.000024\n",
      " Fold 5 Epoch 64/200: Tr L: 0.1053, Tr Acc: 0.9655, Val L: 0.4686, Val Acc: 0.8571, Val F1: 0.8235 lr: 0.000024\n",
      " Fold 5 Epoch 65/200: Tr L: 0.1179, Tr Acc: 0.9741, Val L: 0.4968, Val Acc: 0.8571, Val F1: 0.8235 lr: 0.000024\n",
      " Fold 5 Epoch 66/200: Tr L: 0.1397, Tr Acc: 0.9483, Val L: 0.5583, Val Acc: 0.8095, Val F1: 0.7778 lr: 0.000024\n",
      " Fold 5 Epoch 67/200: Tr L: 0.1505, Tr Acc: 0.9397, Val L: 0.5885, Val Acc: 0.8095, Val F1: 0.7778 lr: 0.000024\n",
      " Fold 5 Epoch 68/200: Tr L: 0.1785, Tr Acc: 0.9397, Val L: 0.7533, Val Acc: 0.7619, Val F1: 0.7368 lr: 0.000024\n",
      " Fold 5 Epoch 69/200: Tr L: 0.1625, Tr Acc: 0.9569, Val L: 0.6626, Val Acc: 0.7619, Val F1: 0.7368 lr: 0.000024\n",
      " Fold 5 Epoch 70/200: Tr L: 0.2259, Tr Acc: 0.9310, Val L: 0.7352, Val Acc: 0.6667, Val F1: 0.6667 lr: 0.000024\n",
      " Fold 5 Epoch 71/200: Tr L: 0.1830, Tr Acc: 0.9397, Val L: 0.9823, Val Acc: 0.6667, Val F1: 0.6667 lr: 0.000024\n",
      " Fold 5 Epoch 72/200: Tr L: 0.0406, Tr Acc: 0.9914, Val L: 0.8986, Val Acc: 0.7143, Val F1: 0.7273 lr: 0.000024\n",
      " Fold 5 Epoch 73/200: Tr L: 0.0544, Tr Acc: 0.9828, Val L: 0.6538, Val Acc: 0.7619, Val F1: 0.6667 lr: 0.000024\n",
      " Fold 5 Epoch 74/200: Tr L: 0.0639, Tr Acc: 0.9569, Val L: 0.7377, Val Acc: 0.7143, Val F1: 0.6250 lr: 0.000024\n",
      " Fold 5 Epoch 75/200: Tr L: 0.0849, Tr Acc: 0.9655, Val L: 1.1429, Val Acc: 0.7143, Val F1: 0.7000 lr: 0.000024\n",
      " Fold 5 Epoch 76/200: Tr L: 0.1006, Tr Acc: 0.9741, Val L: 1.4251, Val Acc: 0.7143, Val F1: 0.7273 lr: 0.000024\n",
      " Fold 5 Epoch 77/200: Tr L: 0.2455, Tr Acc: 0.9310, Val L: 1.3859, Val Acc: 0.7143, Val F1: 0.7273 lr: 0.000024\n",
      " Fold 5 Epoch 78/200: Tr L: 0.0856, Tr Acc: 0.9655, Val L: 0.6780, Val Acc: 0.8095, Val F1: 0.8000 lr: 0.000024\n",
      " Fold 5 Epoch 79/200: Tr L: 0.3982, Tr Acc: 0.9569, Val L: 0.5474, Val Acc: 0.8095, Val F1: 0.7778 lr: 0.000024\n",
      " Fold 5 Epoch 80/200: Tr L: 0.3682, Tr Acc: 0.9052, Val L: 0.6482, Val Acc: 0.7619, Val F1: 0.7368 lr: 0.000024\n",
      " Fold 5 Epoch 81/200: Tr L: 0.0897, Tr Acc: 0.9655, Val L: 0.5832, Val Acc: 0.8095, Val F1: 0.7778 lr: 0.000024\n",
      " Fold 5 Epoch 82/200: Tr L: 0.2575, Tr Acc: 0.9483, Val L: 1.1556, Val Acc: 0.7619, Val F1: 0.5455 lr: 0.000024\n",
      " Fold 5 Epoch 83/200: Tr L: 0.1562, Tr Acc: 0.9741, Val L: 0.6590, Val Acc: 0.7143, Val F1: 0.6667 lr: 0.000024\n",
      " Fold 5 Epoch 84/200: Tr L: 0.1002, Tr Acc: 0.9741, Val L: 0.9249, Val Acc: 0.7143, Val F1: 0.7273 lr: 0.000024\n",
      " Fold 5 Epoch 85/200: Tr L: 0.3571, Tr Acc: 0.9828, Val L: 0.8143, Val Acc: 0.6667, Val F1: 0.6957 lr: 0.000024\n",
      " Fold 5 Epoch 86/200: Tr L: 0.1541, Tr Acc: 0.9483, Val L: 0.6518, Val Acc: 0.8095, Val F1: 0.8000 lr: 0.000024\n",
      " Fold 5 Epoch 87/200: Tr L: 0.2501, Tr Acc: 0.9224, Val L: 0.7402, Val Acc: 0.7143, Val F1: 0.7273 lr: 0.000024\n",
      " Fold 5 Epoch 88/200: Tr L: 0.2253, Tr Acc: 0.9569, Val L: 0.3928, Val Acc: 0.8571, Val F1: 0.8235 lr: 0.000024\n",
      " Fold 5 Epoch 89/200: Tr L: 0.6766, Tr Acc: 0.9138, Val L: 0.6188, Val Acc: 0.7619, Val F1: 0.6667 lr: 0.000024\n",
      " Fold 5 Epoch 90/200: Tr L: 0.1824, Tr Acc: 0.9655, Val L: 0.6264, Val Acc: 0.8095, Val F1: 0.7778 lr: 0.000024\n",
      " Fold 5 Epoch 91/200: Tr L: 0.2414, Tr Acc: 0.9397, Val L: 0.6878, Val Acc: 0.7143, Val F1: 0.7000 lr: 0.000024\n",
      " Fold 5 Epoch 92/200: Tr L: 0.2724, Tr Acc: 0.9397, Val L: 0.6947, Val Acc: 0.7143, Val F1: 0.6250 lr: 0.000024\n",
      " Fold 5 Epoch 93/200: Tr L: 0.0773, Tr Acc: 0.9828, Val L: 0.8813, Val Acc: 0.7143, Val F1: 0.5714 lr: 0.000024\n",
      " Fold 5 Epoch 94/200: Tr L: 0.2923, Tr Acc: 0.9310, Val L: 1.0370, Val Acc: 0.6190, Val F1: 0.6364 lr: 0.000024\n",
      " Fold 5 Epoch 95/200: Tr L: 0.0603, Tr Acc: 0.9828, Val L: 1.0639, Val Acc: 0.7143, Val F1: 0.7273 lr: 0.000024\n",
      " Fold 5 Epoch 96/200: Tr L: 0.1243, Tr Acc: 0.9569, Val L: 0.5738, Val Acc: 0.8095, Val F1: 0.8000 lr: 0.000024\n",
      " Fold 5 Epoch 97/200: Tr L: 0.0172, Tr Acc: 1.0000, Val L: 0.4627, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000024\n",
      " Fold 5 Epoch 98/200: Tr L: 0.1380, Tr Acc: 0.9569, Val L: 0.4623, Val Acc: 0.7619, Val F1: 0.7368 lr: 0.000024\n",
      " Fold 5 Epoch 99/200: Tr L: 0.0645, Tr Acc: 0.9828, Val L: 0.8317, Val Acc: 0.6667, Val F1: 0.6957 lr: 0.000024\n",
      " Fold 5 Epoch 100/200: Tr L: 0.1774, Tr Acc: 0.9828, Val L: 0.7643, Val Acc: 0.7143, Val F1: 0.7273 lr: 0.000024\n",
      " Fold 5 Epoch 101/200: Tr L: 0.0890, Tr Acc: 0.9569, Val L: 0.4478, Val Acc: 0.7619, Val F1: 0.7368 lr: 0.000024\n",
      " Fold 5 Epoch 102/200: Tr L: 0.2417, Tr Acc: 0.9828, Val L: 0.6820, Val Acc: 0.7143, Val F1: 0.6250 lr: 0.000024\n",
      " Fold 5 Epoch 103/200: Tr L: 0.0099, Tr Acc: 1.0000, Val L: 0.7739, Val Acc: 0.7143, Val F1: 0.6667 lr: 0.000024\n",
      " Fold 5 Epoch 104/200: Tr L: 0.0843, Tr Acc: 0.9741, Val L: 0.9650, Val Acc: 0.7143, Val F1: 0.6250 lr: 0.000024\n",
      " Fold 5 Epoch 105/200: Tr L: 0.4669, Tr Acc: 0.8966, Val L: 1.2985, Val Acc: 0.6190, Val F1: 0.6364 lr: 0.000024\n",
      " Fold 5 Epoch 106/200: Tr L: 0.2455, Tr Acc: 0.9655, Val L: 2.1255, Val Acc: 0.6190, Val F1: 0.6667 lr: 0.000024\n",
      " Fold 5 Epoch 107/200: Tr L: 0.0660, Tr Acc: 0.9741, Val L: 0.9187, Val Acc: 0.6667, Val F1: 0.6957 lr: 0.000024\n",
      " Fold 5 Epoch 108/200: Tr L: 0.1073, Tr Acc: 0.9655, Val L: 0.8331, Val Acc: 0.7143, Val F1: 0.7000 lr: 0.000024\n",
      " Fold 5 Epoch 109/200: Tr L: 0.7144, Tr Acc: 0.9138, Val L: 0.6726, Val Acc: 0.7619, Val F1: 0.7368 lr: 0.000024\n",
      " Fold 5 Epoch 110/200: Tr L: 0.2701, Tr Acc: 0.9397, Val L: 0.7717, Val Acc: 0.7619, Val F1: 0.7368 lr: 0.000024\n",
      " Fold 5 Epoch 111/200: Tr L: 0.4556, Tr Acc: 0.9310, Val L: 0.5195, Val Acc: 0.8571, Val F1: 0.8235 lr: 0.000024\n",
      " Fold 5 Epoch 112/200: Tr L: 0.1436, Tr Acc: 0.9310, Val L: 0.4660, Val Acc: 0.8571, Val F1: 0.8235 lr: 0.000024\n",
      " Fold 5 Epoch 113/200: Tr L: 0.0616, Tr Acc: 0.9914, Val L: 0.5020, Val Acc: 0.8095, Val F1: 0.8000 lr: 0.000024\n",
      " Fold 5 Epoch 114/200: Tr L: 0.0870, Tr Acc: 0.9828, Val L: 0.5384, Val Acc: 0.7143, Val F1: 0.7000 lr: 0.000024\n",
      " Fold 5 Epoch 115/200: Tr L: 0.0828, Tr Acc: 0.9655, Val L: 0.8459, Val Acc: 0.7143, Val F1: 0.7273 lr: 0.000024\n",
      " Fold 5 Epoch 116/200: Tr L: 0.0640, Tr Acc: 0.9741, Val L: 0.3925, Val Acc: 0.8571, Val F1: 0.8421 lr: 0.000024\n",
      " Fold 5 Epoch 117/200: Tr L: 0.4281, Tr Acc: 0.9655, Val L: 0.3930, Val Acc: 0.8571, Val F1: 0.8235 lr: 0.000024\n",
      " Fold 5 Epoch 118/200: Tr L: 0.5024, Tr Acc: 0.9310, Val L: 0.5067, Val Acc: 0.8095, Val F1: 0.7778 lr: 0.000024\n",
      " Fold 5 Epoch 119/200: Tr L: 0.1748, Tr Acc: 0.9569, Val L: 1.1590, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000024\n",
      " Fold 5 Epoch 120/200: Tr L: 0.2629, Tr Acc: 0.9052, Val L: 0.8098, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000024\n",
      " Fold 5 Epoch 121/200: Tr L: 0.0602, Tr Acc: 0.9828, Val L: 0.6161, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000024\n",
      " Fold 5 Epoch 122/200: Tr L: 0.5077, Tr Acc: 0.9483, Val L: 0.3470, Val Acc: 0.9048, Val F1: 0.8889 lr: 0.000024\n",
      " Fold 5 Epoch 123/200: Tr L: 0.1311, Tr Acc: 0.9569, Val L: 0.6835, Val Acc: 0.7143, Val F1: 0.7273 lr: 0.000024\n",
      " Fold 5 Epoch 124/200: Tr L: 0.0948, Tr Acc: 0.9655, Val L: 0.5642, Val Acc: 0.7143, Val F1: 0.7273 lr: 0.000024\n",
      " Fold 5 Epoch 125/200: Tr L: 0.0600, Tr Acc: 0.9655, Val L: 0.6362, Val Acc: 0.7619, Val F1: 0.6154 lr: 0.000024\n",
      " Fold 5 Epoch 126/200: Tr L: 0.0663, Tr Acc: 0.9741, Val L: 0.6185, Val Acc: 0.7143, Val F1: 0.6250 lr: 0.000024\n",
      " Fold 5 Epoch 127/200: Tr L: 0.1590, Tr Acc: 0.9569, Val L: 0.5542, Val Acc: 0.7619, Val F1: 0.7059 lr: 0.000024\n",
      " Fold 5 Epoch 128/200: Tr L: 0.1912, Tr Acc: 0.9569, Val L: 0.5390, Val Acc: 0.7619, Val F1: 0.7368 lr: 0.000024\n",
      " Fold 5 Epoch 129/200: Tr L: 0.1208, Tr Acc: 0.9655, Val L: 0.8452, Val Acc: 0.7143, Val F1: 0.7273 lr: 0.000024\n",
      " Fold 5 Epoch 130/200: Tr L: 0.0401, Tr Acc: 0.9914, Val L: 0.9190, Val Acc: 0.7143, Val F1: 0.7273 lr: 0.000024\n",
      " Fold 5 Epoch 131/200: Tr L: 0.3553, Tr Acc: 0.9310, Val L: 0.5007, Val Acc: 0.8095, Val F1: 0.8000 lr: 0.000024\n",
      " Fold 5 Epoch 132/200: Tr L: 0.1023, Tr Acc: 0.9655, Val L: 0.4439, Val Acc: 0.8095, Val F1: 0.7778 lr: 0.000024\n",
      " Fold 5 Epoch 133/200: Tr L: 0.0488, Tr Acc: 0.9741, Val L: 0.4124, Val Acc: 0.8095, Val F1: 0.7778 lr: 0.000024\n",
      " Fold 5 Epoch 134/200: Tr L: 0.0375, Tr Acc: 0.9828, Val L: 0.7299, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000024\n",
      " Fold 5 Epoch 135/200: Tr L: 0.0530, Tr Acc: 0.9741, Val L: 0.3878, Val Acc: 0.8571, Val F1: 0.8421 lr: 0.000024\n",
      " Fold 5 Epoch 136/200: Tr L: 0.2694, Tr Acc: 0.9569, Val L: 0.4882, Val Acc: 0.8571, Val F1: 0.8421 lr: 0.000024\n",
      " Fold 5 Epoch 137/200: Tr L: 0.0454, Tr Acc: 0.9828, Val L: 0.5454, Val Acc: 0.8095, Val F1: 0.8000 lr: 0.000024\n",
      " Fold 5 Epoch 138/200: Tr L: 0.1807, Tr Acc: 0.9741, Val L: 0.4892, Val Acc: 0.7619, Val F1: 0.7368 lr: 0.000024\n",
      " Fold 5 Epoch 139/200: Tr L: 0.2056, Tr Acc: 0.9483, Val L: 0.4929, Val Acc: 0.7143, Val F1: 0.6250 lr: 0.000024\n",
      " Fold 5 Epoch 140/200: Tr L: 0.0158, Tr Acc: 0.9914, Val L: 0.4675, Val Acc: 0.8095, Val F1: 0.7778 lr: 0.000024\n",
      " Fold 5 Epoch 141/200: Tr L: 0.1154, Tr Acc: 0.9569, Val L: 0.8102, Val Acc: 0.8095, Val F1: 0.8000 lr: 0.000024\n",
      " Fold 5 Epoch 142/200: Tr L: 0.0470, Tr Acc: 0.9828, Val L: 0.5670, Val Acc: 0.7619, Val F1: 0.7368 lr: 0.000024\n",
      " Fold 5 Epoch 143/200: Tr L: 0.1548, Tr Acc: 0.9483, Val L: 0.5924, Val Acc: 0.7619, Val F1: 0.7368 lr: 0.000024\n",
      " Fold 5 Epoch 144/200: Tr L: 0.1399, Tr Acc: 0.9655, Val L: 0.6137, Val Acc: 0.7619, Val F1: 0.7059 lr: 0.000024\n",
      " Fold 5 Epoch 145/200: Tr L: 0.1518, Tr Acc: 0.9310, Val L: 0.5760, Val Acc: 0.8095, Val F1: 0.7778 lr: 0.000024\n",
      " Fold 5 Epoch 146/200: Tr L: 0.4404, Tr Acc: 0.9310, Val L: 1.0698, Val Acc: 0.7143, Val F1: 0.7273 lr: 0.000024\n",
      " Fold 5 Epoch 147/200: Tr L: 0.2603, Tr Acc: 0.9397, Val L: 1.5983, Val Acc: 0.6667, Val F1: 0.6957 lr: 0.000024\n",
      " Fold 5 Epoch 148/200: Tr L: 0.4976, Tr Acc: 0.9483, Val L: 0.6789, Val Acc: 0.8095, Val F1: 0.8000 lr: 0.000024\n",
      " Fold 5 Epoch 149/200: Tr L: 0.1698, Tr Acc: 0.9397, Val L: 0.5107, Val Acc: 0.8571, Val F1: 0.8421 lr: 0.000024\n",
      " Fold 5 Epoch 150/200: Tr L: 0.3297, Tr Acc: 0.9655, Val L: 0.5869, Val Acc: 0.7143, Val F1: 0.7273 lr: 0.000024\n",
      " Fold 5 Epoch 151/200: Tr L: 0.0801, Tr Acc: 0.9828, Val L: 1.4556, Val Acc: 0.6667, Val F1: 0.6957 lr: 0.000024\n",
      " Fold 5 Epoch 152/200: Tr L: 0.1810, Tr Acc: 0.9569, Val L: 0.8355, Val Acc: 0.8095, Val F1: 0.8000 lr: 0.000024\n",
      " Fold 5 Epoch 153/200: Tr L: 0.0763, Tr Acc: 0.9741, Val L: 0.7234, Val Acc: 0.6190, Val F1: 0.5556 lr: 0.000024\n",
      " Fold 5 Epoch 154/200: Tr L: 0.2307, Tr Acc: 0.9397, Val L: 0.9056, Val Acc: 0.6190, Val F1: 0.5556 lr: 0.000012\n",
      " Fold 5 Epoch 155/200: Tr L: 0.0840, Tr Acc: 0.9569, Val L: 0.8504, Val Acc: 0.6190, Val F1: 0.5556 lr: 0.000012\n",
      " Fold 5 Epoch 156/200: Tr L: 0.2074, Tr Acc: 0.9483, Val L: 0.9425, Val Acc: 0.7143, Val F1: 0.7000 lr: 0.000012\n",
      " Fold 5 Epoch 157/200: Tr L: 0.2375, Tr Acc: 0.9310, Val L: 1.1374, Val Acc: 0.6667, Val F1: 0.6957 lr: 0.000012\n",
      " Fold 5 Epoch 158/200: Tr L: 0.0646, Tr Acc: 0.9655, Val L: 1.2875, Val Acc: 0.6667, Val F1: 0.6957 lr: 0.000012\n",
      " Fold 5 Epoch 159/200: Tr L: 0.0555, Tr Acc: 0.9741, Val L: 1.5178, Val Acc: 0.6667, Val F1: 0.6957 lr: 0.000012\n",
      " Fold 5 Epoch 160/200: Tr L: 0.5606, Tr Acc: 0.9569, Val L: 0.8932, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000012\n",
      " Fold 5 Epoch 161/200: Tr L: 0.2831, Tr Acc: 0.9741, Val L: 0.4988, Val Acc: 0.8095, Val F1: 0.8000 lr: 0.000012\n",
      " Fold 5 Epoch 162/200: Tr L: 0.3274, Tr Acc: 0.9397, Val L: 0.3762, Val Acc: 0.8571, Val F1: 0.8421 lr: 0.000012\n",
      " Fold 5 Epoch 163/200: Tr L: 0.1727, Tr Acc: 0.9741, Val L: 0.6865, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000012\n",
      " Fold 5 Epoch 164/200: Tr L: 0.2552, Tr Acc: 0.9397, Val L: 0.3528, Val Acc: 0.7619, Val F1: 0.7059 lr: 0.000012\n",
      " Fold 5 Epoch 165/200: Tr L: 0.1360, Tr Acc: 0.9655, Val L: 0.3685, Val Acc: 0.7143, Val F1: 0.6250 lr: 0.000012\n",
      " Fold 5 Epoch 166/200: Tr L: 0.1112, Tr Acc: 0.9655, Val L: 0.4599, Val Acc: 0.8095, Val F1: 0.8000 lr: 0.000012\n",
      " Fold 5 Epoch 167/200: Tr L: 0.3709, Tr Acc: 0.9569, Val L: 0.7014, Val Acc: 0.8571, Val F1: 0.8421 lr: 0.000012\n",
      " Fold 5 Epoch 168/200: Tr L: 0.2287, Tr Acc: 0.9655, Val L: 0.7924, Val Acc: 0.8095, Val F1: 0.8000 lr: 0.000012\n",
      " Fold 5 Epoch 169/200: Tr L: 0.8118, Tr Acc: 0.9310, Val L: 0.6585, Val Acc: 0.8571, Val F1: 0.8421 lr: 0.000012\n",
      " Fold 5 Epoch 170/200: Tr L: 0.1562, Tr Acc: 0.9655, Val L: 0.7467, Val Acc: 0.8571, Val F1: 0.8421 lr: 0.000012\n",
      " Fold 5 Epoch 171/200: Tr L: 0.1101, Tr Acc: 0.9655, Val L: 0.5209, Val Acc: 0.8571, Val F1: 0.8421 lr: 0.000012\n",
      " Fold 5 Epoch 172/200: Tr L: 0.1684, Tr Acc: 0.9483, Val L: 0.5862, Val Acc: 0.8095, Val F1: 0.7778 lr: 0.000012\n",
      " Fold 5 Epoch 173/200: Tr L: 0.4785, Tr Acc: 0.9741, Val L: 0.4999, Val Acc: 0.8095, Val F1: 0.7778 lr: 0.000012\n",
      " Fold 5 Epoch 174/200: Tr L: 0.4628, Tr Acc: 0.9310, Val L: 0.6936, Val Acc: 0.8095, Val F1: 0.8000 lr: 0.000012\n",
      " Fold 5 Epoch 175/200: Tr L: 0.0909, Tr Acc: 0.9655, Val L: 0.6156, Val Acc: 0.8095, Val F1: 0.8000 lr: 0.000012\n",
      " Fold 5 Epoch 176/200: Tr L: 0.0349, Tr Acc: 0.9828, Val L: 0.3555, Val Acc: 0.8571, Val F1: 0.8235 lr: 0.000012\n",
      " Fold 5 Epoch 177/200: Tr L: 0.2602, Tr Acc: 0.9828, Val L: 0.5530, Val Acc: 0.8095, Val F1: 0.8000 lr: 0.000012\n",
      " Fold 5 Epoch 178/200: Tr L: 0.2108, Tr Acc: 0.9310, Val L: 1.0215, Val Acc: 0.7143, Val F1: 0.7273 lr: 0.000012\n",
      " Fold 5 Epoch 179/200: Tr L: 0.1387, Tr Acc: 0.9655, Val L: 0.8424, Val Acc: 0.7143, Val F1: 0.7273 lr: 0.000012\n",
      " Fold 5 Epoch 180/200: Tr L: 0.0769, Tr Acc: 0.9655, Val L: 0.3089, Val Acc: 0.9048, Val F1: 0.8889 lr: 0.000012\n",
      " Fold 5 Epoch 181/200: Tr L: 0.1170, Tr Acc: 0.9569, Val L: 0.3754, Val Acc: 0.8571, Val F1: 0.8421 lr: 0.000012\n",
      " Fold 5 Epoch 182/200: Tr L: 0.0968, Tr Acc: 0.9655, Val L: 0.3434, Val Acc: 0.8571, Val F1: 0.8421 lr: 0.000012\n",
      " Fold 5 Epoch 183/200: Tr L: 0.3117, Tr Acc: 0.9310, Val L: 0.4976, Val Acc: 0.9048, Val F1: 0.8889 lr: 0.000012\n",
      " Fold 5 Epoch 184/200: Tr L: 0.3487, Tr Acc: 0.9569, Val L: 0.7357, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000012\n",
      " Fold 5 Epoch 185/200: Tr L: 0.0306, Tr Acc: 0.9914, Val L: 0.6786, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000012\n",
      " Fold 5 Epoch 186/200: Tr L: 0.1965, Tr Acc: 0.9569, Val L: 0.3396, Val Acc: 0.8571, Val F1: 0.8421 lr: 0.000012\n",
      " Fold 5 Epoch 187/200: Tr L: 0.2565, Tr Acc: 0.9828, Val L: 0.4010, Val Acc: 0.8095, Val F1: 0.7778 lr: 0.000012\n",
      " Fold 5 Epoch 188/200: Tr L: 0.0632, Tr Acc: 0.9828, Val L: 0.4288, Val Acc: 0.8095, Val F1: 0.7778 lr: 0.000012\n",
      " Fold 5 Epoch 189/200: Tr L: 0.1257, Tr Acc: 0.9828, Val L: 0.3212, Val Acc: 0.8095, Val F1: 0.7778 lr: 0.000012\n",
      " Fold 5 Epoch 190/200: Tr L: 0.1228, Tr Acc: 0.9569, Val L: 0.5306, Val Acc: 0.8571, Val F1: 0.8421 lr: 0.000012\n",
      " Fold 5 Epoch 191/200: Tr L: 0.2510, Tr Acc: 0.9569, Val L: 0.3788, Val Acc: 0.8571, Val F1: 0.8421 lr: 0.000012\n",
      " Fold 5 Epoch 192/200: Tr L: 0.0755, Tr Acc: 0.9655, Val L: 0.3690, Val Acc: 0.8571, Val F1: 0.8421 lr: 0.000012\n",
      " Fold 5 Epoch 193/200: Tr L: 0.2506, Tr Acc: 0.9483, Val L: 0.5834, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000012\n",
      " Fold 5 Epoch 194/200: Tr L: 0.0269, Tr Acc: 0.9914, Val L: 0.6811, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000012\n",
      " Fold 5 Epoch 195/200: Tr L: 0.1459, Tr Acc: 0.9828, Val L: 0.6071, Val Acc: 0.8095, Val F1: 0.8000 lr: 0.000012\n",
      " Fold 5 Epoch 196/200: Tr L: 0.1621, Tr Acc: 0.9655, Val L: 0.3502, Val Acc: 0.8095, Val F1: 0.7778 lr: 0.000012\n",
      " Fold 5 Epoch 197/200: Tr L: 0.1972, Tr Acc: 0.9569, Val L: 0.3664, Val Acc: 0.8571, Val F1: 0.8235 lr: 0.000012\n",
      " Fold 5 Epoch 198/200: Tr L: 0.0558, Tr Acc: 0.9655, Val L: 0.3078, Val Acc: 0.8095, Val F1: 0.7778 lr: 0.000012\n",
      " Fold 5 Epoch 199/200: Tr L: 0.1071, Tr Acc: 0.9741, Val L: 0.4588, Val Acc: 0.8095, Val F1: 0.7778 lr: 0.000012\n",
      " Fold 5 Epoch 200/200: Tr L: 0.0214, Tr Acc: 0.9828, Val L: 0.4531, Val Acc: 0.8095, Val F1: 0.8000 lr: 0.000012\n",
      "--- Evaluating Fold 5 on Outer Test Set ---\n",
      "Warning: model_factory used without specific LR, using default/cfg LR: 0.001\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Test set class counts for fold 5: {0: 12, 1: 9}\n",
      "percentage of classes in test set: 0    0.571429\n",
      "1    0.428571\n",
      "Name: count, dtype: float64\n",
      " [FOLD 5 FINAL] Test Loss: 2.5699 | Test Acc: 0.5714 | test Balanced Acc: 0.5417 | test F1: 0.4000 | Test AUC: 1.0000\n",
      "model class name: SSLClassifierModule\n",
      "model class name: SSLClassifierModule\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-30 15:18:16,424] A new study created in memory with name: no-name-a144a4b3-8db8-4fd1-a87d-b151737a82ac\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== OUTER FOLD 6 / 6 =====\n",
      "Outer Train images: 118 | Outer Test images: 22\n",
      "--- Calculating normalization stats for Fold 6 Training Data ---\n",
      "Fold 6 stats: {'mean': [0.029349206015467644, 0.010484875179827213, 0.0792645812034607], 'std': [0.055062755942344666, 0.01500023528933525, 0.08182613551616669]}\n",
      "--- Generating data transforms for Fold 6 ---\n",
      "Using pretrained model: False\n",
      "Using pretrained model: False and supported by torchvision: <function is_supported_by_torchvision at 0x72cd7d419440> with color transforms: False\n",
      "the model is not supported by torchvision or is not pretrained\n",
      "Model Resnet18 not supported using custom transforms\n",
      "Applying fold-specific normalization with mean: [0.029349206015467644, 0.010484875179827213, 0.0792645812034607], std: [0.055062755942344666, 0.01500023528933525, 0.08182613551616669]\n",
      "Applying fold-specific normalization to validation data with mean: [0.029349206015467644, 0.010484875179827213, 0.0792645812034607], std: [0.055062755942344666, 0.01500023528933525, 0.08182613551616669]\n",
      "Transforms generated for Fold 6.\n",
      "--- Starting Hyperparameter Tuning for Fold 6 ---\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-30 15:18:25,693] Trial 0 finished with value: 0.6853212788701057 and parameters: {'lr': 4.715696678089837e-05}. Best is trial 0 with value: 0.6853212788701057.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-30 15:18:34,910] Trial 1 finished with value: 13.863952980842441 and parameters: {'lr': 0.0014886262201211794}. Best is trial 0 with value: 0.6853212788701057.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-30 15:18:44,221] Trial 2 finished with value: 0.8915468733757734 and parameters: {'lr': 0.0004014783718209777}. Best is trial 0 with value: 0.6853212788701057.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Best LR from inner CV = 0.000047\n",
      "--- Starting Final Model Training for Fold 6 with LR=0.000047 ---\n",
      "X_train_es: (97,) | X_val_es: (21,)\n",
      "Early stopping split: Train images: 97, Validation images: 21\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "===========================\n",
      "Model Architecture:\n",
      "==================\n",
      "Total parameters: 11,177,538\n",
      "Trainable parameters: 11,177,538\n",
      "Non-trainable parameters: 0\n",
      "===========================\n",
      " Fold 6 Epoch 1/200: Tr L: 0.7241, Tr Acc: 0.5259, Val L: 0.6707, Val Acc: 0.5238, Val F1: 0.6429 lr: 0.000047\n",
      " Fold 6 Epoch 2/200: Tr L: 0.6505, Tr Acc: 0.6897, Val L: 0.5346, Val Acc: 0.6190, Val F1: 0.6667 lr: 0.000047\n",
      " Fold 6 Epoch 3/200: Tr L: 0.5846, Tr Acc: 0.7672, Val L: 0.5905, Val Acc: 0.5714, Val F1: 0.6087 lr: 0.000047\n",
      " Fold 6 Epoch 4/200: Tr L: 0.6174, Tr Acc: 0.6983, Val L: 0.4834, Val Acc: 0.6190, Val F1: 0.5556 lr: 0.000047\n",
      " Fold 6 Epoch 5/200: Tr L: 0.5428, Tr Acc: 0.7500, Val L: 0.5997, Val Acc: 0.6190, Val F1: 0.6667 lr: 0.000047\n",
      " Fold 6 Epoch 6/200: Tr L: 0.4798, Tr Acc: 0.7759, Val L: 0.4142, Val Acc: 0.6667, Val F1: 0.6667 lr: 0.000047\n",
      " Fold 6 Epoch 7/200: Tr L: 0.3832, Tr Acc: 0.8534, Val L: 0.4077, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000047\n",
      " Fold 6 Epoch 8/200: Tr L: 0.4172, Tr Acc: 0.8534, Val L: 0.4181, Val Acc: 0.7143, Val F1: 0.6667 lr: 0.000047\n",
      " Fold 6 Epoch 9/200: Tr L: 0.3066, Tr Acc: 0.8534, Val L: 0.3931, Val Acc: 0.6667, Val F1: 0.6667 lr: 0.000047\n",
      " Fold 6 Epoch 10/200: Tr L: 0.4701, Tr Acc: 0.7845, Val L: 0.4836, Val Acc: 0.7143, Val F1: 0.7273 lr: 0.000047\n",
      " Fold 6 Epoch 11/200: Tr L: 0.4827, Tr Acc: 0.8534, Val L: 0.4130, Val Acc: 0.7619, Val F1: 0.7059 lr: 0.000047\n",
      " Fold 6 Epoch 12/200: Tr L: 0.2009, Tr Acc: 0.9310, Val L: 0.5376, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000047\n",
      " Fold 6 Epoch 13/200: Tr L: 0.1990, Tr Acc: 0.9397, Val L: 0.4916, Val Acc: 0.6667, Val F1: 0.5333 lr: 0.000047\n",
      " Fold 6 Epoch 14/200: Tr L: 0.3795, Tr Acc: 0.8707, Val L: 0.7350, Val Acc: 0.6190, Val F1: 0.6000 lr: 0.000047\n",
      " Fold 6 Epoch 15/200: Tr L: 0.3360, Tr Acc: 0.8621, Val L: 0.4714, Val Acc: 0.7619, Val F1: 0.6667 lr: 0.000047\n",
      " Fold 6 Epoch 16/200: Tr L: 0.2402, Tr Acc: 0.9052, Val L: 0.4823, Val Acc: 0.7143, Val F1: 0.7273 lr: 0.000047\n",
      " Fold 6 Epoch 17/200: Tr L: 0.2960, Tr Acc: 0.9052, Val L: 0.3809, Val Acc: 0.8095, Val F1: 0.8000 lr: 0.000047\n",
      " Fold 6 Epoch 18/200: Tr L: 0.4183, Tr Acc: 0.9052, Val L: 0.5447, Val Acc: 0.6190, Val F1: 0.5000 lr: 0.000047\n",
      " Fold 6 Epoch 19/200: Tr L: 0.3628, Tr Acc: 0.8879, Val L: 0.7007, Val Acc: 0.7143, Val F1: 0.7273 lr: 0.000047\n",
      " Fold 6 Epoch 20/200: Tr L: 0.4165, Tr Acc: 0.8448, Val L: 0.4545, Val Acc: 0.7143, Val F1: 0.5714 lr: 0.000047\n",
      " Fold 6 Epoch 21/200: Tr L: 0.4535, Tr Acc: 0.8879, Val L: 0.6751, Val Acc: 0.6667, Val F1: 0.6957 lr: 0.000047\n",
      " Fold 6 Epoch 22/200: Tr L: 0.1983, Tr Acc: 0.9224, Val L: 0.4283, Val Acc: 0.8095, Val F1: 0.8000 lr: 0.000047\n",
      " Fold 6 Epoch 23/200: Tr L: 0.3520, Tr Acc: 0.8966, Val L: 0.4167, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000047\n",
      " Fold 6 Epoch 24/200: Tr L: 0.2744, Tr Acc: 0.9138, Val L: 0.5212, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000047\n",
      " Fold 6 Epoch 25/200: Tr L: 0.1702, Tr Acc: 0.9655, Val L: 0.4174, Val Acc: 0.8095, Val F1: 0.8000 lr: 0.000047\n",
      " Fold 6 Epoch 26/200: Tr L: 0.4574, Tr Acc: 0.8707, Val L: 0.4972, Val Acc: 0.7143, Val F1: 0.7000 lr: 0.000047\n",
      " Fold 6 Epoch 27/200: Tr L: 0.2212, Tr Acc: 0.8966, Val L: 1.0529, Val Acc: 0.6667, Val F1: 0.6957 lr: 0.000047\n",
      " Fold 6 Epoch 28/200: Tr L: 0.1270, Tr Acc: 0.9397, Val L: 0.4295, Val Acc: 0.7619, Val F1: 0.7368 lr: 0.000047\n",
      " Fold 6 Epoch 29/200: Tr L: 0.1787, Tr Acc: 0.9052, Val L: 0.3866, Val Acc: 0.8095, Val F1: 0.8000 lr: 0.000047\n",
      " Fold 6 Epoch 30/200: Tr L: 0.3752, Tr Acc: 0.9138, Val L: 0.4903, Val Acc: 0.6667, Val F1: 0.5882 lr: 0.000047\n",
      " Fold 6 Epoch 31/200: Tr L: 0.1625, Tr Acc: 0.9569, Val L: 0.3516, Val Acc: 0.8095, Val F1: 0.8000 lr: 0.000047\n",
      " Fold 6 Epoch 32/200: Tr L: 0.2752, Tr Acc: 0.9138, Val L: 0.4247, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000047\n",
      " Fold 6 Epoch 33/200: Tr L: 0.2222, Tr Acc: 0.9138, Val L: 0.4605, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000047\n",
      " Fold 6 Epoch 34/200: Tr L: 0.1825, Tr Acc: 0.9138, Val L: 1.3769, Val Acc: 0.6667, Val F1: 0.6957 lr: 0.000047\n",
      " Fold 6 Epoch 35/200: Tr L: 0.3589, Tr Acc: 0.8966, Val L: 0.6983, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000047\n",
      " Fold 6 Epoch 36/200: Tr L: 0.2466, Tr Acc: 0.9655, Val L: 0.5318, Val Acc: 0.7143, Val F1: 0.5714 lr: 0.000047\n",
      " Fold 6 Epoch 37/200: Tr L: 0.1815, Tr Acc: 0.9310, Val L: 0.4762, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000047\n",
      " Fold 6 Epoch 38/200: Tr L: 0.0845, Tr Acc: 0.9828, Val L: 0.7757, Val Acc: 0.7143, Val F1: 0.7273 lr: 0.000047\n",
      " Fold 6 Epoch 39/200: Tr L: 0.2442, Tr Acc: 0.9224, Val L: 1.2144, Val Acc: 0.6667, Val F1: 0.3636 lr: 0.000047\n",
      " Fold 6 Epoch 40/200: Tr L: 0.3332, Tr Acc: 0.9224, Val L: 0.5843, Val Acc: 0.7619, Val F1: 0.6667 lr: 0.000047\n",
      " Fold 6 Epoch 41/200: Tr L: 0.2760, Tr Acc: 0.9310, Val L: 0.9517, Val Acc: 0.7143, Val F1: 0.7273 lr: 0.000047\n",
      " Fold 6 Epoch 42/200: Tr L: 0.1602, Tr Acc: 0.9138, Val L: 0.4708, Val Acc: 0.8095, Val F1: 0.8000 lr: 0.000047\n",
      " Fold 6 Epoch 43/200: Tr L: 0.3799, Tr Acc: 0.9224, Val L: 0.4848, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000047\n",
      " Fold 6 Epoch 44/200: Tr L: 0.1821, Tr Acc: 0.9397, Val L: 0.5433, Val Acc: 0.7143, Val F1: 0.6667 lr: 0.000047\n",
      " Fold 6 Epoch 45/200: Tr L: 0.2338, Tr Acc: 0.9310, Val L: 0.5872, Val Acc: 0.7143, Val F1: 0.7000 lr: 0.000047\n",
      " Fold 6 Epoch 46/200: Tr L: 0.2776, Tr Acc: 0.9397, Val L: 0.6029, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000047\n",
      " Fold 6 Epoch 47/200: Tr L: 0.1220, Tr Acc: 0.9483, Val L: 0.4812, Val Acc: 0.8571, Val F1: 0.8235 lr: 0.000047\n",
      " Fold 6 Epoch 48/200: Tr L: 0.3690, Tr Acc: 0.9138, Val L: 0.5339, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000047\n",
      " Fold 6 Epoch 49/200: Tr L: 0.2539, Tr Acc: 0.8966, Val L: 1.0594, Val Acc: 0.7143, Val F1: 0.7500 lr: 0.000047\n",
      " Fold 6 Epoch 50/200: Tr L: 0.2683, Tr Acc: 0.9483, Val L: 0.3667, Val Acc: 0.8095, Val F1: 0.8000 lr: 0.000047\n",
      " Fold 6 Epoch 51/200: Tr L: 0.2933, Tr Acc: 0.8966, Val L: 0.4866, Val Acc: 0.8095, Val F1: 0.7778 lr: 0.000047\n",
      " Fold 6 Epoch 52/200: Tr L: 0.3934, Tr Acc: 0.9310, Val L: 1.0238, Val Acc: 0.6667, Val F1: 0.6957 lr: 0.000047\n",
      " Fold 6 Epoch 53/200: Tr L: 0.1382, Tr Acc: 0.9224, Val L: 0.4427, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000047\n",
      " Fold 6 Epoch 54/200: Tr L: 0.3709, Tr Acc: 0.9052, Val L: 0.5944, Val Acc: 0.7143, Val F1: 0.5714 lr: 0.000047\n",
      " Fold 6 Epoch 55/200: Tr L: 0.4931, Tr Acc: 0.8879, Val L: 0.3683, Val Acc: 0.7143, Val F1: 0.7273 lr: 0.000047\n",
      " Fold 6 Epoch 56/200: Tr L: 0.2746, Tr Acc: 0.9052, Val L: 1.5391, Val Acc: 0.6190, Val F1: 0.6667 lr: 0.000047\n",
      " Fold 6 Epoch 57/200: Tr L: 0.1126, Tr Acc: 0.9655, Val L: 0.7692, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000047\n",
      " Fold 6 Epoch 58/200: Tr L: 0.4245, Tr Acc: 0.9224, Val L: 0.9265, Val Acc: 0.7143, Val F1: 0.7273 lr: 0.000047\n",
      " Fold 6 Epoch 59/200: Tr L: 0.3706, Tr Acc: 0.8793, Val L: 0.5042, Val Acc: 0.8095, Val F1: 0.8000 lr: 0.000047\n",
      " Fold 6 Epoch 60/200: Tr L: 0.0943, Tr Acc: 0.9655, Val L: 0.6717, Val Acc: 0.7143, Val F1: 0.7273 lr: 0.000047\n",
      " Fold 6 Epoch 61/200: Tr L: 0.1284, Tr Acc: 0.9483, Val L: 0.7836, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000047\n",
      " Fold 6 Epoch 62/200: Tr L: 0.1788, Tr Acc: 0.9483, Val L: 0.5403, Val Acc: 0.7619, Val F1: 0.7059 lr: 0.000047\n",
      " Fold 6 Epoch 63/200: Tr L: 0.3107, Tr Acc: 0.8793, Val L: 0.6622, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000024\n",
      " Fold 6 Epoch 64/200: Tr L: 0.0858, Tr Acc: 0.9483, Val L: 0.8868, Val Acc: 0.7143, Val F1: 0.7273 lr: 0.000024\n",
      " Fold 6 Epoch 65/200: Tr L: 0.1074, Tr Acc: 0.9569, Val L: 0.3888, Val Acc: 0.8095, Val F1: 0.8000 lr: 0.000024\n",
      " Fold 6 Epoch 66/200: Tr L: 0.0668, Tr Acc: 0.9741, Val L: 0.4407, Val Acc: 0.7619, Val F1: 0.6667 lr: 0.000024\n",
      " Fold 6 Epoch 67/200: Tr L: 0.2255, Tr Acc: 0.9483, Val L: 0.4182, Val Acc: 0.7619, Val F1: 0.7368 lr: 0.000024\n",
      " Fold 6 Epoch 68/200: Tr L: 0.0869, Tr Acc: 0.9828, Val L: 0.6265, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000024\n",
      " Fold 6 Epoch 69/200: Tr L: 0.2647, Tr Acc: 0.9310, Val L: 0.7409, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000024\n",
      " Fold 6 Epoch 70/200: Tr L: 0.0709, Tr Acc: 0.9569, Val L: 0.3387, Val Acc: 0.8095, Val F1: 0.8000 lr: 0.000024\n",
      " Fold 6 Epoch 71/200: Tr L: 0.2047, Tr Acc: 0.8879, Val L: 0.3298, Val Acc: 0.7619, Val F1: 0.7368 lr: 0.000024\n",
      " Fold 6 Epoch 72/200: Tr L: 0.1197, Tr Acc: 0.9569, Val L: 0.2995, Val Acc: 0.7619, Val F1: 0.7368 lr: 0.000024\n",
      " Fold 6 Epoch 73/200: Tr L: 0.0540, Tr Acc: 0.9741, Val L: 0.3745, Val Acc: 0.8095, Val F1: 0.8000 lr: 0.000024\n",
      " Fold 6 Epoch 74/200: Tr L: 0.0659, Tr Acc: 0.9569, Val L: 0.3957, Val Acc: 0.8095, Val F1: 0.8000 lr: 0.000024\n",
      " Fold 6 Epoch 75/200: Tr L: 0.0689, Tr Acc: 0.9828, Val L: 0.3703, Val Acc: 0.7619, Val F1: 0.7368 lr: 0.000024\n",
      " Fold 6 Epoch 76/200: Tr L: 0.0363, Tr Acc: 0.9828, Val L: 0.4484, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000024\n",
      " Fold 6 Epoch 77/200: Tr L: 0.2556, Tr Acc: 0.9224, Val L: 0.6967, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000024\n",
      " Fold 6 Epoch 78/200: Tr L: 0.0749, Tr Acc: 0.9741, Val L: 0.4610, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000024\n",
      " Fold 6 Epoch 79/200: Tr L: 0.2692, Tr Acc: 0.9397, Val L: 0.7038, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000024\n",
      " Fold 6 Epoch 80/200: Tr L: 0.0605, Tr Acc: 0.9914, Val L: 1.1380, Val Acc: 0.7143, Val F1: 0.7273 lr: 0.000024\n",
      " Fold 6 Epoch 81/200: Tr L: 0.2442, Tr Acc: 0.9483, Val L: 1.0187, Val Acc: 0.7143, Val F1: 0.7273 lr: 0.000024\n",
      " Fold 6 Epoch 82/200: Tr L: 0.0490, Tr Acc: 0.9828, Val L: 0.5259, Val Acc: 0.7143, Val F1: 0.6667 lr: 0.000024\n",
      " Fold 6 Epoch 83/200: Tr L: 0.2904, Tr Acc: 0.9310, Val L: 0.5650, Val Acc: 0.7143, Val F1: 0.6667 lr: 0.000024\n",
      " Fold 6 Epoch 84/200: Tr L: 0.2389, Tr Acc: 0.9397, Val L: 0.6110, Val Acc: 0.7143, Val F1: 0.7000 lr: 0.000024\n",
      " Fold 6 Epoch 85/200: Tr L: 0.1168, Tr Acc: 0.9828, Val L: 0.6018, Val Acc: 0.7143, Val F1: 0.7273 lr: 0.000024\n",
      " Fold 6 Epoch 86/200: Tr L: 0.1893, Tr Acc: 0.9397, Val L: 0.2815, Val Acc: 0.9048, Val F1: 0.8889 lr: 0.000024\n",
      " Fold 6 Epoch 87/200: Tr L: 0.2791, Tr Acc: 0.9052, Val L: 0.3176, Val Acc: 0.8095, Val F1: 0.7778 lr: 0.000024\n",
      " Fold 6 Epoch 88/200: Tr L: 0.1588, Tr Acc: 0.9397, Val L: 0.5046, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000024\n",
      " Fold 6 Epoch 89/200: Tr L: 0.2676, Tr Acc: 0.9397, Val L: 1.1815, Val Acc: 0.6667, Val F1: 0.6957 lr: 0.000024\n",
      " Fold 6 Epoch 90/200: Tr L: 0.1141, Tr Acc: 0.9397, Val L: 0.9353, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000024\n",
      " Fold 6 Epoch 91/200: Tr L: 0.5972, Tr Acc: 0.9310, Val L: 0.5925, Val Acc: 0.8095, Val F1: 0.8000 lr: 0.000024\n",
      " Fold 6 Epoch 92/200: Tr L: 0.1536, Tr Acc: 0.9310, Val L: 0.6488, Val Acc: 0.8095, Val F1: 0.8000 lr: 0.000024\n",
      " Fold 6 Epoch 93/200: Tr L: 0.0811, Tr Acc: 0.9655, Val L: 0.8590, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000024\n",
      " Fold 6 Epoch 94/200: Tr L: 0.1031, Tr Acc: 0.9828, Val L: 0.7865, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000024\n",
      " Fold 6 Epoch 95/200: Tr L: 0.2686, Tr Acc: 0.9310, Val L: 0.7071, Val Acc: 0.7143, Val F1: 0.7000 lr: 0.000024\n",
      " Fold 6 Epoch 96/200: Tr L: 0.0564, Tr Acc: 0.9828, Val L: 0.5696, Val Acc: 0.7143, Val F1: 0.5714 lr: 0.000024\n",
      " Fold 6 Epoch 97/200: Tr L: 0.0791, Tr Acc: 0.9828, Val L: 0.4735, Val Acc: 0.8571, Val F1: 0.8421 lr: 0.000024\n",
      " Fold 6 Epoch 98/200: Tr L: 0.2392, Tr Acc: 0.9483, Val L: 0.6474, Val Acc: 0.7143, Val F1: 0.7273 lr: 0.000024\n",
      " Fold 6 Epoch 99/200: Tr L: 0.3206, Tr Acc: 0.9655, Val L: 1.1194, Val Acc: 0.7143, Val F1: 0.7273 lr: 0.000024\n",
      " Fold 6 Epoch 100/200: Tr L: 0.1674, Tr Acc: 0.9483, Val L: 0.8249, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000024\n",
      " Fold 6 Epoch 101/200: Tr L: 0.1081, Tr Acc: 0.9741, Val L: 0.8109, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000024\n",
      " Fold 6 Epoch 102/200: Tr L: 0.3349, Tr Acc: 0.9655, Val L: 0.6908, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000024\n",
      " Fold 6 Epoch 103/200: Tr L: 0.1476, Tr Acc: 0.9655, Val L: 0.7525, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000024\n",
      " Fold 6 Epoch 104/200: Tr L: 0.1025, Tr Acc: 0.9655, Val L: 0.6310, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000024\n",
      " Fold 6 Epoch 105/200: Tr L: 0.1053, Tr Acc: 0.9655, Val L: 0.6030, Val Acc: 0.8095, Val F1: 0.8000 lr: 0.000024\n",
      " Fold 6 Epoch 106/200: Tr L: 0.1727, Tr Acc: 0.9655, Val L: 0.5567, Val Acc: 0.8095, Val F1: 0.8000 lr: 0.000024\n",
      " Fold 6 Epoch 107/200: Tr L: 0.0723, Tr Acc: 0.9828, Val L: 0.4900, Val Acc: 0.8095, Val F1: 0.7778 lr: 0.000024\n",
      " Fold 6 Epoch 108/200: Tr L: 0.2240, Tr Acc: 0.9310, Val L: 0.5764, Val Acc: 0.8095, Val F1: 0.8000 lr: 0.000024\n",
      " Fold 6 Epoch 109/200: Tr L: 0.1781, Tr Acc: 0.9397, Val L: 0.9944, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000024\n",
      " Fold 6 Epoch 110/200: Tr L: 0.1339, Tr Acc: 0.9569, Val L: 0.9716, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000024\n",
      " Fold 6 Epoch 111/200: Tr L: 0.4413, Tr Acc: 0.9483, Val L: 0.4887, Val Acc: 0.7143, Val F1: 0.6667 lr: 0.000024\n",
      " Fold 6 Epoch 112/200: Tr L: 0.0566, Tr Acc: 0.9914, Val L: 0.5070, Val Acc: 0.7143, Val F1: 0.6667 lr: 0.000024\n",
      " Fold 6 Epoch 113/200: Tr L: 0.1636, Tr Acc: 0.9741, Val L: 0.6984, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000024\n",
      " Fold 6 Epoch 114/200: Tr L: 0.2456, Tr Acc: 0.9569, Val L: 0.9107, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000024\n",
      " Fold 6 Epoch 115/200: Tr L: 0.2443, Tr Acc: 0.9310, Val L: 0.6652, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000024\n",
      " Fold 6 Epoch 116/200: Tr L: 0.2149, Tr Acc: 0.9310, Val L: 0.6067, Val Acc: 0.7143, Val F1: 0.7000 lr: 0.000024\n",
      " Fold 6 Epoch 117/200: Tr L: 0.4704, Tr Acc: 0.9655, Val L: 0.5130, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000024\n",
      " Fold 6 Epoch 118/200: Tr L: 0.0839, Tr Acc: 0.9655, Val L: 0.8037, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000012\n",
      " Fold 6 Epoch 119/200: Tr L: 0.1676, Tr Acc: 0.9569, Val L: 1.0932, Val Acc: 0.7143, Val F1: 0.7273 lr: 0.000012\n",
      " Fold 6 Epoch 120/200: Tr L: 0.1418, Tr Acc: 0.9397, Val L: 0.7077, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000012\n",
      " Fold 6 Epoch 121/200: Tr L: 0.1244, Tr Acc: 0.9655, Val L: 0.6466, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000012\n",
      " Fold 6 Epoch 122/200: Tr L: 0.2156, Tr Acc: 0.9655, Val L: 0.6676, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000012\n",
      " Fold 6 Epoch 123/200: Tr L: 0.1256, Tr Acc: 0.9655, Val L: 0.6553, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000012\n",
      " Fold 6 Epoch 124/200: Tr L: 0.2544, Tr Acc: 0.9569, Val L: 0.5674, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000012\n",
      " Fold 6 Epoch 125/200: Tr L: 0.1718, Tr Acc: 0.9569, Val L: 0.6785, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000012\n",
      " Fold 6 Epoch 126/200: Tr L: 0.0468, Tr Acc: 0.9914, Val L: 0.7333, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000012\n",
      " Fold 6 Epoch 127/200: Tr L: 0.1423, Tr Acc: 0.9741, Val L: 0.6795, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000012\n",
      " Fold 6 Epoch 128/200: Tr L: 0.1525, Tr Acc: 0.9483, Val L: 0.6912, Val Acc: 0.8095, Val F1: 0.8000 lr: 0.000012\n",
      " Fold 6 Epoch 129/200: Tr L: 0.0429, Tr Acc: 0.9741, Val L: 0.7665, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000012\n",
      " Fold 6 Epoch 130/200: Tr L: 0.0933, Tr Acc: 0.9741, Val L: 0.8752, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000012\n",
      " Fold 6 Epoch 131/200: Tr L: 0.1788, Tr Acc: 0.9397, Val L: 0.7339, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000012\n",
      " Fold 6 Epoch 132/200: Tr L: 0.0339, Tr Acc: 0.9914, Val L: 0.7266, Val Acc: 0.8095, Val F1: 0.8000 lr: 0.000012\n",
      " Fold 6 Epoch 133/200: Tr L: 0.0303, Tr Acc: 0.9828, Val L: 0.6536, Val Acc: 0.8095, Val F1: 0.8000 lr: 0.000012\n",
      " Fold 6 Epoch 134/200: Tr L: 0.7236, Tr Acc: 0.9397, Val L: 0.5500, Val Acc: 0.8095, Val F1: 0.8000 lr: 0.000012\n",
      " Fold 6 Epoch 135/200: Tr L: 0.0907, Tr Acc: 0.9828, Val L: 0.6371, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000012\n",
      " Fold 6 Epoch 136/200: Tr L: 0.0395, Tr Acc: 0.9828, Val L: 0.6011, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000012\n",
      " Fold 6 Epoch 137/200: Tr L: 0.0793, Tr Acc: 0.9655, Val L: 0.6218, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000012\n",
      " Fold 6 Epoch 138/200: Tr L: 0.2232, Tr Acc: 0.9569, Val L: 0.8448, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000012\n",
      " Fold 6 Epoch 139/200: Tr L: 0.1416, Tr Acc: 0.9569, Val L: 1.0880, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000012\n",
      " Fold 6 Epoch 140/200: Tr L: 0.1187, Tr Acc: 0.9569, Val L: 0.9601, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000012\n",
      " Fold 6 Epoch 141/200: Tr L: 0.1245, Tr Acc: 0.9569, Val L: 0.6968, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000012\n",
      " Fold 6 Epoch 142/200: Tr L: 0.2650, Tr Acc: 0.9397, Val L: 0.6493, Val Acc: 0.8095, Val F1: 0.8000 lr: 0.000012\n",
      " Fold 6 Epoch 143/200: Tr L: 0.0178, Tr Acc: 1.0000, Val L: 0.6712, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000012\n",
      " Fold 6 Epoch 144/200: Tr L: 0.1642, Tr Acc: 0.9483, Val L: 0.7151, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000012\n",
      " Fold 6 Epoch 145/200: Tr L: 0.0933, Tr Acc: 0.9569, Val L: 0.7338, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000012\n",
      " Fold 6 Epoch 146/200: Tr L: 0.3195, Tr Acc: 0.9483, Val L: 0.9151, Val Acc: 0.7619, Val F1: 0.7619 lr: 0.000012\n",
      "Early stopping triggered at epoch 146 for fold 6\n",
      "--- Evaluating Fold 6 on Outer Test Set ---\n",
      "Warning: model_factory used without specific LR, using default/cfg LR: 0.001\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Test set class counts for fold 6: {0: 13, 1: 9}\n",
      "percentage of classes in test set: 0    0.590909\n",
      "1    0.409091\n",
      "Name: count, dtype: float64\n",
      " [FOLD 6 FINAL] Test Loss: 0.6915 | Test Acc: 0.8636 | test Balanced Acc: 0.8504 | test F1: 0.8235 | Test AUC: 1.0000\n",
      "model class name: SSLClassifierModule\n",
      "model class name: SSLClassifierModule\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Malformed experiment '833162958986127285'. Detailed error Yaml file '/home/zano/Documents/TESI/mlruns/833162958986127285/meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/zano/Documents/TESI/TESI/venv/lib/python3.12/site-packages/mlflow/store/tracking/file_store.py\", line 328, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zano/Documents/TESI/TESI/venv/lib/python3.12/site-packages/mlflow/store/tracking/file_store.py\", line 422, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zano/Documents/TESI/TESI/venv/lib/python3.12/site-packages/mlflow/store/tracking/file_store.py\", line 1368, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zano/Documents/TESI/TESI/venv/lib/python3.12/site-packages/mlflow/store/tracking/file_store.py\", line 1361, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zano/Documents/TESI/TESI/venv/lib/python3.12/site-packages/mlflow/utils/file_utils.py\", line 310, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/zano/Documents/TESI/mlruns/833162958986127285/meta.yaml' does not exist.\n",
      "WARNING:root:Malformed experiment '833162958986127285'. Detailed error Yaml file '/home/zano/Documents/TESI/mlruns/833162958986127285/meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/zano/Documents/TESI/TESI/venv/lib/python3.12/site-packages/mlflow/store/tracking/file_store.py\", line 328, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zano/Documents/TESI/TESI/venv/lib/python3.12/site-packages/mlflow/store/tracking/file_store.py\", line 422, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zano/Documents/TESI/TESI/venv/lib/python3.12/site-packages/mlflow/store/tracking/file_store.py\", line 1368, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zano/Documents/TESI/TESI/venv/lib/python3.12/site-packages/mlflow/store/tracking/file_store.py\", line 1361, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zano/Documents/TESI/TESI/venv/lib/python3.12/site-packages/mlflow/utils/file_utils.py\", line 310, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/zano/Documents/TESI/mlruns/833162958986127285/meta.yaml' does not exist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------------------\n",
      "Cross-validation results (outer folds):\n",
      "  Fold 1: Test Loss=0.5910, Acc=0.8000, F1=0.7059, Bal Acc=0.7708, AUC=1.0000 (Best LR=0.000047)\n",
      "  Fold 2: Test Loss=0.7470, Acc=0.5385, F1=0.2500, Bal Acc=0.5119, AUC=1.0000 (Best LR=0.000047)\n",
      "  Fold 3: Test Loss=0.4310, Acc=0.6957, F1=0.6316, Bal Acc=0.6905, AUC=1.0000 (Best LR=0.000047)\n",
      "  Fold 4: Test Loss=0.7026, Acc=0.6957, F1=0.6316, Bal Acc=0.6905, AUC=1.0000 (Best LR=0.000047)\n",
      "  Fold 5: Test Loss=2.5699, Acc=0.5714, F1=0.4000, Bal Acc=0.5417, AUC=1.0000 (Best LR=0.000047)\n",
      "  Fold 6: Test Loss=0.6915, Acc=0.8636, F1=0.8235, Bal Acc=0.8504, AUC=1.0000 (Best LR=0.000047)\n",
      "\n",
      "--- Aggregate Results ---\n",
      "Avg Test Accuracy: 0.6941 +/- 0.1149\n",
      "Avg Test F1-Score: 0.5738 +/- 0.1922\n",
      "Avg Test Balanced Acc: 0.6760 +/- 0.1189\n",
      "Avg Test Precision: 0.6375 +/- 0.1352\n",
      "Avg Test Recall: 0.5463 +/- 0.2185\n",
      "-------------------------------------------------\n",
      "[{'fold': 1, 'test_loss': 0.5910089612007141, 'test_acc': 0.8, 'test_f1': 0.7058823529411765, 'test_balanced_acc': 0.7708333333333333, 'test_auc': 1, 'test_precision': 0.75, 'test_recall': 0.6666666666666666, 'best_lr': 4.715696678089837e-05}, {'fold': 2, 'test_loss': 0.7469957172870636, 'test_acc': 0.5384615384615384, 'test_f1': 0.25, 'test_balanced_acc': 0.5119047619047619, 'test_auc': 1, 'test_precision': 0.5, 'test_recall': 0.16666666666666666, 'best_lr': 4.715696678089837e-05}, {'fold': 3, 'test_loss': 0.43098102509975433, 'test_acc': 0.6956521739130435, 'test_f1': 0.631578947368421, 'test_balanced_acc': 0.6904761904761905, 'test_auc': 1, 'test_precision': 0.6, 'test_recall': 0.6666666666666666, 'best_lr': 4.715696678089837e-05}, {'fold': 4, 'test_loss': 0.7025914490222931, 'test_acc': 0.6956521739130435, 'test_f1': 0.631578947368421, 'test_balanced_acc': 0.6904761904761905, 'test_auc': 1, 'test_precision': 0.6, 'test_recall': 0.6666666666666666, 'best_lr': 4.715696678089837e-05}, {'fold': 5, 'test_loss': 2.5699156522750854, 'test_acc': 0.5714285714285714, 'test_f1': 0.4, 'test_balanced_acc': 0.5416666666666666, 'test_auc': 1, 'test_precision': 0.5, 'test_recall': 0.3333333333333333, 'best_lr': 4.715696678089837e-05}, {'fold': 6, 'test_loss': 0.6914881765842438, 'test_acc': 0.8636363636363636, 'test_f1': 0.8235294117647058, 'test_balanced_acc': 0.8504273504273505, 'test_auc': 1, 'test_precision': 0.875, 'test_recall': 0.7777777777777778, 'best_lr': 4.715696678089837e-05}]\n",
      "Best test_balanced_acc Fold Result: {'fold': 6, 'test_loss': 0.6914881765842438, 'test_acc': 0.8636363636363636, 'test_f1': 0.8235294117647058, 'test_balanced_acc': 0.8504273504273505, 'test_auc': 1, 'test_precision': 0.875, 'test_recall': 0.7777777777777778, 'best_lr': 4.715696678089837e-05}\n",
      "Warning: model_factory used without specific LR, using default/cfg LR: 0.001\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "Encoder does not have attribute 'fc' to remove.\n",
      "Using provided classifier head: LinearProbeHead(\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ") of class LinearProbeHead\n",
      "you are on linux\n",
      "Run name: Resnet18_oversamp_torchvision_color_transforms:False_05-30_at:15-20-41\n",
      "Current tracking uri: /home/zano/Documents/TESI/mlruns\n",
      "None\n",
      "Target layer: classifier_model.encoder.7.1.conv2\n",
      "Target layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2025/05/30 15:20:48 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating GradCAM for test images: cannot access local variable 'test_loader' where it is not associated with a value\n",
      "Processing batch 1, shape: torch.Size([16, 3, 256, 256])\n",
      "Saved GRADCAMPP overlay with DAPI-like channel to: /home/zano/Documents/TESI/TESI/gradcampp_outputs/barlow_freezed:False_Resnet18_oversamp_torchvision_color_transforms:False_05-30_at:15-20-41/batch_0_img_0.png\n",
      "Saved GRADCAMPP overlay with DAPI-like channel to: /home/zano/Documents/TESI/TESI/gradcampp_outputs/barlow_freezed:False_Resnet18_oversamp_torchvision_color_transforms:False_05-30_at:15-20-41/batch_0_img_1.png\n",
      "Saved GRADCAMPP overlay with DAPI-like channel to: /home/zano/Documents/TESI/TESI/gradcampp_outputs/barlow_freezed:False_Resnet18_oversamp_torchvision_color_transforms:False_05-30_at:15-20-41/batch_0_img_2.png\n",
      "Saved GRADCAMPP overlay with DAPI-like channel to: /home/zano/Documents/TESI/TESI/gradcampp_outputs/barlow_freezed:False_Resnet18_oversamp_torchvision_color_transforms:False_05-30_at:15-20-41/batch_0_img_3.png\n",
      "Saved GRADCAMPP overlay with DAPI-like channel to: /home/zano/Documents/TESI/TESI/gradcampp_outputs/barlow_freezed:False_Resnet18_oversamp_torchvision_color_transforms:False_05-30_at:15-20-41/batch_0_img_4.png\n",
      "Saved GRADCAMPP overlay with DAPI-like channel to: /home/zano/Documents/TESI/TESI/gradcampp_outputs/barlow_freezed:False_Resnet18_oversamp_torchvision_color_transforms:False_05-30_at:15-20-41/batch_0_img_5.png\n",
      "Saved GRADCAMPP overlay with DAPI-like channel to: /home/zano/Documents/TESI/TESI/gradcampp_outputs/barlow_freezed:False_Resnet18_oversamp_torchvision_color_transforms:False_05-30_at:15-20-41/batch_0_img_6.png\n",
      "Saved GRADCAMPP overlay with DAPI-like channel to: /home/zano/Documents/TESI/TESI/gradcampp_outputs/barlow_freezed:False_Resnet18_oversamp_torchvision_color_transforms:False_05-30_at:15-20-41/batch_0_img_7.png\n",
      "Saved GRADCAMPP overlay with DAPI-like channel to: /home/zano/Documents/TESI/TESI/gradcampp_outputs/barlow_freezed:False_Resnet18_oversamp_torchvision_color_transforms:False_05-30_at:15-20-41/batch_0_img_8.png\n",
      "Saved GRADCAMPP overlay with DAPI-like channel to: /home/zano/Documents/TESI/TESI/gradcampp_outputs/barlow_freezed:False_Resnet18_oversamp_torchvision_color_transforms:False_05-30_at:15-20-41/batch_0_img_9.png\n",
      "Saved GRADCAMPP overlay with DAPI-like channel to: /home/zano/Documents/TESI/TESI/gradcampp_outputs/barlow_freezed:False_Resnet18_oversamp_torchvision_color_transforms:False_05-30_at:15-20-41/batch_0_img_10.png\n",
      "Saved GRADCAMPP overlay with DAPI-like channel to: /home/zano/Documents/TESI/TESI/gradcampp_outputs/barlow_freezed:False_Resnet18_oversamp_torchvision_color_transforms:False_05-30_at:15-20-41/batch_0_img_11.png\n",
      "Saved GRADCAMPP overlay with DAPI-like channel to: /home/zano/Documents/TESI/TESI/gradcampp_outputs/barlow_freezed:False_Resnet18_oversamp_torchvision_color_transforms:False_05-30_at:15-20-41/batch_0_img_12.png\n",
      "Saved GRADCAMPP overlay with DAPI-like channel to: /home/zano/Documents/TESI/TESI/gradcampp_outputs/barlow_freezed:False_Resnet18_oversamp_torchvision_color_transforms:False_05-30_at:15-20-41/batch_0_img_13.png\n",
      "Saved GRADCAMPP overlay with DAPI-like channel to: /home/zano/Documents/TESI/TESI/gradcampp_outputs/barlow_freezed:False_Resnet18_oversamp_torchvision_color_transforms:False_05-30_at:15-20-41/batch_0_img_14.png\n",
      "Saved GRADCAMPP overlay with DAPI-like channel to: /home/zano/Documents/TESI/TESI/gradcampp_outputs/barlow_freezed:False_Resnet18_oversamp_torchvision_color_transforms:False_05-30_at:15-20-41/batch_0_img_15.png\n",
      "Processing batch 2, shape: torch.Size([5, 3, 256, 256])\n",
      "Saved GRADCAMPP overlay with DAPI-like channel to: /home/zano/Documents/TESI/TESI/gradcampp_outputs/barlow_freezed:False_Resnet18_oversamp_torchvision_color_transforms:False_05-30_at:15-20-41/batch_1_img_0.png\n",
      "Saved GRADCAMPP overlay with DAPI-like channel to: /home/zano/Documents/TESI/TESI/gradcampp_outputs/barlow_freezed:False_Resnet18_oversamp_torchvision_color_transforms:False_05-30_at:15-20-41/batch_1_img_1.png\n",
      "Saved GRADCAMPP overlay with DAPI-like channel to: /home/zano/Documents/TESI/TESI/gradcampp_outputs/barlow_freezed:False_Resnet18_oversamp_torchvision_color_transforms:False_05-30_at:15-20-41/batch_1_img_2.png\n",
      "Saved GRADCAMPP overlay with DAPI-like channel to: /home/zano/Documents/TESI/TESI/gradcampp_outputs/barlow_freezed:False_Resnet18_oversamp_torchvision_color_transforms:False_05-30_at:15-20-41/batch_1_img_3.png\n",
      "Saved GRADCAMPP overlay with DAPI-like channel to: /home/zano/Documents/TESI/TESI/gradcampp_outputs/barlow_freezed:False_Resnet18_oversamp_torchvision_color_transforms:False_05-30_at:15-20-41/batch_1_img_4.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils.data_visualization_functions import generate_cv_results_figure\n",
    "from utils.train_functions import train_epoch, undersample_majority, val_epoch, train_epoch_mixUp, oversample_minority\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "from utils.train_functions import SSLClassifierModule, BaseSSLClassifier\n",
    "\n",
    "import torchvision, copy, torch.nn as nn\n",
    "\n",
    "\n",
    "# barlow_encoder.eval()         \n",
    "\n",
    "feature_dim = 512                             # ResNet-18 â†’ 512\n",
    "\n",
    "#########################################\n",
    "import copy\n",
    "from utils.train_functions import  SSLClassifierModule\n",
    "\n",
    "FREEZE_ENCODER = True  \n",
    "bool_list = [True, False] \n",
    "for FREEZE_ENCODER in bool_list:\n",
    "    for encoder in [pretrained_backbone, randomly_initialized_backbone]:\n",
    "        if encoder == pretrained_backbone:\n",
    "            encoder_type = \"barlow\"\n",
    "        else:\n",
    "            encoder_type = \"random\"\n",
    "        \n",
    "        def model_factory(lr: float)-> torch.nn.Module:\n",
    "            \"\"\"\n",
    "            Modelâ€factory that instantiates a fresh SSLClassifierModule\n",
    "            with the given learning rate.\n",
    "            \"\"\"\n",
    "            fresh_encoder = copy.deepcopy(encoder)  # Create a fresh copy of the encoder\n",
    "            return SSLClassifierModule(\n",
    "                encoder=fresh_encoder,     # capture this from your outer scope\n",
    "                num_classes=2,\n",
    "                freeze_encoder=FREEZE_ENCODER,\n",
    "                lr=lr,\n",
    "                backbone_output_dim=feature_dim,\n",
    "                # input_shape=(1,3,256,256),  # Adjust as needed (B, C, H, W)\n",
    "            )\n",
    "            \n",
    "\n",
    "        from classes.NestedCVStratifiedByPatient import NestedCVStratifiedByPatient\n",
    "        # cfg.set_freezed_layer_index(None)\n",
    "        experiment = NestedCVStratifiedByPatient(\n",
    "            df=df, cfg=cfg, labels_np=labels_np, pat_labels=pat_labels, unique_pat_ids=unique_pat_ids,\n",
    "            pretrained_weights = None,\n",
    "            class_names = class_names, model_factory=model_factory, num_folds=6\n",
    "        )\n",
    "        # cfg.set_freezed_layer_index(None)\n",
    "        # hold_out_cv = True\n",
    "        using_cosine_scheduler = False\n",
    "\n",
    "        per_fold_training_metrics, outer_fold_test_results = experiment.run_experiment()\n",
    "\n",
    "\n",
    "        from utils.mlflow_functions import log_SSL_run_to_mlflow\n",
    "        def get_best_fold_idx(outer_fold_test_results, metric=\"test_balanced_acc\"):\n",
    "            \"\"\"\n",
    "            Get the index of the best fold based on a specified metric.\n",
    "\n",
    "            Args:\n",
    "                outer_fold_test_results (list of dict): List containing test results for \n",
    "                    each outer fold. Each element should be a dictionary with metrics \n",
    "                    as keys.\n",
    "                metric (str): The metric name to use for selecting the best fold. \n",
    "                    Default is \"test_balanced_acc\".\n",
    "\n",
    "            Returns:\n",
    "                int: The index of the fold with the highest value for the specified metric.\n",
    "\n",
    "            Example:\n",
    "                best_idx = get_best_fold_idx(results, metric=\"test_f1\")\n",
    "            \"\"\"\n",
    "\n",
    "            print(outer_fold_test_results)\n",
    "            best_fold_idx = np.argmax([r[metric] for r in outer_fold_test_results])\n",
    "            # print(f\"Best Balanced Accuracy Fold Index: {best_bac_fold_idx}\")\n",
    "            best_fold_result = outer_fold_test_results[best_fold_idx]\n",
    "            print(f\"Best {metric} Fold Result: {best_fold_result}\")\n",
    "            fold_idx = best_fold_result[\"fold\"]\n",
    "            return fold_idx\n",
    "\n",
    "        #loading the best model for the metric selected, it's then used for computing gradcams during logging\n",
    "        best_fold_idx = get_best_fold_idx(outer_fold_test_results, metric=\"test_balanced_acc\")\n",
    "        model_instance_for_logging, _ = experiment._get_model_and_device()\n",
    "        model_instance_for_logging.eval()\n",
    "        try:\n",
    "            model_instance_for_logging.load_state_dict(torch.load(f\"best_model_fold_{best_fold_idx}.pth\"))\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(\"Could not find best_model_fold_{best_fold_idx}.pth\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error loading model weights: {str(e)}\")\n",
    "\n",
    "        # Now, call the logging function:\n",
    "        train_transforms, val_transforms,_ = experiment.get_current_fold_transforms()\n",
    "        # Now, call the logging function:\n",
    "        # or \"moco\" if using MoCo\n",
    "        log_SSL_run_to_mlflow(\n",
    "            environmentFlags=environment_flags,\n",
    "            cfg=cfg,\n",
    "            model=model_factory(0.001),  # Pass a dummy model to log the run\n",
    "            class_names=class_names,\n",
    "            fold_results=outer_fold_test_results,\n",
    "            per_fold_metrics=per_fold_training_metrics,\n",
    "            hold_out_cv=True,\n",
    "            test_transforms=val_transforms,\n",
    "            all_images_paths_np=images_paths_np,\n",
    "            all_labels_np=labels_np,\n",
    "            test_images_paths_np=test_images_paths_np,\n",
    "            test_true_labels_np=test_true_labels_np,\n",
    "            yaml_path=yaml_path,\n",
    "            color_transforms=False,\n",
    "            model_library=\"torchvision\",\n",
    "            encoder_type= \"barlow\",\n",
    "            pretrained_backbone_path = STABLE_BACKBONE_PATH,\n",
    "            train_transforms=train_transforms,\n",
    "            # val_transforms=val_transforms,\n",
    "            freeze_encoder=FREEZE_ENCODER,\n",
    "            ssl=True,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
