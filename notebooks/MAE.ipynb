{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d4f8add",
   "metadata": {},
   "source": [
    "# MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1245ff51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/zano/Documents/TESI/TESI/notebooks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zano/Documents/TESI/TESI/venv/lib/python3.12/site-packages/IPython/core/magics/osm.py:393: UserWarning: This is now an optional IPython functionality, using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n",
      "/home/zano/Documents/TESI/TESI/venv/lib/python3.12/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd ~/Documents/TESI/TESI/notebooks\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4afb9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu124\n",
      "1.4.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import tifffile\n",
    "import glob\n",
    "import random\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "print(torch.__version__)\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from configs.ConfigLoader import ConfigLoader\n",
    "from utils.train_functions import (\n",
    "# train_epoch,\n",
    "# val_epoch,\n",
    "# print_model_summary,\n",
    "# plot_cv_results,\n",
    "# train_epoch_mixUp,\n",
    "# print_layers,\n",
    "# oversample_minority,\n",
    "# undersample_majority,\n",
    "# freeze_layers_up_to,\n",
    "# freeze_layers_up_to_progressive_ft,\n",
    "train_epoch_vit,\n",
    "val_epoch_vit,\n",
    ")\n",
    "\n",
    "import utils.transformations_functions as tf\n",
    "# Removed redundant import: from configs.ConfigLoader import ConfigLoader\n",
    "\n",
    "from classes.ModelManager import ModelManager\n",
    "import monai\n",
    "print(monai.__version__)\n",
    "#import tifffile\n",
    "#from monai.networks.nets import DenseNet121\n",
    "# import torch.nn.functional as F\n",
    "# from monai.visualize.class_activation_maps import GradCAMpp,GradCAM  \n",
    "#kaggle = input(\"Are you on Kaggle? Enter 'T' for True or 'F' for False:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88eb2862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment settings: {'gdrive': False, 'linux': True, 'kaggle': False, 'ssl': True}\n"
     ]
    }
   ],
   "source": [
    "from utils.setup_functions import set_environment_flags\n",
    "# Example usage:\n",
    "environment_flags = set_environment_flags()\n",
    "kaggle,gdrive,linux = environment_flags[\"kaggle\"], environment_flags[\"gdrive\"], environment_flags[\"linux\"]\n",
    "from utils.reproducibility_functions import set_global_seed\n",
    "set_global_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ee332c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you are on linux\n",
      "Linux detected, setting tracking URI\n",
      "Final Tracking URI: /home/zano/Documents/TESI/mlruns\n",
      "Does the directory exist? True\n"
     ]
    }
   ],
   "source": [
    "# start mlflow ui\n",
    "from utils.mlflow_functions import *\n",
    "from utils.directory_functions import *\n",
    "\n",
    "tracking_uri = get_tracking_uri(gdrive,kaggle,linux)\n",
    "mlflow.set_tracking_uri(tracking_uri)\n",
    "start_mlflow_ui(tracking_uri) # start mlflow ui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7beaba6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 channels input\n",
      "you are in linux\n",
      "/home/zano/Documents/TESI/3c_MIP_new\n"
     ]
    }
   ],
   "source": [
    "num_input_channels = int(input(\"Enter the number of input channels (3 or 4): \"))\n",
    "from utils.directory_functions import get_data_and_base_directory\n",
    "data_dir, base_dir = get_data_and_base_directory(environment_flags[\"kaggle\"], environment_flags[\"gdrive\"], environment_flags[\"linux\"], num_input_channels=num_input_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc41c7cf",
   "metadata": {},
   "source": [
    "# DATA EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3924a547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00b0091922384896bfdb76f24b4ceb3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Class Set:', index=1, options=('MSA vs Control', 'MSA vs PD', 'MSA-P vs MSA-C', 'MSA-P vâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "CLASS_NAME_SETS = {\n",
    "    \"MSA vs Control\": [\"MSA\", \"control\"],\n",
    "    \"MSA vs PD\": [\"MSA\", \"PD\"],\n",
    "    \"MSA-P vs MSA-C\": [\"MSA-P\", \"MSA-C\"],\n",
    "    \"MSA-P vs PD\": [\"MSA-P\", \"PD\"],\n",
    "    \"PD vs MSA-P vs MSA-C\": [\"PD\", \"MSA-P\", \"MSA-C\"]\n",
    "}\n",
    "\n",
    "dropdown = widgets.Dropdown(\n",
    "    options=list(CLASS_NAME_SETS.keys()),\n",
    "    value=\"MSA vs PD\",\n",
    "    description='Class Set:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "def on_dropdown_change(change):\n",
    "    \"\"\"\n",
    "    Update the class_names variable when the dropdown selection changes.\n",
    "    \"\"\"\n",
    "    global class_names\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        class_names = CLASS_NAME_SETS[change['new']]\n",
    "        print(f\"class_names set to: {class_names}\")\n",
    "\n",
    "\n",
    "class_names = CLASS_NAME_SETS[dropdown.value]\n",
    "\n",
    "dropdown.observe(on_dropdown_change)\n",
    "\n",
    "display(dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7269b40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in /home/zano/Documents/TESI/3c_MIP_new/ALL: 152\n",
      "Number of images in ALL folder: 152\n"
     ]
    }
   ],
   "source": [
    "## Paths of ALL images into a numpy array without labels used for SSL\n",
    "def from_tif_folder_to_np_paths_array(folder_path: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Load all .tif images from a folder into a numpy array.\n",
    "    \"\"\"\n",
    "    image_paths = glob.glob(os.path.join(folder_path, \"*.tif\"))\n",
    "    image_paths_np = np.array(image_paths)\n",
    "    print(f\"Number of images in {folder_path}: {len(image_paths)}\")\n",
    "    return image_paths_np\n",
    "\n",
    "all_images_folder_path = os.path.join(data_dir, \"ALL\")\n",
    "all_images_paths = from_tif_folder_to_np_paths_array(all_images_folder_path)\n",
    "print(\"Number of images in ALL folder:\", len(all_images_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d61bcd73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MSA', 'PD']\n",
      "Number of images in /home/zano/Documents/TESI/3c_MIP_new/CONTROL: 19\n",
      "Number of images in /home/zano/Documents/TESI/3c_MIP_new/CONTROL folder: 19\n"
     ]
    }
   ],
   "source": [
    "## Paths of ALL images into a numpy array without labels used for SSL\n",
    "print(class_names)\n",
    "if class_names == ['MSA-P', 'PD']:\n",
    "    ssl_images_folder_path = os.path.join(data_dir, \"CONTROL+MSA-C\")\n",
    "else:\n",
    "    ssl_images_folder_path = os.path.join(data_dir, \"CONTROL\")\n",
    "    \n",
    "\n",
    "ssl_images_paths_np = from_tif_folder_to_np_paths_array(ssl_images_folder_path)\n",
    "print(f\"Number of images in {ssl_images_folder_path} folder:\", len(ssl_images_paths_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ba51763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MSA': '/home/zano/Documents/TESI/3c_MIP_new/MSA', 'PD': '/home/zano/Documents/TESI/3c_MIP_new/PD'}\n",
      "Class directories:\n",
      "{'MSA': '/home/zano/Documents/TESI/3c_MIP_new/MSA', 'PD': '/home/zano/Documents/TESI/3c_MIP_new/PD'}\n",
      "MSA images (before filtering): 'gh' count: 83, 'vaso' count: 0\n",
      "Number of glandular images before filtering: 83\n",
      "Number of glandular images after filtering: 83\n",
      "PD images (before filtering): 'gh' count: 57, 'vaso' count: 0\n",
      "Number of glandular images before filtering: 57\n",
      "Number of glandular images after filtering: 57\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking image shapes:\n",
      "MSA image: MAX_4092.lif - 4092 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_4092.lif - 4092 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_4121.lif - 4121 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_4121.lif - 4121 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5358.lif - 5358 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5358.lif - 5358 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5435 gh.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5435 gh2.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5463 gh.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5717.lif - 5717 DL VIP r TH b Sinapto gr DAPI grey 63x z2 gh 2 pinhole 1 z 05.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5717.lif - 5717 DL VIP r TH b Sinapto gr DAPI grey 63x z2 gh pinhole 1 z 05.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5745 gh.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5745 gh2.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5753 gh.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5753 gh2.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5753 gh3.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5767.lif - 5767 DL VIP r TH b Sinapto gr DAPI grey 63x z2 gh pinhole 1 z 05.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5767.lif - 5767 DL VIP r TH b Sinapto gr DAPI grey 63x z2 gh2 pinhole 1 z 05.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5776 gh.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5776 gh2.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5878.lif - 5878 DL VIP r TH b Sinapto gr DAPI grey 63x z2 gh2 pinhole 1 z 05.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5878.lif - 5878 DL VIP r TH b Sinapto gr DAPIgrey 63x z2 gh pinhole 1 z 05.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5881 gh.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5881 gh2.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5904 gh.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5904 gh2.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5954 gh.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5954 gh2.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5969 gh.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5969 gh2.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5978 gh.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5992 gh.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5992 gh2.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5996 gh.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_5996 gh2.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_6046 gh.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_6046 gh2.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_6053 gh.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_6053 gh2.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_6060 gh.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_6085 gh.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_6085 gh2.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_6179 gh.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_6179 gh2.tif.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_6308.lif - 6308 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_6308.lif - 6308 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_6311.lif - 6311 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_6311.lif - 6311 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_6326.lif - 6326 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_6326.lif - 6326 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_6485.lif - 6485 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_6485.lif - 6485 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_6491.lif - 6491 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_6491.lif - 6491 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_6593.lif - 6593 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_6593.lif - 6593 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_6599.lif - 6599 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_6599.lif - 6599 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_6657.lif - 6657 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_6657.lif - 6657 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_6663.lif - 6663 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_6663.lif - 6663 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_7105.lif - 7105 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh  n2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_7105.lif - 7105 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_7120.lif - 7120 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_7120.lif - 7120 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_7132.lif - 7132 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh n2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_7132.lif - 7132 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_7144.lif - 7144 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_7144.lif - 7144 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_7179.lif - 7179 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh  n2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_7179.lif - 7179 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_7185.lif - 7185 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh  n2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_7185.lif - 7185 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_7191.lif - 7191 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh  n2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_7191.lif - 7191 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_7239.lif - 7239 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_7239.lif - 7239 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_7293.lif - 7293 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_7293.lif - 7293 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_7343.lif - 7343 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_7343.lif - 7343 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "MSA image: MAX_7579.lif - 7579 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6008.lif - 6008 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh  n2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6008.lif - 6008 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6320 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6323.lif - 6323 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh  n2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6323.lif - 6323 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6337.lif - 6337 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh  n2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6337.lif - 6337 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6340.lif - 6340 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh  n2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6340.lif - 6340 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6351.lif - 6351 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh n2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6351.lif - 6351 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6363.lif - 6363 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6363.lif - 6363 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6366.lif - 6366 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh n2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6366.lif - 6366 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6375.lif - 6375 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6375.lif - 6375 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6383.lif - 6383 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6383.lif - 6383 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6424.lif - 6424 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6424.lif - 6424 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6427.lif - 6427 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6427.lif - 6427 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6459.lif - 6459 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh  n2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6459.lif - 6459 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6571.lif - 6571 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6571.lif - 6571 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6577.lif - 6577 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh  n2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6577.lif - 6577 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6616.lif - 6616 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh  n2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6616.lif - 6616 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6651.lif - 6651 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6651.lif - 6651 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6690.lif - 6690 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh  n2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6690.lif - 6690 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6696.lif - 6696 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh  n2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6696.lif - 6696 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6749.lif - 6749 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6749.lif - 6749 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6773.lif - 6773 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh  n2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6773.lif - 6773 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6791.lif - 6791 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_6791.lif - 6791 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_7155.lif - 7155 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_7155.lif - 7155 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_7222.lif - 7222 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_7222.lif - 7222 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_7229.lif - 7229 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_7229.lif - 7229 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_7284.lif - 7284 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_7284.lif - 7284 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_7461.lif - 7461 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_7461.lif - 7461 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh2.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_7544.lif - 7544 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_7677.lif - 7677 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_7688.lif - 7688 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "PD image: MAX_7710.lif - 7710 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif  dtype: uint8, shape: (3, 1024, 1024)\n",
      "\n",
      "Minority label for resampling purposes: 1\n",
      "\n",
      "Sample of image paths: ('/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_4092.lif - 4092 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif', '/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_4092.lif - 4092 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif', '/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_4121.lif - 4121 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif', '/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_4121.lif - 4121 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif', '/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5358.lif - 5358 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif')\n",
      "Total images found: 140\n",
      "\n",
      "Sample of image paths (NumPy): ['/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_4092.lif - 4092 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif'\n",
      " '/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_4092.lif - 4092 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif'\n",
      " '/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_4121.lif - 4121 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif'\n",
      " '/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_4121.lif - 4121 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif'\n",
      " '/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5358.lif - 5358 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif']\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary mapping each class to its directory\n",
    "class_dirs = {} # { \"class_name\": \"path/to/class_dir\", \"class_name2\": \"path/to/class_dir2\", ... }\n",
    "is_three_classes = (len(class_names) == 3)\n",
    "\n",
    "for class_name in class_names:\n",
    "    class_dirs[class_name] = os.path.join(data_dir, class_name)\n",
    "    \n",
    "print(class_dirs)\n",
    "if is_three_classes:\n",
    "    class2_name, class1_name, class0_name = class_names\n",
    "    class2_dir, class1_dir, class0_dir = class_dirs.values()\n",
    "else:\n",
    "    class1_name, class0_name = class_names\n",
    "    class1_dir, class0_dir = class_dirs.values()\n",
    "\n",
    "print(\"Class directories:\")\n",
    "print(class_dirs)\n",
    "\n",
    "# Dictionaries to store image paths and counts for each class\n",
    "images_paths_dict = {}\n",
    "counts_dict = {}\n",
    "\n",
    "# Loop over classes to process each folder\n",
    "for class_name in class_names:\n",
    "    class_dir = class_dirs[class_name]\n",
    "    image_paths = sorted(glob.glob(os.path.join(class_dir, \"*.tif\")))\n",
    "    \n",
    "    # Check if images were found; otherwise raise an error\n",
    "    if not image_paths:\n",
    "        raise FileNotFoundError(f\"No TIFF image file found in {class_dir}\")\n",
    "    \n",
    "    # Count occurrences of 'gh' and 'vaso' in the filenames (using .lower() for case insensitivity)\n",
    "    gh_count = sum('gh' in os.path.basename(path).lower() for path in image_paths)\n",
    "    vaso_count = sum('vaso' in os.path.basename(path).lower() for path in image_paths)\n",
    "    print(f\"{class_name} images (before filtering): 'gh' count: {gh_count}, 'vaso' count: {vaso_count}\")\n",
    "    \n",
    "    # Filter out images that contain 'vaso' (if needed)\n",
    "    from utils.data_extraction_functions import remove_non_gland_images\n",
    "    image_paths = remove_non_gland_images(image_paths)\n",
    "    # counts after filtering\n",
    "    gh_count_after = sum('gh' in os.path.basename(path).lower() for path in image_paths)\n",
    "    vaso_count_after = sum('vaso' in os.path.basename(path).lower() for path in image_paths)\n",
    "    # print(f\"After removing 'vaso', {class_name} images: 'gh' count: {gh_count_after}, 'vaso' count: {vaso_count_after}\")\n",
    "    \n",
    "    # Store the filtered image paths and counts for later use\n",
    "    images_paths_dict[class_name] = image_paths\n",
    "    counts_dict[class_name] = {\"gh_count\": gh_count_after, \"vaso_count\": vaso_count_after}\n",
    "\n",
    "# Visualize the number of 'gh' counts per class in a bar chart\n",
    "# def plot_counts_bar_chart(counts_dict, class_names):\n",
    "#     \"\"\"\n",
    "#     Plot a bar chart of counts for each class.\n",
    "#     \"\"\"\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.bar(class_names, [counts_dict[cn][\"gh_count\"] for cn in class_names], color='blue')\n",
    "#     plt.xlabel(\"Class\")\n",
    "#     plt.ylabel(\"Number of 'gh' occurrences\")\n",
    "#     plt.title(\"Number of 'gh' occurrences per class\")\n",
    "#     plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Number of 'gh' occurrences\")\n",
    "plt.title(\"Number of 'gh' occurrences per class\")\n",
    "bar_heights = [counts_dict[cn][\"gh_count\"] for cn in class_names]\n",
    "bar_colors = ['red', 'blue', 'lightblue']\n",
    "plt.bar(class_names, bar_heights, color=bar_colors)\n",
    "plt.show()\n",
    "\n",
    "# --- Debug: Check image shapes after initial loading ---\n",
    "print(\"\\nChecking image shapes:\")\n",
    "for class_name, image_paths in images_paths_dict.items():\n",
    "    for path in image_paths:\n",
    "        img = tifffile.imread(path)  # Read image as a numpy array\n",
    "        print(f\"{class_name} image: {os.path.basename(path)}  dtype: {img.dtype}, shape: {img.shape}\")\n",
    "\n",
    "# Combine image paths and labels for the three classes; \n",
    "# the label here is simply the index of the class in class_names (0, 1, 2)\n",
    "combined = [] # List to store tuples of (image_path, label)\n",
    "for label, class_name in enumerate(class_names):\n",
    "    for path in images_paths_dict[class_name]:\n",
    "        combined.append((path, label))\n",
    "# print(\"\\nSample of combined image paths and labels:\", combined[:5])\n",
    "# random.shuffle(combined)  # Shuffle the combined list to mix classes\n",
    "\n",
    "# Optionally, determine the minority label for resampling purposes\n",
    "counts = {label: len(images_paths_dict[class_name]) for label, class_name in enumerate(class_names)}\n",
    "minority_label = min(counts.keys(), key=lambda k: counts[k])\n",
    "print(f\"\\nMinority label for resampling purposes: {minority_label}\")\n",
    "\n",
    "# Unzip the combined list back into separate tuples (if needed)\n",
    "images_paths, labels = zip(*combined)\n",
    "print(\"\\nSample of image paths:\", images_paths[:5])\n",
    "print(\"Total images found:\", len(combined))\n",
    "\n",
    "# Optionally, convert to NumPy arrays (helpful for further processing or k-fold splitting)\n",
    "images_paths_np = np.array(images_paths)\n",
    "labels_np = np.array(labels)\n",
    "print(\"\\nSample of image paths (NumPy):\", images_paths_np[:5])\n",
    "print((labels_np))\n",
    "#print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5763ba96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 140\n",
      "Original label distribution: {0: 83, 1: 57}\n",
      "\n",
      "Aiming for a balanced test set with 57 samples per class.\n",
      "Total balanced test set size will be: 114\n",
      "Test set size: 114\n",
      "\n",
      "Test set distribution: {0: 57, 1: 57}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set counts and percentages:\n",
      "MSA: 57 images (50.0% of test set)\n",
      "PD: 57 images (50.0% of test set)\n"
     ]
    }
   ],
   "source": [
    "# --- Your Split Logic for 50/50 distribution in test set ---\n",
    "print(\"Original dataset size:\", len(images_paths_np))\n",
    "\n",
    "# Find unique labels and their counts in the original dataset\n",
    "unique_labels, counts = np.unique(labels_np, return_counts=True)\n",
    "original_distribution = dict(zip(unique_labels, counts))\n",
    "print(f\"Original label distribution: {original_distribution}\")\n",
    "\n",
    "# Determine the maximum possible size for a balanced test set per class\n",
    "# This is limited by the count of the smallest class\n",
    "if len(unique_labels) > 1:\n",
    "    min_class_count = min(counts)\n",
    "    # We want a balanced test set, so take 'min_class_count' samples from each class\n",
    "    test_samples_per_class = min_class_count\n",
    "    total_balanced_test_size = test_samples_per_class * len(unique_labels)\n",
    "\n",
    "    print(f\"\\nAiming for a balanced test set with {test_samples_per_class} samples per class.\")\n",
    "    print(f\"Total balanced test set size will be: {total_balanced_test_size}\")\n",
    "\n",
    "    test_indices = []\n",
    "    train_indices = []\n",
    "\n",
    "    # Iterate through each class to split\n",
    "    for label in unique_labels:\n",
    "        # Get the indices in the original array that correspond to the current class\n",
    "        # print ( labels_np == label) # returns a boolean array\n",
    "        class_indices = np.where(labels_np == label)[0]  #use the boolean array to get the indices where cond is true \n",
    "        # print(f\"\\nClass {label} indices: {class_indices}\") #retuns the indices of the class in the original array and the boolarray so we use [0] to get the indices\n",
    "\n",
    "        # Randomly select a fixed number of indices for the test set from this class\n",
    "        # Use np.random.choice with replace=False for sampling without replacement\n",
    "        # Set a random_state for reproducibility if needed\n",
    "        rng = np.random.default_rng(42) # Use new random generator recommended over np.random.seed\n",
    "        test_class_indices = rng.choice(\n",
    "            class_indices,\n",
    "            size=test_samples_per_class,\n",
    "            replace=False\n",
    "        )\n",
    "        test_indices.extend(test_class_indices)\n",
    "\n",
    "    # Convert lists of indices to NumPy arrays\n",
    "    test_indices = np.array(test_indices)\n",
    "    train_indices = np.array(train_indices)\n",
    "\n",
    "    # Shuffle the indices to mix up the classes in the final arrays (optional but good practice)\n",
    "    # rng.shuffle(test_indices)\n",
    "    # rng.shuffle(train_indices)\n",
    "\n",
    "    balanced_test_images_paths = images_paths_np[test_indices]\n",
    "    balanced_test_true_labels = labels_np[test_indices]\n",
    "    print(f\"Test set size: {len(balanced_test_images_paths)}\")\n",
    "\n",
    "    # Verify the test set distribution\n",
    "    test_unique_labels, test_counts = np.unique(balanced_test_true_labels, return_counts=True)\n",
    "    test_distribution = dict(zip(test_unique_labels, test_counts))\n",
    "    print(f\"\\nTest set distribution: {test_distribution}\")\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    # Use test_unique_labels and test_counts for the bar plot\n",
    "    labels_for_plot = [class_names[label] if 'class_names' in locals() else f\"Label {label}\" for label in test_unique_labels]\n",
    "    plt.bar(labels_for_plot, test_counts)\n",
    "    plt.xlabel(\"Class\")\n",
    "    plt.ylabel(\"Number of test images\")\n",
    "    plt.title(\"Test Set Label Distribution (Balanced)\")\n",
    "    plt.show()\n",
    "\n",
    "    # Print counts and percentages for the balanced test set\n",
    "    print(\"\\nTest set counts and percentages:\")\n",
    "    for label, count in zip(test_unique_labels, test_counts):\n",
    "         class_name = class_names[label] if 'class_names' in locals() else f\"Label {label}\"\n",
    "         print(f\"{class_name}: {count} images ({count/len(balanced_test_true_labels):.1%} of test set)\")\n",
    "\n",
    "else:\n",
    "    print(\"Cannot perform a balanced split with less than two unique classes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e782dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119 training images\n",
      "21 test images\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0 (MSA): 12 images\n",
      "Label 1 (PD): 9 images\n",
      "Label 0 (MSA) is: 0.5714285714285714\n",
      "Label 1 (PD) is: 0.42857142857142855\n"
     ]
    }
   ],
   "source": [
    "# NB the test set must be splitted BEFORE oversampling to avoid data leakage!\n",
    "# -------------------------------------------------------------------------\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#returns numpy arrays containing the paths to images and the labels\n",
    "train_images_paths, test_images_paths, train_true_labels, test_true_labels = train_test_split(\n",
    "    images_paths_np,\n",
    "    labels_np,\n",
    "    test_size= 0.15,\n",
    "    stratify=labels,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "test_images_paths_np = np.array(test_images_paths)\n",
    "test_true_labels_np = np.array(test_true_labels)\n",
    "# print(\"train images paths:\", train_images_paths)\n",
    "# print(\"true test labels:\", test_true_labels)\n",
    "# # For the cross-validation, we'll use train_images_paths and labels_temp\n",
    "train_images_paths_np = np.array(train_images_paths) #contains the images paths\n",
    "train_labels_np = np.array(train_true_labels) #contains the labels\n",
    "print(f\"{train_images_paths_np.shape[0]} training images\")\n",
    "print(f\"{len(test_images_paths)} test images\")\n",
    "#test_images_paths = [os.path.basename(path) for path in test_images_paths]\n",
    "# print(test_images_paths)\n",
    "print(type(train_images_paths))\n",
    "\n",
    "unique_labels, counts = np.unique(test_true_labels_np, return_counts=True)\n",
    "\n",
    "\n",
    "plt.bar([class_names[label] for label in unique_labels], counts)\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Number of test images\")\n",
    "plt.title(\"Test Set Label Distribution\")\n",
    "plt.show()\n",
    "\n",
    "for label, count in zip(unique_labels, counts):\n",
    "    print(f\"Label {label} ({class_names[label]}): {count} images\")\n",
    "\n",
    "for label, count in zip(unique_labels, counts):\n",
    "    print(f\"Label {label} ({class_names[label]}) is: {count/test_true_labels_np.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448f2055",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Paths of ALL images into a numpy array without labels used for SSL\n",
    "print(class_names)\n",
    "if class_names == ['MSA-P', 'PD']:\n",
    "    ssl_images_folder_path = os.path.join(data_dir, \"CONTROL+MSA-C\")\n",
    "else:\n",
    "    ssl_images_folder_path = os.path.join(data_dir, \"CONTROL\")\n",
    "    \n",
    "ssl_images_paths = glob.glob(os.path.join(ssl_images_folder_path, \"*.tif\"))\n",
    "ssl_images_paths_np = np.array(ssl_images_paths)\n",
    "print(\"Number of images in ALL folder:\", len(ssl_images_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f60f714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "image_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "label",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "patient_id",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "a80a70e7-0d18-49a9-b669-2dea6580dc0f",
       "rows": [
        [
         "0",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_4092.lif - 4092 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif",
         "0",
         "4092"
        ],
        [
         "1",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_4092.lif - 4092 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif",
         "0",
         "4092"
        ],
        [
         "2",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_4121.lif - 4121 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif",
         "0",
         "4121"
        ],
        [
         "3",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_4121.lif - 4121 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif",
         "0",
         "4121"
        ],
        [
         "4",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5358.lif - 5358 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif",
         "0",
         "5358"
        ],
        [
         "5",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5358.lif - 5358 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh2.tif",
         "0",
         "5358"
        ],
        [
         "6",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5435 gh.tif.tif",
         "0",
         "5435"
        ],
        [
         "7",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5435 gh2.tif.tif",
         "0",
         "5435"
        ],
        [
         "8",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5463 gh.tif.tif",
         "0",
         "5463"
        ],
        [
         "9",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5717.lif - 5717 DL VIP r TH b Sinapto gr DAPI grey 63x z2 gh 2 pinhole 1 z 05.tif",
         "0",
         "5717"
        ],
        [
         "10",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5717.lif - 5717 DL VIP r TH b Sinapto gr DAPI grey 63x z2 gh pinhole 1 z 05.tif",
         "0",
         "5717"
        ],
        [
         "11",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5745 gh.tif.tif",
         "0",
         "5745"
        ],
        [
         "12",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5745 gh2.tif.tif",
         "0",
         "5745"
        ],
        [
         "13",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5753 gh.tif.tif",
         "0",
         "5753"
        ],
        [
         "14",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5753 gh2.tif.tif",
         "0",
         "5753"
        ],
        [
         "15",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5753 gh3.tif.tif",
         "0",
         "5753"
        ],
        [
         "16",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5767.lif - 5767 DL VIP r TH b Sinapto gr DAPI grey 63x z2 gh pinhole 1 z 05.tif",
         "0",
         "5767"
        ],
        [
         "17",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5767.lif - 5767 DL VIP r TH b Sinapto gr DAPI grey 63x z2 gh2 pinhole 1 z 05.tif",
         "0",
         "5767"
        ],
        [
         "18",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5776 gh.tif.tif",
         "0",
         "5776"
        ],
        [
         "19",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5776 gh2.tif.tif",
         "0",
         "5776"
        ],
        [
         "20",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5878.lif - 5878 DL VIP r TH b Sinapto gr DAPI grey 63x z2 gh2 pinhole 1 z 05.tif",
         "0",
         "5878"
        ],
        [
         "21",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5878.lif - 5878 DL VIP r TH b Sinapto gr DAPIgrey 63x z2 gh pinhole 1 z 05.tif",
         "0",
         "5878"
        ],
        [
         "22",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5881 gh.tif.tif",
         "0",
         "5881"
        ],
        [
         "23",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5881 gh2.tif.tif",
         "0",
         "5881"
        ],
        [
         "24",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5904 gh.tif.tif",
         "0",
         "5904"
        ],
        [
         "25",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5904 gh2.tif.tif",
         "0",
         "5904"
        ],
        [
         "26",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5954 gh.tif.tif",
         "0",
         "5954"
        ],
        [
         "27",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5954 gh2.tif.tif",
         "0",
         "5954"
        ],
        [
         "28",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5969 gh.tif.tif",
         "0",
         "5969"
        ],
        [
         "29",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5969 gh2.tif.tif",
         "0",
         "5969"
        ],
        [
         "30",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5978 gh.tif.tif",
         "0",
         "5978"
        ],
        [
         "31",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5992 gh.tif.tif",
         "0",
         "5992"
        ],
        [
         "32",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5992 gh2.tif.tif",
         "0",
         "5992"
        ],
        [
         "33",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5996 gh.tif.tif",
         "0",
         "5996"
        ],
        [
         "34",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5996 gh2.tif.tif",
         "0",
         "5996"
        ],
        [
         "35",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_6046 gh.tif.tif",
         "0",
         "6046"
        ],
        [
         "36",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_6046 gh2.tif.tif",
         "0",
         "6046"
        ],
        [
         "37",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_6053 gh.tif.tif",
         "0",
         "6053"
        ],
        [
         "38",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_6053 gh2.tif.tif",
         "0",
         "6053"
        ],
        [
         "39",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_6060 gh.tif.tif",
         "0",
         "6060"
        ],
        [
         "40",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_6085 gh.tif.tif",
         "0",
         "6085"
        ],
        [
         "41",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_6085 gh2.tif.tif",
         "0",
         "6085"
        ],
        [
         "42",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_6179 gh.tif.tif",
         "0",
         "6179"
        ],
        [
         "43",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_6179 gh2.tif.tif",
         "0",
         "6179"
        ],
        [
         "44",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_6308.lif - 6308 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif",
         "0",
         "6308"
        ],
        [
         "45",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_6308.lif - 6308 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif",
         "0",
         "6308"
        ],
        [
         "46",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_6311.lif - 6311 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif",
         "0",
         "6311"
        ],
        [
         "47",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_6311.lif - 6311 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif",
         "0",
         "6311"
        ],
        [
         "48",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_6326.lif - 6326 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh 2.tif",
         "0",
         "6326"
        ],
        [
         "49",
         "/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_6326.lif - 6326 DL VIP red Sinapto gr TH b D grey 63x z 2 pinhole 1 z 05 gh.tif",
         "0",
         "6326"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 140
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>label</th>\n",
       "      <th>patient_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_4...</td>\n",
       "      <td>0</td>\n",
       "      <td>4092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_4...</td>\n",
       "      <td>0</td>\n",
       "      <td>4092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_4...</td>\n",
       "      <td>0</td>\n",
       "      <td>4121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_4...</td>\n",
       "      <td>0</td>\n",
       "      <td>4121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5...</td>\n",
       "      <td>0</td>\n",
       "      <td>5358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>/home/zano/Documents/TESI/3c_MIP_new/PD/MAX_74...</td>\n",
       "      <td>1</td>\n",
       "      <td>7461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>/home/zano/Documents/TESI/3c_MIP_new/PD/MAX_75...</td>\n",
       "      <td>1</td>\n",
       "      <td>7544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>/home/zano/Documents/TESI/3c_MIP_new/PD/MAX_76...</td>\n",
       "      <td>1</td>\n",
       "      <td>7677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>/home/zano/Documents/TESI/3c_MIP_new/PD/MAX_76...</td>\n",
       "      <td>1</td>\n",
       "      <td>7688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>/home/zano/Documents/TESI/3c_MIP_new/PD/MAX_77...</td>\n",
       "      <td>1</td>\n",
       "      <td>7710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            image_path  label patient_id\n",
       "0    /home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_4...      0       4092\n",
       "1    /home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_4...      0       4092\n",
       "2    /home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_4...      0       4121\n",
       "3    /home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_4...      0       4121\n",
       "4    /home/zano/Documents/TESI/3c_MIP_new/MSA/MAX_5...      0       5358\n",
       "..                                                 ...    ...        ...\n",
       "135  /home/zano/Documents/TESI/3c_MIP_new/PD/MAX_74...      1       7461\n",
       "136  /home/zano/Documents/TESI/3c_MIP_new/PD/MAX_75...      1       7544\n",
       "137  /home/zano/Documents/TESI/3c_MIP_new/PD/MAX_76...      1       7677\n",
       "138  /home/zano/Documents/TESI/3c_MIP_new/PD/MAX_76...      1       7688\n",
       "139  /home/zano/Documents/TESI/3c_MIP_new/PD/MAX_77...      1       7710\n",
       "\n",
       "[140 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique patient IDs: ['4092' '4121' '5358' '5435' '5463' '5717' '5745' '5753' '5767' '5776'\n",
      " '5878' '5881' '5904' '5954' '5969' '5978' '5992' '5996' '6008' '6046'\n",
      " '6053' '6060' '6085' '6179' '6308' '6311' '6320' '6323' '6326' '6337'\n",
      " '6340' '6351' '6363' '6366' '6375' '6383' '6424' '6427' '6459' '6485'\n",
      " '6491' '6571' '6577' '6593' '6599' '6616' '6651' '6657' '6663' '6690'\n",
      " '6696' '6749' '6773' '6791' '7105' '7120' '7132' '7144' '7155' '7179'\n",
      " '7185' '7191' '7222' '7229' '7239' '7284' '7293' '7343' '7461' '7544'\n",
      " '7579' '7677' '7688' '7710']\n",
      "Number of unique patients: 74\n",
      "Unique patient labels: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1\n",
      " 1 1 0 0 1 1 0 0 1 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 1 1 0 1 1 1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "patient_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "label",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "70ff57a2-b3c1-4d4a-9bb1-b01dfccaeff8",
       "rows": [
        [
         "0",
         "4092",
         "0"
        ],
        [
         "1",
         "4121",
         "0"
        ],
        [
         "2",
         "5358",
         "0"
        ],
        [
         "3",
         "5435",
         "0"
        ],
        [
         "4",
         "5463",
         "0"
        ],
        [
         "5",
         "5717",
         "0"
        ],
        [
         "6",
         "5745",
         "0"
        ],
        [
         "7",
         "5753",
         "0"
        ],
        [
         "8",
         "5767",
         "0"
        ],
        [
         "9",
         "5776",
         "0"
        ],
        [
         "10",
         "5878",
         "0"
        ],
        [
         "11",
         "5881",
         "0"
        ],
        [
         "12",
         "5904",
         "0"
        ],
        [
         "13",
         "5954",
         "0"
        ],
        [
         "14",
         "5969",
         "0"
        ],
        [
         "15",
         "5978",
         "0"
        ],
        [
         "16",
         "5992",
         "0"
        ],
        [
         "17",
         "5996",
         "0"
        ],
        [
         "18",
         "6008",
         "1"
        ],
        [
         "19",
         "6046",
         "0"
        ],
        [
         "20",
         "6053",
         "0"
        ],
        [
         "21",
         "6060",
         "0"
        ],
        [
         "22",
         "6085",
         "0"
        ],
        [
         "23",
         "6179",
         "0"
        ],
        [
         "24",
         "6308",
         "0"
        ],
        [
         "25",
         "6311",
         "0"
        ],
        [
         "26",
         "6320",
         "1"
        ],
        [
         "27",
         "6323",
         "1"
        ],
        [
         "28",
         "6326",
         "0"
        ],
        [
         "29",
         "6337",
         "1"
        ],
        [
         "30",
         "6340",
         "1"
        ],
        [
         "31",
         "6351",
         "1"
        ],
        [
         "32",
         "6363",
         "1"
        ],
        [
         "33",
         "6366",
         "1"
        ],
        [
         "34",
         "6375",
         "1"
        ],
        [
         "35",
         "6383",
         "1"
        ],
        [
         "36",
         "6424",
         "1"
        ],
        [
         "37",
         "6427",
         "1"
        ],
        [
         "38",
         "6459",
         "1"
        ],
        [
         "39",
         "6485",
         "0"
        ],
        [
         "40",
         "6491",
         "0"
        ],
        [
         "41",
         "6571",
         "1"
        ],
        [
         "42",
         "6577",
         "1"
        ],
        [
         "43",
         "6593",
         "0"
        ],
        [
         "44",
         "6599",
         "0"
        ],
        [
         "45",
         "6616",
         "1"
        ],
        [
         "46",
         "6651",
         "1"
        ],
        [
         "47",
         "6657",
         "0"
        ],
        [
         "48",
         "6663",
         "0"
        ],
        [
         "49",
         "6690",
         "1"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 74
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4092</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4121</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5358</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5435</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5463</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>7544</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>7579</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>7677</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>7688</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>7710</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   patient_id  label\n",
       "0        4092      0\n",
       "1        4121      0\n",
       "2        5358      0\n",
       "3        5435      0\n",
       "4        5463      0\n",
       "..        ...    ...\n",
       "69       7544      1\n",
       "70       7579      0\n",
       "71       7677      1\n",
       "72       7688      1\n",
       "73       7710      1\n",
       "\n",
       "[74 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_patient_id(image_path):\n",
    "    # Example: parse from the file name\n",
    "    # In real code, you might have a different pattern\n",
    "    match = re.search(r'(\\d{4})', image_path)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        return \"UNKNOWN\"\n",
    "\n",
    "# Build a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"image_path\": images_paths_np,\n",
    "    \"label\": labels_np\n",
    "})\n",
    "\n",
    "df[\"patient_id\"] = df[\"image_path\"].apply(extract_patient_id)\n",
    "\n",
    "display(df)\n",
    "\n",
    "# Ensure everything is string or int\n",
    "df[\"patient_id\"] = df[\"patient_id\"].astype(str)\n",
    "\n",
    "# Now group by patient to get a single label per patient.\n",
    "# If every patient truly has exactly one label, we can just take .first()\n",
    "patient_label_df = df.groupby(\"patient_id\", as_index=False)[\"label\"].first()\n",
    "\n",
    "unique_pat_ids = patient_label_df[\"patient_id\"].values  # need these to stratify for patient\n",
    "print(f\"Unique patient IDs: {unique_pat_ids}\")\n",
    "print(f\"Number of unique patients: {len(unique_pat_ids)}\")\n",
    "pat_labels = patient_label_df[\"label\"].values\n",
    "print(f\"Unique patient labels: {pat_labels}\")\n",
    "\n",
    "patient_label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec89b219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Configuration loaded from /home/zano/Documents/TESI/TESI/configs/3c/base.yaml\n",
      "Configuration: {'data_splitting': {'random_seed': 42, 'val_set_size': 0.2, 'test_set_size': 0.1, 'num_folds': 6}, 'data_augmentation': {'resize_spatial_size': [256, 256], 'rand_flip_prob': 0.3, 'rand_flip_spatial_axes': [0, 1], 'rand_rotate90_prob': 0.3, 'rand_rotate90_max_k': 3, 'rand_gaussian_noise_prob': 0.4, 'rand_gaussian_noise_mean': 0.0, 'rand_gaussian_noise_std': 0.1}, 'data_loading': {'batch_size': 8, 'num_workers': 0}, 'model': {'model_name': 'base', 'spatial_dims': 2, 'in_channels': 3, 'out_channels': 2, 'dropout_prob': 0.1}, 'training': {'num_epochs': 50, 'early_stopping_patience': 17, 'mixup_alpha': 0, 'oversample': True, 'undersample': False, 'weighted_loss': False, 'fine_tuning': False, 'transfer_learning': True, 'freezed_layerIndex': None}, 'optimizer': {'learning_rate': '1e-4', 'optimizer_name': 'Adam', 'weight_decay': '2e-5'}, 'scheduler': {'scheduler_name': 'ReduceLROnPlateau', 'factor': 0.5, 'scheduler_patience': 9, 'threshold': '1e-4', 'min_lr': '1e-8'}}\n",
      "Configuration loaded from /home/zano/Documents/TESI/TESI/configs/3c/vit.yaml\n",
      "Configuration: {'data_splitting': {'random_seed': 42, 'val_set_size': 0.15, 'test_set_size': 0.1, 'num_folds': 6}, 'data_augmentation': {'resize_spatial_size': [512, 512], 'rand_flip_prob': 0.3, 'rand_flip_spatial_axes': [0, 1], 'rand_rotate90_prob': 0.3, 'rand_rotate90_max_k': 3, 'rand_gaussian_noise_prob': 0.5, 'rand_gaussian_noise_mean': 0.0, 'rand_gaussian_noise_std': 0.1}, 'data_loading': {'batch_size': 32, 'num_workers': 0}, 'model': {'model_name': 'ViT', 'spatial_dims': 2, 'in_channels': 3, 'out_channels': 2, 'dropout_prob': 0.1, 'patch_size': [16, 16], 'proj_type': 'conv', 'pos_embed_type': 'sincos'}, 'training': {'num_epochs': 230, 'early_stopping_patience': 40, 'mixup_alpha': 0, 'oversample': True, 'undersample': False, 'weighted_loss': False, 'fine_tuning': False, 'transfer_learning': False, 'freezed_layerIndex': None, 'pretrained': True, 'dropout_prob': 0.1}, 'optimizer': {'learning_rate': '1e-3', 'optimizer_name': 'Adam', 'weight_decay': '1e-5'}, 'scheduler': {'scheduler_name': 'ReduceLROnPlateau', 'factor': 0.5, 'scheduler_patience': 9, 'threshold': '1e-4', 'min_lr': '1e-8'}, 'num_epochs': None, 'freeze_layers': None}\n",
      "Using pretrained model: False\n",
      "Using pretrained model: False and supported by torchvision: <function is_supported_by_torchvision at 0x7744948579c0> with color transforms: False\n",
      "the model is not supported by torchvision or is not pretrained\n",
      "Model ViT not supported using custom transforms\n",
      "No fold-specific stats provided or incomplete; proceeding without specific normalization step (only ScaleIntensityd).\n",
      "Number of classes in the dataset: 2\n",
      "Using MONAI for model instantiation.\n",
      "Building MONAI ViT for classification with 3-channel input...\n",
      " -> Image size for ViT: (512, 512)\n",
      " -> Patch size for ViT: [16, 16]\n",
      "ViT(\n",
      "  (patch_embedding): PatchEmbeddingBlock(\n",
      "    (patch_embeddings): Conv2d(3, 128, kernel_size=(16, 16), stride=(16, 16))\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (blocks): ModuleList(\n",
      "    (0-3): 4 x TransformerBlock(\n",
      "      (mlp): MLPBlock(\n",
      "        (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (fn): GELU(approximate='none')\n",
      "        (drop1): Dropout(p=0.1, inplace=False)\n",
      "        (drop2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): SABlock(\n",
      "        (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (qkv): Linear(in_features=128, out_features=384, bias=False)\n",
      "        (to_q): Identity()\n",
      "        (to_k): Identity()\n",
      "        (to_v): Identity()\n",
      "        (input_rearrange): Rearrange('b h (qkv l d) -> qkv b l h d', qkv=3, l=4)\n",
      "        (out_rearrange): Rearrange('b l h d -> b h (l d)')\n",
      "        (drop_output): Dropout(p=0.1, inplace=False)\n",
      "        (drop_weights): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm_cross_attn): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (cross_attn): CrossAttentionBlock(\n",
      "        (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (to_q): Linear(in_features=128, out_features=128, bias=False)\n",
      "        (to_k): Linear(in_features=128, out_features=128, bias=False)\n",
      "        (to_v): Linear(in_features=128, out_features=128, bias=False)\n",
      "        (input_rearrange): Rearrange('b h (l d) -> b l h d', l=4)\n",
      "        (out_rearrange): Rearrange('b l h d -> b h (l d)')\n",
      "        (drop_output): Dropout(p=0.1, inplace=False)\n",
      "        (drop_weights): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (classification_head): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=2, bias=True)\n",
      "    (1): Tanh()\n",
      "  )\n",
      ")\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# ## MAE\n",
    "# %pip install \"lightly[timm]\" pytorch-lightning torch torchvision\n",
    "# %pip install --upgrade timm\n",
    "from lightly.utils import dependency as _dependency\n",
    "print(_dependency.timm_vit_available())\n",
    "from configs.ConfigLoader import ConfigLoader\n",
    "from torch.utils.data import DataLoader\n",
    "import utils.transformations_functions as tf\n",
    "# Removed redundant import: from configs.ConfigLoader import ConfigLoader\n",
    "import numpy as np\n",
    "from classes.ModelManager import ModelManager\n",
    "\n",
    "yaml_path = f\"/home/zano/Documents/TESI/TESI/configs/{num_input_channels}c/vit.yaml\"\n",
    "cfg = ConfigLoader(yaml_path) \n",
    "cfg.set_freezed_layer_index(None)\n",
    "transfer_learning = cfg.get_transfer_learning()\n",
    "pretrained_weights = \"imagenet\" if transfer_learning else \"\" # 'microscopynet' or \"imagenet\"\n",
    "model_library = \"monai\" # or \"torchvision\" or \"monai\"\n",
    "color_transforms = False\n",
    "train_transforms, val_transforms, test_transforms = tf.get_transforms(cfg,color_transforms=color_transforms)\n",
    "model_manager = ModelManager(cfg, library=model_library)\n",
    "# Verify the number of unique labels in the dataset\n",
    "num_classes = len(np.unique(train_labels_np))\n",
    "print(f\"Number of classes in the dataset: {num_classes}\")\n",
    "# Ensure the model's output matches the number of classes\n",
    "model, device = model_manager.setup_model(num_classes=num_classes, pretrained_weights=pretrained_weights)\n",
    "print(model)\n",
    "print(cfg.get_model_input_channels())\n",
    "\n",
    "###########################################################\n",
    "# mae_lightning_example.py\n",
    "# Example script for Masked Autoencoder (MAE) pre-training using PyTorch Lightning,\n",
    "# Lightly (for data handling and transforms), and TIMM (for ViT models).\n",
    "# Installation: pip install \"lightly[timm]\" pytorch-lightning torch torchvision timm\n",
    "###########################################################\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from timm.models.vision_transformer import vit_base_patch32_224 # Base Vision Transformer model from TIMM\n",
    "from torch import nn\n",
    "# from torch.utils.data import DataLoader\n",
    "# Lightly imports\n",
    "from lightly.data import LightlyDataset                        \n",
    "from lightly.transforms.mae_transform import MAETransform                    # Specific data transforms for MAE\n",
    "from lightly.models import utils                             # Helper utilities (e.g., masking)\n",
    "from lightly.models.modules import MAEDecoderTIMM, MaskedVisionTransformerTIMM # MAE-specific encoder and decoder modules\n",
    "\n",
    "###########################################################\n",
    "# 1) Callback to Print Epoch Loss\n",
    "###########################################################\n",
    "class PrintEpochLossCallback(pl.Callback):\n",
    "    \"\"\"\n",
    "    A simple PyTorch Lightning Callback that prints the average 'train_loss'\n",
    "    logged during the epoch to the console after each training epoch finishes.\n",
    "    Useful for quick monitoring when a full progress bar is disabled.\n",
    "    \"\"\"\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        # Access metrics logged during the epoch via the trainer\n",
    "        epoch_loss = trainer.callback_metrics.get(\"train_loss\")\n",
    "        current_epoch = trainer.current_epoch\n",
    "        if epoch_loss is not None:\n",
    "            # Print formatted loss\n",
    "            print(f\"[Epoch {current_epoch}] => train_loss: {epoch_loss:.4f}\")\n",
    "        else:\n",
    "            # Handle case where loss might not have been logged (e.g., first epoch sanity check)\n",
    "            print(f\"[Epoch {current_epoch}] => train_loss not found in metrics\")\n",
    "\n",
    "###########################################################\n",
    "# 2) MAE LightningModule Definition\n",
    "###########################################################\n",
    "class MAE(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Implementation of the Masked Autoencoder (MAE) self-supervised learning strategy\n",
    "    using a Vision Transformer (ViT) backbone from TIMM.\n",
    "\n",
    "    The core idea is to:\n",
    "    1. Randomly mask a large portion of input image patches.\n",
    "    2. Feed only the *visible* (unmasked) patches to the ViT encoder.\n",
    "    3. Use a lightweight decoder to reconstruct the *original pixels* of the *masked* patches\n",
    "       based on the encoded representations of the visible patches and learnable mask tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mask_ratio: float = 0.75):\n",
    "        \"\"\"\n",
    "        Initializes the MAE model components.\n",
    "\n",
    "        Args:\n",
    "            mask_ratio: The fraction of input image patches to randomly mask (hide) during training.\n",
    "                        Defaults to 0.75 (75%), as used in the original MAE paper.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters() # Saves mask_ratio to hparams\n",
    "\n",
    "        # --- Encoder Setup ---\n",
    "        # Load a pre-defined Vision Transformer base model from TIMM library.\n",
    "        # vit_base_patch32_224: Base-sized ViT, 32x32 patches, expects 224x224 input.\n",
    "        # Note: weights are not loaded here; we train from scratch or load MAE pre-trained later.\n",
    "        vit = vit_base_patch32_224(pretrained=False) # Start from scratch for pre-training\n",
    "        self.patch_size = vit.patch_embed.patch_size[0]\n",
    "\n",
    "        # Wrap the TIMM ViT with Lightly's MaskedVisionTransformerTIMM.\n",
    "        # This wrapper allows the ViT encoder to process only a subset of patch tokens (the visible ones).\n",
    "        self.backbone = MaskedVisionTransformerTIMM(vit=vit)\n",
    "\n",
    "        # Store the sequence length (number of patches + each with a class token)\n",
    "        self.sequence_length = self.backbone.sequence_length\n",
    "\n",
    "        # --- Decoder Setup ---\n",
    "        # Create the MAE decoder module. It's typically much smaller/shallower than the encoder.\n",
    "        self.decoder = MAEDecoderTIMM(\n",
    "            num_patches=vit.patch_embed.num_patches, # Total number of patches (e.g., 49)\n",
    "            patch_size=self.patch_size,              # Patch size (e.g., 32)\n",
    "            embed_dim=vit.embed_dim,                 # Encoder's embedding dimension (e.g., 768 for ViT-Base)\n",
    "            decoder_embed_dim=512,                   # Decoder's embedding dimension (can be smaller)\n",
    "            decoder_depth=1,                         # Number of transformer blocks in the decoder (shallow)\n",
    "            decoder_num_heads=16,                    # Number of attention heads in the decoder\n",
    "            mlp_ratio=4.0,                           # MLP expansion ratio in decoder blocks\n",
    "            proj_drop_rate=0.0,                      # Dropout rates (often disabled in decoder)\n",
    "            attn_drop_rate=0.0,\n",
    "        )\n",
    "\n",
    "        # --- Loss Function ---\n",
    "        # MAE reconstructs raw pixel values of the masked patches.\n",
    "        # Mean Squared Error (MSE) loss is used to measure the reconstruction quality.\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def forward_encoder(self, images: torch.Tensor, idx_keep: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Runs the forward pass through the MAE encoder (ViT backbone).\n",
    "\n",
    "        Args:\n",
    "            images: Batch of input images (B, C, H, W).\n",
    "            idx_keep: Tensor containing the indices of the patches to *keep* (process).\n",
    "                      If None, all patches are processed. Shape: (B, num_kept_patches).\n",
    "\n",
    "        Returns:\n",
    "            Tensor containing the encoded representations of the kept patches.\n",
    "            Shape: (B, num_kept_patches, encoder_embed_dim).\n",
    "        \"\"\"\n",
    "        return self.backbone.encode(images=images, idx_keep=idx_keep)\n",
    "\n",
    "    def forward_decoder(self, x_encoded: torch.Tensor, idx_keep: torch.Tensor, idx_mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Runs the forward pass through the MAE decoder to reconstruct masked patches.\n",
    "\n",
    "        Args:\n",
    "            x_encoded: Encoded representations of the visible patches from the encoder.\n",
    "                       Shape: (B, num_kept_patches, encoder_embed_dim).\n",
    "            idx_keep: Indices of the patches that were *kept* (visible) by the encoder.\n",
    "                      Shape: (B, num_kept_patches).\n",
    "            idx_mask: Indices of the patches that were *masked* (hidden) by the encoder.\n",
    "                      Shape: (B, num_masked_patches).\n",
    "\n",
    "        Returns:\n",
    "            Tensor containing the predicted pixel values for the *masked* patches.\n",
    "            Shape: (B, num_masked_patches, patch_size*patch_size*num_channels).\n",
    "        \"\"\"\n",
    "        batch_size = x_encoded.shape[0]\n",
    "\n",
    "        # 1. Project the encoded visible patches to the decoder's embedding dimension.\n",
    "        x_decode = self.decoder.embed(x_encoded) # (B, num_kept, decoder_embed_dim)\n",
    "\n",
    "        # 2. Create the full sequence input for the decoder:\n",
    "        #    - Start with learnable mask tokens for all positions.\n",
    "        #    - Place the embedded visible patches back into their original positions.\n",
    "        # Create placeholder mask tokens for the full sequence length.\n",
    "        # Shape: (B, sequence_length, decoder_embed_dim)\n",
    "        x_masked = utils.repeat_token(self.decoder.mask_token, (batch_size, self.sequence_length))\n",
    "        # Use `set_at_index` to insert the visible patch embeddings `x_decode`\n",
    "        # at the `idx_keep` positions within the `x_masked` tensor.\n",
    "        # This replaces the mask tokens at the indices where patches were kept.\n",
    "        x_masked = utils.set_at_index(x_masked, idx_keep, x_decode.type_as(x_masked))\n",
    "\n",
    "        # 3. Pass the full sequence (visible patches + mask tokens) through the decoder's transformer blocks.\n",
    "        # Shape: (B, sequence_length, decoder_embed_dim)\n",
    "        x_decoded = self.decoder.decode(x_masked)\n",
    "\n",
    "        # 4. Select only the decoder outputs corresponding to the *originally masked* patch positions.\n",
    "        # Shape: (B, num_masked_patches, decoder_embed_dim)\n",
    "        x_pred = utils.get_at_index(x_decoded, idx_mask)\n",
    "\n",
    "        # 5. Project the decoder outputs for masked patches back into pixel space.\n",
    "        # Shape: (B, num_masked_patches, patch_size*patch_size*num_channels)\n",
    "        x_pred = self.decoder.predict(x_pred)\n",
    "        return x_pred\n",
    "\n",
    "    def training_step(self, batch: tuple, batch_idx: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Performs a single training step for MAE.\n",
    "        \"\"\"\n",
    "        views, _, _ = batch # Assuming LightlyDataset structure: (views_tuple), labels, fnames\n",
    "        images = views[0]   # Get the actual image tensor: shape (B, C, H, W) \n",
    "        batch_size = images.shape[0]\n",
    "\n",
    "        # 1. Determine which patches to mask and which to keep.\n",
    "        # `random_token_mask` generates boolean masks or indices based on the mask_ratio.\n",
    "        idx_keep, idx_mask = utils.random_token_mask(\n",
    "            size=(batch_size, self.sequence_length), # (Batch Size, Num Patches + CLS Token)\n",
    "            mask_ratio=self.hparams[\"mask_ratio\"],   # Use the stored hyperparameter\n",
    "            device=images.device,\n",
    "        )\n",
    "\n",
    "        # 2. Encode the visible patches using the ViT backbone.\n",
    "        x_encoded = self.forward_encoder(images=images, idx_keep=idx_keep)\n",
    "\n",
    "        # 3. Decode and predict the pixel values for the masked patches.\n",
    "        x_pred = self.forward_decoder(x_encoded, idx_keep, idx_mask)\n",
    "\n",
    "        # 4. Prepare the target: Get the original pixel values of the masked patches.\n",
    "        # `patchify` converts the original images into flattened patches.\n",
    "        patches = utils.patchify(images, self.patch_size) # (B, num_patches, patch_size*patch_size*C)\n",
    "        # Select the original patches corresponding to the masked indices.\n",
    "        # Note: `idx_mask` might include an index for the CLS token position if the encoder uses one.\n",
    "        # `patchify` output does not have a CLS token patch, so we might need to adjust indices\n",
    "        # if the CLS token is index 0. Lightly's `utils.get_at_index` might handle this,\n",
    "        # but subtracting 1 is common if idx_mask assumes a prepended CLS token. Check MaskedVisionTransformerTIMM details.\n",
    "        # Assuming idx_mask needs offset if CLS token exists:\n",
    "        target = utils.get_at_index(patches, idx_mask - 1 if self.backbone.vit.cls_token is not None else idx_mask)\n",
    "\n",
    "        # 5. Calculate the reconstruction loss (MSE) between predicted and target patches.\n",
    "        loss = self.criterion(x_pred, target)\n",
    "\n",
    "        # Log the loss for monitoring (average over epoch).\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True) # Added prog_bar\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self) -> torch.optim.Optimizer:\n",
    "        \"\"\"\n",
    "        Sets up the optimizer for training. AdamW is commonly used for ViT/MAE.\n",
    "        Learning rate might need tuning based on dataset and training duration.\n",
    "        Consider adding a learning rate scheduler (e.g., Cosine Annealing with Warmup)\n",
    "        for longer training runs to improve stability and performance.\n",
    "        \"\"\"\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=1.5e-4, weight_decay=0.05) # Added weight decay common for MAE\n",
    "        # Example of adding a scheduler (optional, adjust parameters):\n",
    "        # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=self.trainer.max_epochs, eta_min=1e-6)\n",
    "        # return [optimizer], [scheduler]\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "28169096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for the MAE model\n",
    "DATA_DIR = ssl_images_folder_path\n",
    "INPUT_SIZE = 224 # Input size expected by the ViT model (vit_base_patch32_224)\n",
    "BATCH_SIZE = 16  # Adjust based on GPU memory (e.g., 16, 32, 64)\n",
    "NUM_WORKERS = 2  # Number of parallel workers for data loading\n",
    "MAX_EPOCHS = 250  # Number of epochs to train for (MAE often needs many more, e.g., 100+)\n",
    "SAVE_PATH = \"mae_backbone.pth\" # Path to save the trained encoder weights\n",
    "CUSTOM_NORMALIZATION = True # Set to True if you want to use custom normalization stats\n",
    "MASK_RATIO = 0.75  # Fraction of patches to mask (75% is common in MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccbc4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing MAE model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                        | Params | Mode \n",
      "------------------------------------------------------------------\n",
      "0 | backbone  | MaskedVisionTransformerTIMM | 88.2 M | train\n",
      "1 | decoder   | MAEDecoderTIMM              | 5.1 M  | train\n",
      "2 | criterion | MSELoss                     | 0      | train\n",
      "------------------------------------------------------------------\n",
      "93.3 M    Trainable params\n",
      "64.0 K    Non-trainable params\n",
      "93.4 M    Total params\n",
      "373.497   Total estimated model params size (MB)\n",
      "292       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up MAETransform with input size 224...\n",
      "Computing custom normalization stats from dataset sample...\n",
      "Loading dataset from: /home/zano/Documents/TESI/3c_MIP_new/CONTROL\n",
      "Dataset size: 19\n",
      "Creating DataLoader with batch size 16...\n",
      "Configuring PyTorch Lightning Trainer for 250 epochs...\n",
      "Starting MAE pre-training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] => train_loss: 0.8553\n",
      "[Epoch 1] => train_loss: 0.7633\n",
      "[Epoch 2] => train_loss: 0.6543\n",
      "[Epoch 3] => train_loss: 0.6939\n",
      "[Epoch 4] => train_loss: 0.6442\n",
      "[Epoch 5] => train_loss: 0.6436\n",
      "[Epoch 6] => train_loss: 0.5776\n",
      "[Epoch 7] => train_loss: 0.4755\n",
      "[Epoch 8] => train_loss: 0.4621\n",
      "[Epoch 9] => train_loss: 0.5317\n",
      "[Epoch 10] => train_loss: 0.3803\n",
      "[Epoch 11] => train_loss: 0.3673\n",
      "[Epoch 12] => train_loss: 0.4121\n",
      "[Epoch 13] => train_loss: 0.4208\n",
      "[Epoch 14] => train_loss: 0.3467\n",
      "[Epoch 15] => train_loss: 0.5310\n",
      "[Epoch 16] => train_loss: 0.4414\n",
      "[Epoch 17] => train_loss: 0.3601\n",
      "[Epoch 18] => train_loss: 0.3225\n",
      "[Epoch 19] => train_loss: 0.4140\n",
      "[Epoch 20] => train_loss: 0.4655\n",
      "[Epoch 21] => train_loss: 0.5204\n",
      "[Epoch 22] => train_loss: 0.3619\n",
      "[Epoch 23] => train_loss: 0.4642\n",
      "[Epoch 24] => train_loss: 0.4613\n",
      "[Epoch 25] => train_loss: 0.4079\n",
      "[Epoch 26] => train_loss: 0.3531\n",
      "[Epoch 27] => train_loss: 0.4301\n",
      "[Epoch 28] => train_loss: 0.3979\n",
      "[Epoch 29] => train_loss: 0.4634\n",
      "[Epoch 30] => train_loss: 0.3921\n",
      "[Epoch 31] => train_loss: 0.4777\n",
      "[Epoch 32] => train_loss: 0.2953\n",
      "[Epoch 33] => train_loss: 0.4457\n",
      "[Epoch 34] => train_loss: 0.4208\n",
      "[Epoch 35] => train_loss: 0.4162\n",
      "[Epoch 36] => train_loss: 0.3569\n",
      "[Epoch 37] => train_loss: 0.4574\n",
      "[Epoch 38] => train_loss: 0.3186\n",
      "[Epoch 39] => train_loss: 0.3471\n",
      "[Epoch 40] => train_loss: 0.2565\n",
      "[Epoch 41] => train_loss: 0.4145\n",
      "[Epoch 42] => train_loss: 0.2521\n",
      "[Epoch 43] => train_loss: 0.3152\n",
      "[Epoch 44] => train_loss: 0.3387\n",
      "[Epoch 45] => train_loss: 0.4349\n",
      "[Epoch 46] => train_loss: 0.3907\n",
      "[Epoch 47] => train_loss: 0.3677\n",
      "[Epoch 48] => train_loss: 0.3708\n",
      "[Epoch 49] => train_loss: 0.3578\n",
      "[Epoch 50] => train_loss: 0.3464\n",
      "[Epoch 51] => train_loss: 0.3986\n",
      "[Epoch 52] => train_loss: 0.4067\n",
      "[Epoch 53] => train_loss: 0.3976\n",
      "[Epoch 54] => train_loss: 0.3188\n",
      "[Epoch 55] => train_loss: 0.3916\n",
      "[Epoch 56] => train_loss: 0.3487\n",
      "[Epoch 57] => train_loss: 0.3578\n",
      "[Epoch 58] => train_loss: 0.3198\n",
      "[Epoch 59] => train_loss: 0.3387\n",
      "[Epoch 60] => train_loss: 0.4249\n",
      "[Epoch 61] => train_loss: 0.2825\n",
      "[Epoch 62] => train_loss: 0.4365\n",
      "[Epoch 63] => train_loss: 0.4382\n",
      "[Epoch 64] => train_loss: 0.4472\n",
      "[Epoch 65] => train_loss: 0.3061\n",
      "[Epoch 66] => train_loss: 0.3958\n",
      "[Epoch 67] => train_loss: 0.4339\n",
      "[Epoch 68] => train_loss: 0.4131\n",
      "[Epoch 69] => train_loss: 0.3559\n",
      "[Epoch 70] => train_loss: 0.2993\n",
      "[Epoch 71] => train_loss: 0.4158\n",
      "[Epoch 72] => train_loss: 0.4207\n",
      "[Epoch 73] => train_loss: 0.3432\n",
      "[Epoch 74] => train_loss: 0.4250\n",
      "[Epoch 75] => train_loss: 0.3702\n",
      "[Epoch 76] => train_loss: 0.3784\n",
      "[Epoch 77] => train_loss: 0.3752\n",
      "[Epoch 78] => train_loss: 0.3847\n",
      "[Epoch 79] => train_loss: 0.3806\n",
      "[Epoch 80] => train_loss: 0.3272\n",
      "[Epoch 81] => train_loss: 0.3013\n",
      "[Epoch 82] => train_loss: 0.3070\n",
      "[Epoch 83] => train_loss: 0.3883\n",
      "[Epoch 84] => train_loss: 0.3840\n",
      "[Epoch 85] => train_loss: 0.3439\n",
      "[Epoch 86] => train_loss: 0.4322\n",
      "[Epoch 87] => train_loss: 0.3434\n",
      "[Epoch 88] => train_loss: 0.4498\n",
      "[Epoch 89] => train_loss: 0.3166\n",
      "[Epoch 90] => train_loss: 0.3534\n",
      "[Epoch 91] => train_loss: 0.2838\n",
      "[Epoch 92] => train_loss: 0.4493\n",
      "[Epoch 93] => train_loss: 0.4212\n",
      "[Epoch 94] => train_loss: 0.3589\n",
      "[Epoch 95] => train_loss: 0.3462\n",
      "[Epoch 96] => train_loss: 0.2976\n",
      "[Epoch 97] => train_loss: 0.3176\n",
      "[Epoch 98] => train_loss: 0.3542\n",
      "[Epoch 99] => train_loss: 0.4567\n",
      "[Epoch 100] => train_loss: 0.3679\n",
      "[Epoch 101] => train_loss: 0.4276\n",
      "[Epoch 102] => train_loss: 0.2290\n",
      "[Epoch 103] => train_loss: 0.3975\n",
      "[Epoch 104] => train_loss: 0.2975\n",
      "[Epoch 105] => train_loss: 0.2868\n",
      "[Epoch 106] => train_loss: 0.4571\n",
      "[Epoch 107] => train_loss: 0.3850\n",
      "[Epoch 108] => train_loss: 0.4143\n",
      "[Epoch 109] => train_loss: 0.3447\n",
      "[Epoch 110] => train_loss: 0.3777\n",
      "[Epoch 111] => train_loss: 0.3933\n",
      "[Epoch 112] => train_loss: 0.2482\n",
      "[Epoch 113] => train_loss: 0.3879\n",
      "[Epoch 114] => train_loss: 0.2778\n",
      "[Epoch 115] => train_loss: 0.3984\n",
      "[Epoch 116] => train_loss: 0.4347\n",
      "[Epoch 117] => train_loss: 0.4389\n",
      "[Epoch 118] => train_loss: 0.3090\n",
      "[Epoch 119] => train_loss: 0.3905\n",
      "[Epoch 120] => train_loss: 0.3578\n",
      "[Epoch 121] => train_loss: 0.2587\n",
      "[Epoch 122] => train_loss: 0.3268\n",
      "[Epoch 123] => train_loss: 0.3924\n",
      "[Epoch 124] => train_loss: 0.4440\n",
      "[Epoch 125] => train_loss: 0.2669\n",
      "[Epoch 126] => train_loss: 0.4401\n",
      "[Epoch 127] => train_loss: 0.4100\n",
      "[Epoch 128] => train_loss: 0.3157\n",
      "[Epoch 129] => train_loss: 0.3545\n",
      "[Epoch 130] => train_loss: 0.3190\n",
      "[Epoch 131] => train_loss: 0.2645\n",
      "[Epoch 132] => train_loss: 0.3712\n",
      "[Epoch 133] => train_loss: 0.3908\n",
      "[Epoch 134] => train_loss: 0.3653\n",
      "[Epoch 135] => train_loss: 0.4501\n",
      "[Epoch 136] => train_loss: 0.3549\n",
      "[Epoch 137] => train_loss: 0.2691\n",
      "[Epoch 138] => train_loss: 0.3889\n",
      "[Epoch 139] => train_loss: 0.2955\n",
      "[Epoch 140] => train_loss: 0.4213\n",
      "[Epoch 141] => train_loss: 0.4548\n",
      "[Epoch 142] => train_loss: 0.3630\n",
      "[Epoch 143] => train_loss: 0.3467\n",
      "[Epoch 144] => train_loss: 0.3801\n",
      "[Epoch 145] => train_loss: 0.2672\n",
      "[Epoch 146] => train_loss: 0.2915\n",
      "[Epoch 147] => train_loss: 0.2698\n",
      "[Epoch 148] => train_loss: 0.3266\n",
      "[Epoch 149] => train_loss: 0.2836\n",
      "[Epoch 150] => train_loss: 0.2769\n",
      "[Epoch 151] => train_loss: 0.3779\n",
      "[Epoch 152] => train_loss: 0.4039\n",
      "[Epoch 153] => train_loss: 0.4136\n",
      "[Epoch 154] => train_loss: 0.2681\n",
      "[Epoch 155] => train_loss: 0.4985\n",
      "[Epoch 156] => train_loss: 0.4311\n",
      "[Epoch 157] => train_loss: 0.3388\n",
      "[Epoch 158] => train_loss: 0.3058\n",
      "[Epoch 159] => train_loss: 0.4071\n",
      "[Epoch 160] => train_loss: 0.4187\n",
      "[Epoch 161] => train_loss: 0.3845\n",
      "[Epoch 162] => train_loss: 0.3136\n",
      "[Epoch 163] => train_loss: 0.3614\n",
      "[Epoch 164] => train_loss: 0.3775\n",
      "[Epoch 165] => train_loss: 0.4667\n",
      "[Epoch 166] => train_loss: 0.3830\n",
      "[Epoch 167] => train_loss: 0.3670\n",
      "[Epoch 168] => train_loss: 0.3352\n",
      "[Epoch 169] => train_loss: 0.4035\n",
      "[Epoch 170] => train_loss: 0.4041\n",
      "[Epoch 171] => train_loss: 0.3441\n",
      "[Epoch 172] => train_loss: 0.3155\n",
      "[Epoch 173] => train_loss: 0.4282\n",
      "[Epoch 174] => train_loss: 0.2876\n",
      "[Epoch 175] => train_loss: 0.3528\n",
      "[Epoch 176] => train_loss: 0.4265\n",
      "[Epoch 177] => train_loss: 0.4317\n",
      "[Epoch 178] => train_loss: 0.3145\n",
      "[Epoch 179] => train_loss: 0.2785\n",
      "[Epoch 180] => train_loss: 0.4617\n",
      "[Epoch 181] => train_loss: 0.4260\n",
      "[Epoch 182] => train_loss: 0.4111\n",
      "[Epoch 183] => train_loss: 0.3937\n",
      "[Epoch 184] => train_loss: 0.2791\n",
      "[Epoch 185] => train_loss: 0.2820\n",
      "[Epoch 186] => train_loss: 0.2576\n",
      "[Epoch 187] => train_loss: 0.3045\n",
      "[Epoch 188] => train_loss: 0.2898\n",
      "[Epoch 189] => train_loss: 0.3172\n",
      "[Epoch 190] => train_loss: 0.3168\n",
      "[Epoch 191] => train_loss: 0.3859\n",
      "[Epoch 192] => train_loss: 0.3124\n",
      "[Epoch 193] => train_loss: 0.3845\n",
      "[Epoch 194] => train_loss: 0.3196\n",
      "[Epoch 195] => train_loss: 0.3581\n",
      "[Epoch 196] => train_loss: 0.3911\n",
      "[Epoch 197] => train_loss: 0.4377\n",
      "[Epoch 198] => train_loss: 0.3993\n",
      "[Epoch 199] => train_loss: 0.3755\n",
      "[Epoch 200] => train_loss: 0.3004\n",
      "[Epoch 201] => train_loss: 0.2934\n",
      "[Epoch 202] => train_loss: 0.3705\n",
      "[Epoch 203] => train_loss: 0.3858\n",
      "[Epoch 204] => train_loss: 0.3542\n",
      "[Epoch 205] => train_loss: 0.4357\n",
      "[Epoch 206] => train_loss: 0.3765\n",
      "[Epoch 207] => train_loss: 0.2564\n",
      "[Epoch 208] => train_loss: 0.2785\n",
      "[Epoch 209] => train_loss: 0.4727\n",
      "[Epoch 210] => train_loss: 0.3457\n",
      "[Epoch 211] => train_loss: 0.5094\n",
      "[Epoch 212] => train_loss: 0.4939\n",
      "[Epoch 213] => train_loss: 0.3631\n",
      "[Epoch 214] => train_loss: 0.4086\n",
      "[Epoch 215] => train_loss: 0.3697\n",
      "[Epoch 216] => train_loss: 0.3872\n",
      "[Epoch 217] => train_loss: 0.3781\n",
      "[Epoch 218] => train_loss: 0.3991\n",
      "[Epoch 219] => train_loss: 0.3773\n",
      "[Epoch 220] => train_loss: 0.3828\n",
      "[Epoch 221] => train_loss: 0.3256\n",
      "[Epoch 222] => train_loss: 0.3523\n",
      "[Epoch 223] => train_loss: 0.3951\n",
      "[Epoch 224] => train_loss: 0.2989\n",
      "[Epoch 225] => train_loss: 0.2684\n",
      "[Epoch 226] => train_loss: 0.3113\n",
      "[Epoch 227] => train_loss: 0.3640\n",
      "[Epoch 228] => train_loss: 0.3905\n",
      "[Epoch 229] => train_loss: 0.3207\n",
      "[Epoch 230] => train_loss: 0.2260\n",
      "[Epoch 231] => train_loss: 0.3742\n",
      "[Epoch 232] => train_loss: 0.3502\n",
      "[Epoch 233] => train_loss: 0.4274\n",
      "[Epoch 234] => train_loss: 0.2847\n",
      "[Epoch 235] => train_loss: 0.3178\n",
      "[Epoch 236] => train_loss: 0.3780\n",
      "[Epoch 237] => train_loss: 0.3275\n",
      "[Epoch 238] => train_loss: 0.3088\n",
      "[Epoch 239] => train_loss: 0.3870\n",
      "[Epoch 240] => train_loss: 0.3570\n",
      "[Epoch 241] => train_loss: 0.3785\n",
      "[Epoch 242] => train_loss: 0.3761\n",
      "[Epoch 243] => train_loss: 0.3754\n",
      "[Epoch 244] => train_loss: 0.3930\n",
      "[Epoch 245] => train_loss: 0.3858\n",
      "[Epoch 246] => train_loss: 0.4405\n",
      "[Epoch 247] => train_loss: 0.3047\n",
      "[Epoch 248] => train_loss: 0.3487\n",
      "[Epoch 249] => train_loss: 0.3891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=250` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE pre-training finished.\n",
      "Saving trained MAE encoder backbone to: mae_backbone.pth\n",
      "Backbone saved successfully.\n"
     ]
    }
   ],
   "source": [
    "###########################################################\n",
    "# 3) Main Function to Set Up and Run Training\n",
    "###########################################################\n",
    "# --- Configuration ---\n",
    "from utils.transformations_functions import compute_dataset_mean_std\n",
    "from classes.MAECustomTransform import MAECustomTransform\n",
    "from lightly.data import LightlyDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "# 1) Create the MAE model instance\n",
    "model = MAE(mask_ratio=MASK_RATIO) # Can adjust mask_ratio here if needed\n",
    "\n",
    "# 2) Create the specific MAE data transform\n",
    "# This handles resizing to INPUT_SIZE, normalization (usually ImageNet stats),\n",
    "# and any minimal augmentations suitable for MAE.\n",
    "print(f\"Setting up MAETransform with input size {INPUT_SIZE}...\")\n",
    "#take random crops from 0.2% to 100% of the image, resize it to INPUT_SIZE, horizontal flip with 50% probability, \n",
    "# and normalize with ImageNet stats if not put to None\n",
    "\n",
    "##### WITH CUSTOM NORMALIZATION ####\n",
    "if CUSTOM_NORMALIZATION:\n",
    "    \n",
    "# If you want to use custom normalization stats, compute them from a sample of images.\n",
    "    print(\"Computing custom normalization stats from dataset sample...\")\n",
    "    # Note: This function should be defined in your utils or transformations module.\n",
    "    # It computes mean and std from a sample of images in the dataset.\n",
    "    # Adjust the sample size as needed (e.g., 1000 images).\n",
    "    from utils.setup_functions import get_tif_image_paths_from_folder\n",
    "    images_paths = get_tif_image_paths_from_folder(DATA_DIR) # Get all image paths from the dataset directory\n",
    "    custom_normalization_stats = compute_dataset_mean_std(images_paths, cfg) # Compute mean and std from a sample of 1000 images\n",
    "\n",
    "    # Instantiate the custom transform\n",
    "    transform = MAECustomTransform(\n",
    "        input_size=INPUT_SIZE,\n",
    "        normalization_stats_dict=custom_normalization_stats, # Use the computed mean and std\n",
    "    )\n",
    "    # Wrap it in a lambda to return a single-element tuple\n",
    "else:\n",
    "    # If not using custom normalization, use the default ImageNet stats.\n",
    "    print(\"Using default ImageNet normalization stats for MAETransform.\")\n",
    "    # Instantiate the MAETransform\n",
    "    mae_transform_instance = MAETransform(input_size=INPUT_SIZE)\n",
    "    # Wrap it in a lambda to return a single-element tuple\n",
    "    transform = lambda img: (mae_transform_instance(img),)\n",
    "    \n",
    "    \n",
    "# 3) Create the LightlyDataset\n",
    "# This dataset automatically finds images in the specified input directory.\n",
    "print(f\"Loading dataset from: {DATA_DIR}\")\n",
    "dataset = LightlyDataset(input_dir=DATA_DIR, transform=transform)\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "\n",
    "# 4) Create the PyTorch DataLoader\n",
    "# Handles batching, shuffling, and parallel data loading.\n",
    "print(f\"Creating DataLoader with batch size {BATCH_SIZE}...\")\n",
    "dataloader = DataLoader(\n",
    "    dataset,  # type: ignore\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,      # Shuffle data each epoch for better training\n",
    "    drop_last=True,    # Drop the last incomplete batch\n",
    "    num_workers=NUM_WORKERS,\n",
    "    persistent_workers=True if NUM_WORKERS > 0 else False # Can speed up epoch start\n",
    ")\n",
    "\n",
    "# 5) Configure the PyTorch Lightning Trainer\n",
    "# Manages the training loop, GPU/CPU placement, callbacks, logging, etc.\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "print(f\"Configuring PyTorch Lightning Trainer for {MAX_EPOCHS} epochs...\")\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\", # Use GPU if available\n",
    "    devices=1,                                         # Number of devices to use (e.g., 1 GPU)\n",
    "    enable_progress_bar=False,                         # Disable default progress bar as we use a custom callback\n",
    "    callbacks=[\n",
    "        PrintEpochLossCallback(),\n",
    "        EarlyStopping(\n",
    "            monitor=\"train_loss\",  # Metric to monitor\n",
    "            patience=20,           # Number of epochs with no improvement after which training will be stopped\n",
    "            verbose=True,          # Log when stopping\n",
    "            mode=\"min\"             # In 'min' mode, training will stop when the quantity monitored has stopped decreasing\n",
    "            )],              \n",
    "    precision=\"16-mixed\"                           \n",
    ")\n",
    "\n",
    "# 6) Start the MAE pre-training process\n",
    "print(\"Starting MAE pre-training...\")\n",
    "trainer.fit(model=model, train_dataloaders=dataloader)\n",
    "print(\"MAE pre-training finished.\")\n",
    "# 7) Save the trained encoder backbone state dictionary\n",
    "# We only save the `backbone` (the ViT encoder), as the decoder is usually discarded.\n",
    "print(f\"Saving trained MAE encoder backbone to: {SAVE_PATH}\")\n",
    "torch.save(model.backbone.state_dict(), SAVE_PATH)\n",
    "print(\"Backbone saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f025155a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  8)load the pre-trained encoder for a downstream task (e.g., classification)\n",
    "print(\"\\nExample: Loading the saved backbone for downstream use:\")\n",
    "# First, create the *same architecture* as the saved backbone\n",
    "vit_for_downstream = vit_base_patch32_224(pretrained=False) # Important: use same ViT config\n",
    "mae_encoder = MaskedVisionTransformerTIMM(vit=vit_for_downstream)\n",
    "print(\" - Created encoder architecture.\")\n",
    "# Second, load the saved state dictionary into the architecture instance\n",
    "state_dict = torch.load(SAVE_PATH)\n",
    "mae_encoder.load_state_dict(state_dict)\n",
    "mae_encoder.eval() # Set to evaluation mode\n",
    "print(f\" - Loaded state dict from '{SAVE_PATH}' into encoder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "61440b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 19:24:35,732] A new study created in memory with name: no-name-5fa64522-fb53-4a10-a7c6-8865a320db6a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining example downstream transforms for input size 224x224...\n",
      "\n",
      "Starting nested cross-validation...\n",
      "Detected 2 unique classes.\n",
      "\n",
      "===== OUTER FOLD 1 / 6 =====\n",
      "Outer Train images: 115 | Outer Test images: 25\n",
      "--- Calculating normalization stats for Fold 1 Training Data ---\n",
      "Fold 1 stats: {'mean': [0.02819509245455265, 0.010962597094476223, 0.0807294100522995], 'std': [0.053679682314395905, 0.017251143231987953, 0.08971326798200607]}\n",
      "--- Generating data transforms for Fold 1 ---\n",
      "Using the train and val transform passed as arguments\n",
      "--- Starting Hyperparameter Tuning for Fold 1 ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=4.715696678089837e-05\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=4.715696678089837e-05\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=4.715696678089837e-05\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=4.715696678089837e-05\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 19:24:40,729] Trial 0 finished with value: 0.7774235755205154 and parameters: {'lr': 4.715696678089837e-05}. Best is trial 0 with value: 0.7774235755205154.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0014886262201211794\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0014886262201211794\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0014886262201211794\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0014886262201211794\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 19:24:45,753] Trial 1 finished with value: 0.742471843957901 and parameters: {'lr': 0.0014886262201211794}. Best is trial 1 with value: 0.742471843957901.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0004014783718209777\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0004014783718209777\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0004014783718209777\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0004014783718209777\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 19:24:50,688] Trial 2 finished with value: 0.6598421037197113 and parameters: {'lr': 0.0004014783718209777}. Best is trial 2 with value: 0.6598421037197113.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Best LR from inner CV = 0.000401\n",
      "--- Starting Final Model Training for Fold 1 with LR=0.000401 ---\n",
      "X_train_es: (97,) | X_val_es: (18,)\n",
      "Early stopping split: Train images: 97, Validation images: 18\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0004014783718209777\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "===========================\n",
      "Model Architecture:\n",
      "==================\n",
      "Total parameters: 87,456,770\n",
      "Trainable parameters: 1,538\n",
      "Non-trainable parameters: 87,455,232\n",
      "===========================\n",
      "Using CosineAnnealingWarmRestarts scheduler\n",
      " Fold 1 Epoch 1/230: Tr L: 0.8031, Tr Acc: 0.3596, Val L: 0.6752, Val Acc: 0.6111, Val F1: 0.6957 lr: 0.000401\n",
      " Fold 1 Epoch 2/230: Tr L: 0.6666, Tr Acc: 0.6754, Val L: 0.6576, Val Acc: 0.6111, Val F1: 0.6957 lr: 0.000401\n",
      " Fold 1 Epoch 3/230: Tr L: 0.6377, Tr Acc: 0.6667, Val L: 0.6301, Val Acc: 0.6667, Val F1: 0.7273 lr: 0.000382\n",
      " Fold 1 Epoch 4/230: Tr L: 0.6317, Tr Acc: 0.6667, Val L: 0.6264, Val Acc: 0.6667, Val F1: 0.7273 lr: 0.000327\n",
      " Fold 1 Epoch 5/230: Tr L: 0.6296, Tr Acc: 0.6667, Val L: 0.6207, Val Acc: 0.6667, Val F1: 0.7273 lr: 0.000247\n",
      " Fold 1 Epoch 6/230: Tr L: 0.6182, Tr Acc: 0.6667, Val L: 0.6265, Val Acc: 0.6667, Val F1: 0.7273 lr: 0.000159\n",
      " Fold 1 Epoch 7/230: Tr L: 0.6267, Tr Acc: 0.6667, Val L: 0.6290, Val Acc: 0.6667, Val F1: 0.7273 lr: 0.000080\n",
      " Fold 1 Epoch 8/230: Tr L: 0.6253, Tr Acc: 0.6667, Val L: 0.6295, Val Acc: 0.6667, Val F1: 0.7273 lr: 0.000025\n",
      " Fold 1 Epoch 9/230: Tr L: 0.6189, Tr Acc: 0.6667, Val L: 0.6316, Val Acc: 0.6111, Val F1: 0.6957 lr: 0.000401\n",
      " Fold 1 Epoch 10/230: Tr L: 0.6264, Tr Acc: 0.6667, Val L: 0.6349, Val Acc: 0.6111, Val F1: 0.6957 lr: 0.000397\n",
      " Fold 1 Epoch 11/230: Tr L: 0.6162, Tr Acc: 0.6667, Val L: 0.6252, Val Acc: 0.6667, Val F1: 0.7273 lr: 0.000382\n",
      " Fold 1 Epoch 12/230: Tr L: 0.6317, Tr Acc: 0.6754, Val L: 0.6257, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000358\n",
      " Fold 1 Epoch 13/230: Tr L: 0.6234, Tr Acc: 0.6491, Val L: 0.6302, Val Acc: 0.6111, Val F1: 0.6957 lr: 0.000327\n",
      " Fold 1 Epoch 14/230: Tr L: 0.6385, Tr Acc: 0.6667, Val L: 0.6662, Val Acc: 0.6111, Val F1: 0.6957 lr: 0.000289\n",
      " Fold 1 Epoch 15/230: Tr L: 0.6080, Tr Acc: 0.6667, Val L: 0.6677, Val Acc: 0.6111, Val F1: 0.6957 lr: 0.000247\n",
      " Fold 1 Epoch 16/230: Tr L: 0.6140, Tr Acc: 0.6667, Val L: 0.6464, Val Acc: 0.6111, Val F1: 0.6957 lr: 0.000203\n",
      " Fold 1 Epoch 17/230: Tr L: 0.6236, Tr Acc: 0.6667, Val L: 0.6317, Val Acc: 0.6111, Val F1: 0.6957 lr: 0.000159\n",
      " Fold 1 Epoch 18/230: Tr L: 0.6003, Tr Acc: 0.6667, Val L: 0.6265, Val Acc: 0.6111, Val F1: 0.6957 lr: 0.000117\n",
      " Fold 1 Epoch 19/230: Tr L: 0.5999, Tr Acc: 0.6667, Val L: 0.6264, Val Acc: 0.6111, Val F1: 0.6957 lr: 0.000080\n",
      " Fold 1 Epoch 20/230: Tr L: 0.6023, Tr Acc: 0.6667, Val L: 0.6272, Val Acc: 0.6111, Val F1: 0.6957 lr: 0.000048\n",
      " Fold 1 Epoch 21/230: Tr L: 0.6026, Tr Acc: 0.6667, Val L: 0.6279, Val Acc: 0.6111, Val F1: 0.6957 lr: 0.000025\n",
      " Fold 1 Epoch 22/230: Tr L: 0.6063, Tr Acc: 0.6667, Val L: 0.6287, Val Acc: 0.6111, Val F1: 0.6957 lr: 0.000010\n",
      " Fold 1 Epoch 23/230: Tr L: 0.6157, Tr Acc: 0.6667, Val L: 0.6479, Val Acc: 0.6111, Val F1: 0.6957 lr: 0.000401\n",
      " Fold 1 Epoch 24/230: Tr L: 0.5987, Tr Acc: 0.6667, Val L: 0.6370, Val Acc: 0.6111, Val F1: 0.6957 lr: 0.000400\n",
      " Fold 1 Epoch 25/230: Tr L: 0.5994, Tr Acc: 0.6667, Val L: 0.6411, Val Acc: 0.6111, Val F1: 0.6957 lr: 0.000397\n",
      " Fold 1 Epoch 26/230: Tr L: 0.5907, Tr Acc: 0.6667, Val L: 0.6481, Val Acc: 0.6111, Val F1: 0.6957 lr: 0.000390\n",
      " Fold 1 Epoch 27/230: Tr L: 0.6241, Tr Acc: 0.6667, Val L: 0.6566, Val Acc: 0.6111, Val F1: 0.6957 lr: 0.000382\n",
      " Fold 1 Epoch 28/230: Tr L: 0.6292, Tr Acc: 0.6667, Val L: 0.6399, Val Acc: 0.6111, Val F1: 0.6957 lr: 0.000371\n",
      " Fold 1 Epoch 29/230: Tr L: 0.6055, Tr Acc: 0.6667, Val L: 0.6180, Val Acc: 0.6667, Val F1: 0.7273 lr: 0.000358\n",
      " Fold 1 Epoch 30/230: Tr L: 0.6067, Tr Acc: 0.6667, Val L: 0.6243, Val Acc: 0.6111, Val F1: 0.6667 lr: 0.000343\n",
      " Fold 1 Epoch 31/230: Tr L: 0.6099, Tr Acc: 0.6404, Val L: 0.6306, Val Acc: 0.6111, Val F1: 0.6667 lr: 0.000327\n",
      " Fold 1 Epoch 32/230: Tr L: 0.5997, Tr Acc: 0.6491, Val L: 0.6325, Val Acc: 0.6111, Val F1: 0.6957 lr: 0.000309\n",
      " Fold 1 Epoch 33/230: Tr L: 0.5963, Tr Acc: 0.6667, Val L: 0.6405, Val Acc: 0.6111, Val F1: 0.6957 lr: 0.000289\n",
      " Fold 1 Epoch 34/230: Tr L: 0.5937, Tr Acc: 0.6667, Val L: 0.6644, Val Acc: 0.6111, Val F1: 0.6957 lr: 0.000269\n",
      " Fold 1 Epoch 35/230: Tr L: 0.6198, Tr Acc: 0.6667, Val L: 0.6656, Val Acc: 0.6111, Val F1: 0.6957 lr: 0.000247\n",
      " Fold 1 Epoch 36/230: Tr L: 0.6023, Tr Acc: 0.6667, Val L: 0.6465, Val Acc: 0.6111, Val F1: 0.6957 lr: 0.000225\n",
      " Fold 1 Epoch 37/230: Tr L: 0.5901, Tr Acc: 0.6667, Val L: 0.6317, Val Acc: 0.6111, Val F1: 0.6957 lr: 0.000203\n",
      " Fold 1 Epoch 38/230: Tr L: 0.5866, Tr Acc: 0.6667, Val L: 0.6311, Val Acc: 0.6111, Val F1: 0.6957 lr: 0.000181\n",
      " Fold 1 Epoch 39/230: Tr L: 0.5923, Tr Acc: 0.6667, Val L: 0.6366, Val Acc: 0.6111, Val F1: 0.6957 lr: 0.000159\n",
      " Fold 1 Epoch 40/230: Tr L: 0.5900, Tr Acc: 0.6667, Val L: 0.6418, Val Acc: 0.6111, Val F1: 0.6957 lr: 0.000138\n",
      " Fold 1 Epoch 41/230: Tr L: 0.5879, Tr Acc: 0.6667, Val L: 0.6454, Val Acc: 0.6111, Val F1: 0.6957 lr: 0.000117\n",
      " Fold 1 Epoch 42/230: Tr L: 0.5850, Tr Acc: 0.6667, Val L: 0.6477, Val Acc: 0.6111, Val F1: 0.6957 lr: 0.000098\n",
      " Fold 1 Epoch 43/230: Tr L: 0.5958, Tr Acc: 0.6667, Val L: 0.6497, Val Acc: 0.6111, Val F1: 0.6957 lr: 0.000080\n",
      " Fold 1 Epoch 44/230: Tr L: 0.5946, Tr Acc: 0.6667, Val L: 0.6481, Val Acc: 0.6111, Val F1: 0.6957 lr: 0.000063\n",
      " Fold 1 Epoch 45/230: Tr L: 0.5887, Tr Acc: 0.6667, Val L: 0.6448, Val Acc: 0.6111, Val F1: 0.6957 lr: 0.000048\n",
      " Fold 1 Epoch 46/230: Tr L: 0.5886, Tr Acc: 0.6667, Val L: 0.6435, Val Acc: 0.6111, Val F1: 0.6957 lr: 0.000035\n",
      " Fold 1 Epoch 47/230: Tr L: 0.5975, Tr Acc: 0.6667, Val L: 0.6427, Val Acc: 0.6111, Val F1: 0.6957 lr: 0.000025\n",
      " Fold 1 Epoch 48/230: Tr L: 0.5918, Tr Acc: 0.6667, Val L: 0.6425, Val Acc: 0.6111, Val F1: 0.6957 lr: 0.000016\n",
      " Fold 1 Epoch 49/230: Tr L: 0.6005, Tr Acc: 0.6667, Val L: 0.6417, Val Acc: 0.6111, Val F1: 0.6957 lr: 0.000010\n",
      " Fold 1 Epoch 50/230: Tr L: 0.6014, Tr Acc: 0.6667, Val L: 0.6417, Val Acc: 0.6111, Val F1: 0.6957 lr: 0.000006\n",
      " Fold 1 Epoch 51/230: Tr L: 0.5960, Tr Acc: 0.6667, Val L: 0.6391, Val Acc: 0.6111, Val F1: 0.6957 lr: 0.000401\n",
      " Fold 1 Epoch 52/230: Tr L: 0.5885, Tr Acc: 0.6754, Val L: 0.6430, Val Acc: 0.5556, Val F1: 0.6667 lr: 0.000401\n",
      " Fold 1 Epoch 53/230: Tr L: 0.5967, Tr Acc: 0.6842, Val L: 0.6397, Val Acc: 0.5556, Val F1: 0.6667 lr: 0.000400\n",
      " Fold 1 Epoch 54/230: Tr L: 0.5967, Tr Acc: 0.6667, Val L: 0.6395, Val Acc: 0.6111, Val F1: 0.6957 lr: 0.000399\n",
      " Fold 1 Epoch 55/230: Tr L: 0.5927, Tr Acc: 0.6667, Val L: 0.6352, Val Acc: 0.6111, Val F1: 0.6957 lr: 0.000397\n",
      " Fold 1 Epoch 56/230: Tr L: 0.5985, Tr Acc: 0.6667, Val L: 0.6356, Val Acc: 0.6111, Val F1: 0.6957 lr: 0.000394\n",
      " Fold 1 Epoch 57/230: Tr L: 0.6034, Tr Acc: 0.6667, Val L: 0.6295, Val Acc: 0.6111, Val F1: 0.6957 lr: 0.000390\n",
      " Fold 1 Epoch 58/230: Tr L: 0.5932, Tr Acc: 0.6667, Val L: 0.6249, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000386\n",
      " Fold 1 Epoch 59/230: Tr L: 0.5929, Tr Acc: 0.6667, Val L: 0.6403, Val Acc: 0.5556, Val F1: 0.6667 lr: 0.000382\n",
      " Fold 1 Epoch 60/230: Tr L: 0.5810, Tr Acc: 0.6667, Val L: 0.6444, Val Acc: 0.6111, Val F1: 0.6957 lr: 0.000377\n",
      " Fold 1 Epoch 61/230: Tr L: 0.6098, Tr Acc: 0.6667, Val L: 0.6548, Val Acc: 0.6111, Val F1: 0.6957 lr: 0.000371\n",
      " Fold 1 Epoch 62/230: Tr L: 0.5940, Tr Acc: 0.6667, Val L: 0.6364, Val Acc: 0.6111, Val F1: 0.6957 lr: 0.000365\n",
      " Fold 1 Epoch 63/230: Tr L: 0.5895, Tr Acc: 0.6667, Val L: 0.6349, Val Acc: 0.6111, Val F1: 0.6957 lr: 0.000358\n",
      " Fold 1 Epoch 64/230: Tr L: 0.5823, Tr Acc: 0.6667, Val L: 0.6312, Val Acc: 0.5556, Val F1: 0.6667 lr: 0.000351\n",
      " Fold 1 Epoch 65/230: Tr L: 0.5938, Tr Acc: 0.6667, Val L: 0.6344, Val Acc: 0.5556, Val F1: 0.6667 lr: 0.000343\n",
      " Fold 1 Epoch 66/230: Tr L: 0.5639, Tr Acc: 0.6930, Val L: 0.6372, Val Acc: 0.5556, Val F1: 0.6667 lr: 0.000335\n",
      " Fold 1 Epoch 67/230: Tr L: 0.5772, Tr Acc: 0.6930, Val L: 0.6461, Val Acc: 0.5556, Val F1: 0.6667 lr: 0.000327\n",
      " Fold 1 Epoch 68/230: Tr L: 0.5828, Tr Acc: 0.7018, Val L: 0.6861, Val Acc: 0.5556, Val F1: 0.6667 lr: 0.000318\n",
      " Fold 1 Epoch 69/230: Tr L: 0.6000, Tr Acc: 0.6842, Val L: 0.6900, Val Acc: 0.6111, Val F1: 0.6957 lr: 0.000309\n",
      "Early stopping triggered at epoch 69 for fold 1\n",
      "--- Evaluating Fold 1 on Outer Test Set ---\n",
      "Warning: model_factory used without specific LR, using default/cfg LR: 0.001\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.001\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "Test set class counts for fold 1: {0: 16, 1: 9}\n",
      "percentage of classes in test set: 0    0.64\n",
      "1    0.36\n",
      "Name: count, dtype: float64\n",
      " [FOLD 1 FINAL] Test Loss: 0.7504 | Test Acc: 0.4400 | test Balanced Acc: 0.4653 | test F1: 0.4167 | Test AUC: 1.0000\n",
      "model class name: ViTFinetuneModule\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 19:25:11,369] A new study created in memory with name: no-name-b486bb18-aa67-4ab9-bab6-75410a05f556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== OUTER FOLD 2 / 6 =====\n",
      "Outer Train images: 114 | Outer Test images: 26\n",
      "--- Calculating normalization stats for Fold 2 Training Data ---\n",
      "Fold 2 stats: {'mean': [0.028932757675647736, 0.010694378055632114, 0.07824025303125381], 'std': [0.05646703764796257, 0.01608475297689438, 0.08640255033969879]}\n",
      "--- Generating data transforms for Fold 2 ---\n",
      "Using the train and val transform passed as arguments\n",
      "--- Starting Hyperparameter Tuning for Fold 2 ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=4.715696678089837e-05\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=4.715696678089837e-05\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=4.715696678089837e-05\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=4.715696678089837e-05\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 19:25:16,378] Trial 0 finished with value: 0.7388374656438828 and parameters: {'lr': 4.715696678089837e-05}. Best is trial 0 with value: 0.7388374656438828.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0014886262201211794\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0014886262201211794\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0014886262201211794\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0014886262201211794\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 19:25:21,380] Trial 1 finished with value: 0.7493647634983063 and parameters: {'lr': 0.0014886262201211794}. Best is trial 0 with value: 0.7388374656438828.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0004014783718209777\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0004014783718209777\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0004014783718209777\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0004014783718209777\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 19:25:26,403] Trial 2 finished with value: 0.6988273561000824 and parameters: {'lr': 0.0004014783718209777}. Best is trial 2 with value: 0.6988273561000824.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Best LR from inner CV = 0.000401\n",
      "--- Starting Final Model Training for Fold 2 with LR=0.000401 ---\n",
      "X_train_es: (96,) | X_val_es: (18,)\n",
      "Early stopping split: Train images: 96, Validation images: 18\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0004014783718209777\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "===========================\n",
      "Model Architecture:\n",
      "==================\n",
      "Total parameters: 87,456,770\n",
      "Trainable parameters: 1,538\n",
      "Non-trainable parameters: 87,455,232\n",
      "===========================\n",
      "Using CosineAnnealingWarmRestarts scheduler\n",
      " Fold 2 Epoch 1/230: Tr L: 0.7555, Tr Acc: 0.4569, Val L: 0.7747, Val Acc: 0.3333, Val F1: 0.5000 lr: 0.000401\n",
      " Fold 2 Epoch 2/230: Tr L: 0.6978, Tr Acc: 0.5948, Val L: 0.6924, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000401\n",
      " Fold 2 Epoch 3/230: Tr L: 0.6730, Tr Acc: 0.5776, Val L: 0.6454, Val Acc: 0.6111, Val F1: 0.5882 lr: 0.000382\n",
      " Fold 2 Epoch 4/230: Tr L: 0.6876, Tr Acc: 0.5776, Val L: 0.6561, Val Acc: 0.5556, Val F1: 0.5556 lr: 0.000327\n",
      " Fold 2 Epoch 5/230: Tr L: 0.6753, Tr Acc: 0.5776, Val L: 0.6728, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000247\n",
      " Fold 2 Epoch 6/230: Tr L: 0.6727, Tr Acc: 0.5690, Val L: 0.6766, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000159\n",
      " Fold 2 Epoch 7/230: Tr L: 0.6741, Tr Acc: 0.5690, Val L: 0.6688, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000080\n",
      " Fold 2 Epoch 8/230: Tr L: 0.6654, Tr Acc: 0.5690, Val L: 0.6667, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000025\n",
      " Fold 2 Epoch 9/230: Tr L: 0.6643, Tr Acc: 0.5690, Val L: 0.6573, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000401\n",
      " Fold 2 Epoch 10/230: Tr L: 0.6695, Tr Acc: 0.5690, Val L: 0.6578, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000397\n",
      " Fold 2 Epoch 11/230: Tr L: 0.6588, Tr Acc: 0.5690, Val L: 0.6569, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000382\n",
      " Fold 2 Epoch 12/230: Tr L: 0.6568, Tr Acc: 0.5690, Val L: 0.6649, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000358\n",
      " Fold 2 Epoch 13/230: Tr L: 0.6594, Tr Acc: 0.5690, Val L: 0.6670, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000327\n",
      " Fold 2 Epoch 14/230: Tr L: 0.6563, Tr Acc: 0.5690, Val L: 0.6571, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000289\n",
      " Fold 2 Epoch 15/230: Tr L: 0.6511, Tr Acc: 0.5690, Val L: 0.6533, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000247\n",
      " Fold 2 Epoch 16/230: Tr L: 0.6534, Tr Acc: 0.5690, Val L: 0.6527, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000203\n",
      " Fold 2 Epoch 17/230: Tr L: 0.6633, Tr Acc: 0.5776, Val L: 0.6557, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000159\n",
      " Fold 2 Epoch 18/230: Tr L: 0.6543, Tr Acc: 0.5948, Val L: 0.6569, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000117\n",
      " Fold 2 Epoch 19/230: Tr L: 0.6513, Tr Acc: 0.6379, Val L: 0.6605, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000080\n",
      " Fold 2 Epoch 20/230: Tr L: 0.6556, Tr Acc: 0.6121, Val L: 0.6620, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000048\n",
      " Fold 2 Epoch 21/230: Tr L: 0.6532, Tr Acc: 0.6034, Val L: 0.6634, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000025\n",
      " Fold 2 Epoch 22/230: Tr L: 0.6476, Tr Acc: 0.6034, Val L: 0.6640, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000010\n",
      " Fold 2 Epoch 23/230: Tr L: 0.6478, Tr Acc: 0.6034, Val L: 0.6756, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000401\n",
      " Fold 2 Epoch 24/230: Tr L: 0.6528, Tr Acc: 0.6034, Val L: 0.6753, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000400\n",
      " Fold 2 Epoch 25/230: Tr L: 0.6577, Tr Acc: 0.6207, Val L: 0.6567, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000397\n",
      " Fold 2 Epoch 26/230: Tr L: 0.6550, Tr Acc: 0.6121, Val L: 0.6629, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000390\n",
      " Fold 2 Epoch 27/230: Tr L: 0.6395, Tr Acc: 0.6034, Val L: 0.6768, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000382\n",
      " Fold 2 Epoch 28/230: Tr L: 0.6471, Tr Acc: 0.6034, Val L: 0.6795, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000371\n",
      " Fold 2 Epoch 29/230: Tr L: 0.6486, Tr Acc: 0.6034, Val L: 0.6671, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000358\n",
      " Fold 2 Epoch 30/230: Tr L: 0.6492, Tr Acc: 0.6034, Val L: 0.6550, Val Acc: 0.5556, Val F1: 0.5556 lr: 0.000343\n",
      " Fold 2 Epoch 31/230: Tr L: 0.6444, Tr Acc: 0.5862, Val L: 0.6583, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000327\n",
      " Fold 2 Epoch 32/230: Tr L: 0.6407, Tr Acc: 0.6034, Val L: 0.6662, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000309\n",
      " Fold 2 Epoch 33/230: Tr L: 0.6411, Tr Acc: 0.6293, Val L: 0.6622, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000289\n",
      " Fold 2 Epoch 34/230: Tr L: 0.6518, Tr Acc: 0.6034, Val L: 0.6645, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000269\n",
      " Fold 2 Epoch 35/230: Tr L: 0.6449, Tr Acc: 0.6034, Val L: 0.6662, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000247\n",
      " Fold 2 Epoch 36/230: Tr L: 0.6389, Tr Acc: 0.6034, Val L: 0.6555, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000225\n",
      " Fold 2 Epoch 37/230: Tr L: 0.6394, Tr Acc: 0.6121, Val L: 0.6526, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000203\n",
      " Fold 2 Epoch 38/230: Tr L: 0.6390, Tr Acc: 0.6034, Val L: 0.6474, Val Acc: 0.5556, Val F1: 0.5556 lr: 0.000181\n",
      " Fold 2 Epoch 39/230: Tr L: 0.6478, Tr Acc: 0.5690, Val L: 0.6429, Val Acc: 0.5000, Val F1: 0.4000 lr: 0.000159\n",
      " Fold 2 Epoch 40/230: Tr L: 0.6378, Tr Acc: 0.6379, Val L: 0.6455, Val Acc: 0.5556, Val F1: 0.5556 lr: 0.000138\n",
      " Fold 2 Epoch 41/230: Tr L: 0.6394, Tr Acc: 0.5948, Val L: 0.6530, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000117\n",
      " Fold 2 Epoch 42/230: Tr L: 0.6293, Tr Acc: 0.6293, Val L: 0.6568, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000098\n",
      " Fold 2 Epoch 43/230: Tr L: 0.6421, Tr Acc: 0.6121, Val L: 0.6569, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000080\n",
      " Fold 2 Epoch 44/230: Tr L: 0.6362, Tr Acc: 0.6207, Val L: 0.6545, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000063\n",
      " Fold 2 Epoch 45/230: Tr L: 0.6442, Tr Acc: 0.6207, Val L: 0.6543, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000048\n",
      " Fold 2 Epoch 46/230: Tr L: 0.6322, Tr Acc: 0.6207, Val L: 0.6537, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000035\n",
      " Fold 2 Epoch 47/230: Tr L: 0.6374, Tr Acc: 0.6379, Val L: 0.6532, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000025\n",
      " Fold 2 Epoch 48/230: Tr L: 0.6257, Tr Acc: 0.6379, Val L: 0.6532, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000016\n",
      " Fold 2 Epoch 49/230: Tr L: 0.6295, Tr Acc: 0.6379, Val L: 0.6534, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000010\n",
      " Fold 2 Epoch 50/230: Tr L: 0.6346, Tr Acc: 0.6379, Val L: 0.6536, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000006\n",
      " Fold 2 Epoch 51/230: Tr L: 0.6305, Tr Acc: 0.6034, Val L: 0.6594, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000401\n",
      " Fold 2 Epoch 52/230: Tr L: 0.6354, Tr Acc: 0.6207, Val L: 0.6704, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000401\n",
      " Fold 2 Epoch 53/230: Tr L: 0.6323, Tr Acc: 0.6293, Val L: 0.6597, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000400\n",
      " Fold 2 Epoch 54/230: Tr L: 0.6340, Tr Acc: 0.5948, Val L: 0.6573, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000399\n",
      " Fold 2 Epoch 55/230: Tr L: 0.6300, Tr Acc: 0.6034, Val L: 0.6629, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000397\n",
      " Fold 2 Epoch 56/230: Tr L: 0.6318, Tr Acc: 0.6207, Val L: 0.6531, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000394\n",
      " Fold 2 Epoch 57/230: Tr L: 0.6345, Tr Acc: 0.6379, Val L: 0.6524, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000390\n",
      " Fold 2 Epoch 58/230: Tr L: 0.6295, Tr Acc: 0.6121, Val L: 0.6444, Val Acc: 0.5556, Val F1: 0.5000 lr: 0.000386\n",
      " Fold 2 Epoch 59/230: Tr L: 0.6324, Tr Acc: 0.6207, Val L: 0.6430, Val Acc: 0.6667, Val F1: 0.5000 lr: 0.000382\n",
      " Fold 2 Epoch 60/230: Tr L: 0.6312, Tr Acc: 0.6121, Val L: 0.6477, Val Acc: 0.5556, Val F1: 0.5000 lr: 0.000377\n",
      " Fold 2 Epoch 61/230: Tr L: 0.6282, Tr Acc: 0.5948, Val L: 0.6558, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000371\n",
      " Fold 2 Epoch 62/230: Tr L: 0.6294, Tr Acc: 0.6207, Val L: 0.6755, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000365\n",
      " Fold 2 Epoch 63/230: Tr L: 0.6403, Tr Acc: 0.6293, Val L: 0.6724, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000358\n",
      " Fold 2 Epoch 64/230: Tr L: 0.6287, Tr Acc: 0.6034, Val L: 0.6589, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000351\n",
      " Fold 2 Epoch 65/230: Tr L: 0.6339, Tr Acc: 0.6293, Val L: 0.6541, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000343\n",
      " Fold 2 Epoch 66/230: Tr L: 0.6279, Tr Acc: 0.6207, Val L: 0.6619, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000335\n",
      " Fold 2 Epoch 67/230: Tr L: 0.6267, Tr Acc: 0.6379, Val L: 0.6593, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000327\n",
      " Fold 2 Epoch 68/230: Tr L: 0.6291, Tr Acc: 0.6293, Val L: 0.6504, Val Acc: 0.5556, Val F1: 0.5556 lr: 0.000318\n",
      " Fold 2 Epoch 69/230: Tr L: 0.6187, Tr Acc: 0.6293, Val L: 0.6429, Val Acc: 0.5556, Val F1: 0.5000 lr: 0.000309\n",
      " Fold 2 Epoch 70/230: Tr L: 0.6145, Tr Acc: 0.6121, Val L: 0.6443, Val Acc: 0.5556, Val F1: 0.5556 lr: 0.000299\n",
      " Fold 2 Epoch 71/230: Tr L: 0.6305, Tr Acc: 0.6034, Val L: 0.6424, Val Acc: 0.5556, Val F1: 0.5000 lr: 0.000289\n",
      " Fold 2 Epoch 72/230: Tr L: 0.6237, Tr Acc: 0.6293, Val L: 0.6479, Val Acc: 0.5556, Val F1: 0.5556 lr: 0.000279\n",
      " Fold 2 Epoch 73/230: Tr L: 0.6205, Tr Acc: 0.6293, Val L: 0.6605, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000269\n",
      " Fold 2 Epoch 74/230: Tr L: 0.6232, Tr Acc: 0.6293, Val L: 0.6679, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000258\n",
      " Fold 2 Epoch 75/230: Tr L: 0.6236, Tr Acc: 0.6293, Val L: 0.6604, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000247\n",
      " Fold 2 Epoch 76/230: Tr L: 0.6185, Tr Acc: 0.6379, Val L: 0.6560, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000236\n",
      " Fold 2 Epoch 77/230: Tr L: 0.6198, Tr Acc: 0.6207, Val L: 0.6492, Val Acc: 0.5556, Val F1: 0.5556 lr: 0.000225\n",
      " Fold 2 Epoch 78/230: Tr L: 0.6248, Tr Acc: 0.6293, Val L: 0.6453, Val Acc: 0.5556, Val F1: 0.5000 lr: 0.000214\n",
      " Fold 2 Epoch 79/230: Tr L: 0.6201, Tr Acc: 0.6293, Val L: 0.6466, Val Acc: 0.5556, Val F1: 0.5556 lr: 0.000203\n",
      " Fold 2 Epoch 80/230: Tr L: 0.6190, Tr Acc: 0.6207, Val L: 0.6467, Val Acc: 0.5556, Val F1: 0.5556 lr: 0.000192\n",
      " Fold 2 Epoch 81/230: Tr L: 0.6208, Tr Acc: 0.6034, Val L: 0.6475, Val Acc: 0.5556, Val F1: 0.5556 lr: 0.000181\n",
      " Fold 2 Epoch 82/230: Tr L: 0.6132, Tr Acc: 0.6121, Val L: 0.6478, Val Acc: 0.5556, Val F1: 0.5556 lr: 0.000170\n",
      " Fold 2 Epoch 83/230: Tr L: 0.6176, Tr Acc: 0.6207, Val L: 0.6481, Val Acc: 0.5556, Val F1: 0.5556 lr: 0.000159\n",
      " Fold 2 Epoch 84/230: Tr L: 0.6213, Tr Acc: 0.6034, Val L: 0.6477, Val Acc: 0.5556, Val F1: 0.5556 lr: 0.000148\n",
      " Fold 2 Epoch 85/230: Tr L: 0.6188, Tr Acc: 0.6207, Val L: 0.6538, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000138\n",
      " Fold 2 Epoch 86/230: Tr L: 0.6233, Tr Acc: 0.6207, Val L: 0.6552, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000127\n",
      " Fold 2 Epoch 87/230: Tr L: 0.6173, Tr Acc: 0.6207, Val L: 0.6539, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000117\n",
      " Fold 2 Epoch 88/230: Tr L: 0.6270, Tr Acc: 0.6293, Val L: 0.6524, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000107\n",
      " Fold 2 Epoch 89/230: Tr L: 0.6194, Tr Acc: 0.6293, Val L: 0.6547, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000098\n",
      " Fold 2 Epoch 90/230: Tr L: 0.6223, Tr Acc: 0.6207, Val L: 0.6519, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000089\n",
      " Fold 2 Epoch 91/230: Tr L: 0.6229, Tr Acc: 0.6121, Val L: 0.6470, Val Acc: 0.5556, Val F1: 0.5556 lr: 0.000080\n",
      " Fold 2 Epoch 92/230: Tr L: 0.6156, Tr Acc: 0.6121, Val L: 0.6476, Val Acc: 0.5556, Val F1: 0.5556 lr: 0.000071\n",
      " Fold 2 Epoch 93/230: Tr L: 0.6202, Tr Acc: 0.6207, Val L: 0.6501, Val Acc: 0.5556, Val F1: 0.5556 lr: 0.000063\n",
      " Fold 2 Epoch 94/230: Tr L: 0.6167, Tr Acc: 0.6293, Val L: 0.6522, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000055\n",
      " Fold 2 Epoch 95/230: Tr L: 0.6148, Tr Acc: 0.6293, Val L: 0.6532, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000048\n",
      " Fold 2 Epoch 96/230: Tr L: 0.6099, Tr Acc: 0.6207, Val L: 0.6540, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000042\n",
      " Fold 2 Epoch 97/230: Tr L: 0.6105, Tr Acc: 0.6207, Val L: 0.6547, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000035\n",
      " Fold 2 Epoch 98/230: Tr L: 0.6169, Tr Acc: 0.6207, Val L: 0.6565, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000030\n",
      " Fold 2 Epoch 99/230: Tr L: 0.6166, Tr Acc: 0.6207, Val L: 0.6568, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000025\n",
      " Fold 2 Epoch 100/230: Tr L: 0.6162, Tr Acc: 0.6207, Val L: 0.6562, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000020\n",
      " Fold 2 Epoch 101/230: Tr L: 0.6165, Tr Acc: 0.6207, Val L: 0.6556, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000016\n",
      " Fold 2 Epoch 102/230: Tr L: 0.6248, Tr Acc: 0.6207, Val L: 0.6549, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000013\n",
      " Fold 2 Epoch 103/230: Tr L: 0.6229, Tr Acc: 0.6121, Val L: 0.6543, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000010\n",
      " Fold 2 Epoch 104/230: Tr L: 0.6163, Tr Acc: 0.6207, Val L: 0.6541, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000008\n",
      " Fold 2 Epoch 105/230: Tr L: 0.6145, Tr Acc: 0.6207, Val L: 0.6538, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000006\n",
      " Fold 2 Epoch 106/230: Tr L: 0.6091, Tr Acc: 0.6207, Val L: 0.6535, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000005\n",
      " Fold 2 Epoch 107/230: Tr L: 0.6369, Tr Acc: 0.6207, Val L: 0.6381, Val Acc: 0.7222, Val F1: 0.5455 lr: 0.000401\n",
      " Fold 2 Epoch 108/230: Tr L: 0.6228, Tr Acc: 0.6207, Val L: 0.6501, Val Acc: 0.5556, Val F1: 0.5556 lr: 0.000401\n",
      " Fold 2 Epoch 109/230: Tr L: 0.6149, Tr Acc: 0.5776, Val L: 0.6514, Val Acc: 0.5556, Val F1: 0.5556 lr: 0.000401\n",
      " Fold 2 Epoch 110/230: Tr L: 0.6274, Tr Acc: 0.6207, Val L: 0.6717, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000401\n",
      " Fold 2 Epoch 111/230: Tr L: 0.6226, Tr Acc: 0.6379, Val L: 0.6561, Val Acc: 0.5556, Val F1: 0.5556 lr: 0.000400\n",
      " Fold 2 Epoch 112/230: Tr L: 0.6163, Tr Acc: 0.6034, Val L: 0.6496, Val Acc: 0.5556, Val F1: 0.5000 lr: 0.000400\n",
      " Fold 2 Epoch 113/230: Tr L: 0.6151, Tr Acc: 0.6293, Val L: 0.6615, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000399\n",
      " Fold 2 Epoch 114/230: Tr L: 0.6215, Tr Acc: 0.6207, Val L: 0.6749, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000398\n",
      " Fold 2 Epoch 115/230: Tr L: 0.6114, Tr Acc: 0.6293, Val L: 0.6541, Val Acc: 0.5556, Val F1: 0.5556 lr: 0.000397\n",
      " Fold 2 Epoch 116/230: Tr L: 0.6164, Tr Acc: 0.6207, Val L: 0.6573, Val Acc: 0.5556, Val F1: 0.5556 lr: 0.000395\n",
      " Fold 2 Epoch 117/230: Tr L: 0.6174, Tr Acc: 0.6207, Val L: 0.6729, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000394\n",
      " Fold 2 Epoch 118/230: Tr L: 0.6051, Tr Acc: 0.6466, Val L: 0.6736, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000392\n",
      " Fold 2 Epoch 119/230: Tr L: 0.6155, Tr Acc: 0.6466, Val L: 0.6632, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000390\n",
      " Fold 2 Epoch 120/230: Tr L: 0.6112, Tr Acc: 0.6207, Val L: 0.6546, Val Acc: 0.5556, Val F1: 0.5556 lr: 0.000388\n",
      " Fold 2 Epoch 121/230: Tr L: 0.6164, Tr Acc: 0.6293, Val L: 0.6521, Val Acc: 0.5556, Val F1: 0.5000 lr: 0.000386\n",
      " Fold 2 Epoch 122/230: Tr L: 0.6087, Tr Acc: 0.6466, Val L: 0.6547, Val Acc: 0.5556, Val F1: 0.5556 lr: 0.000384\n",
      " Fold 2 Epoch 123/230: Tr L: 0.6141, Tr Acc: 0.6034, Val L: 0.6561, Val Acc: 0.5556, Val F1: 0.5556 lr: 0.000382\n",
      " Fold 2 Epoch 124/230: Tr L: 0.6138, Tr Acc: 0.6121, Val L: 0.6455, Val Acc: 0.5556, Val F1: 0.5000 lr: 0.000379\n",
      " Fold 2 Epoch 125/230: Tr L: 0.6201, Tr Acc: 0.6207, Val L: 0.6527, Val Acc: 0.5556, Val F1: 0.5556 lr: 0.000377\n",
      " Fold 2 Epoch 126/230: Tr L: 0.6186, Tr Acc: 0.6121, Val L: 0.6599, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000374\n",
      " Fold 2 Epoch 127/230: Tr L: 0.6158, Tr Acc: 0.6121, Val L: 0.6552, Val Acc: 0.5556, Val F1: 0.5556 lr: 0.000371\n",
      " Fold 2 Epoch 128/230: Tr L: 0.6067, Tr Acc: 0.6293, Val L: 0.6512, Val Acc: 0.5556, Val F1: 0.5556 lr: 0.000368\n",
      " Fold 2 Epoch 129/230: Tr L: 0.6050, Tr Acc: 0.6121, Val L: 0.6539, Val Acc: 0.5556, Val F1: 0.5556 lr: 0.000365\n",
      " Fold 2 Epoch 130/230: Tr L: 0.6080, Tr Acc: 0.5948, Val L: 0.6494, Val Acc: 0.5556, Val F1: 0.5000 lr: 0.000362\n",
      " Fold 2 Epoch 131/230: Tr L: 0.6031, Tr Acc: 0.6034, Val L: 0.6521, Val Acc: 0.5000, Val F1: 0.4706 lr: 0.000358\n",
      " Fold 2 Epoch 132/230: Tr L: 0.6213, Tr Acc: 0.6207, Val L: 0.6482, Val Acc: 0.5556, Val F1: 0.5000 lr: 0.000355\n",
      " Fold 2 Epoch 133/230: Tr L: 0.6270, Tr Acc: 0.6293, Val L: 0.6385, Val Acc: 0.7222, Val F1: 0.5455 lr: 0.000351\n",
      " Fold 2 Epoch 134/230: Tr L: 0.6151, Tr Acc: 0.6466, Val L: 0.6393, Val Acc: 0.5556, Val F1: 0.4286 lr: 0.000347\n",
      " Fold 2 Epoch 135/230: Tr L: 0.6211, Tr Acc: 0.6466, Val L: 0.6388, Val Acc: 0.5556, Val F1: 0.4286 lr: 0.000343\n",
      " Fold 2 Epoch 136/230: Tr L: 0.6132, Tr Acc: 0.6121, Val L: 0.6419, Val Acc: 0.5556, Val F1: 0.5000 lr: 0.000339\n",
      " Fold 2 Epoch 137/230: Tr L: 0.6014, Tr Acc: 0.6379, Val L: 0.6672, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000335\n",
      " Fold 2 Epoch 138/230: Tr L: 0.6056, Tr Acc: 0.6379, Val L: 0.6829, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000331\n",
      " Fold 2 Epoch 139/230: Tr L: 0.6173, Tr Acc: 0.6379, Val L: 0.6632, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000327\n",
      " Fold 2 Epoch 140/230: Tr L: 0.6157, Tr Acc: 0.5948, Val L: 0.6349, Val Acc: 0.6667, Val F1: 0.5000 lr: 0.000322\n",
      " Fold 2 Epoch 141/230: Tr L: 0.6141, Tr Acc: 0.6293, Val L: 0.6377, Val Acc: 0.6111, Val F1: 0.5333 lr: 0.000318\n",
      " Fold 2 Epoch 142/230: Tr L: 0.6124, Tr Acc: 0.6121, Val L: 0.6465, Val Acc: 0.5556, Val F1: 0.5556 lr: 0.000313\n",
      " Fold 2 Epoch 143/230: Tr L: 0.6066, Tr Acc: 0.6293, Val L: 0.6453, Val Acc: 0.5000, Val F1: 0.4706 lr: 0.000309\n",
      " Fold 2 Epoch 144/230: Tr L: 0.6038, Tr Acc: 0.6207, Val L: 0.6473, Val Acc: 0.5556, Val F1: 0.5556 lr: 0.000304\n",
      " Fold 2 Epoch 145/230: Tr L: 0.6111, Tr Acc: 0.5948, Val L: 0.6574, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000299\n",
      " Fold 2 Epoch 146/230: Tr L: 0.6091, Tr Acc: 0.6207, Val L: 0.6568, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000294\n",
      " Fold 2 Epoch 147/230: Tr L: 0.6046, Tr Acc: 0.6034, Val L: 0.6461, Val Acc: 0.5556, Val F1: 0.5000 lr: 0.000289\n",
      " Fold 2 Epoch 148/230: Tr L: 0.6117, Tr Acc: 0.5948, Val L: 0.6363, Val Acc: 0.6667, Val F1: 0.5000 lr: 0.000284\n",
      " Fold 2 Epoch 149/230: Tr L: 0.6102, Tr Acc: 0.6121, Val L: 0.6423, Val Acc: 0.5556, Val F1: 0.5000 lr: 0.000279\n",
      " Fold 2 Epoch 150/230: Tr L: 0.6137, Tr Acc: 0.6121, Val L: 0.6579, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000274\n",
      " Fold 2 Epoch 151/230: Tr L: 0.6013, Tr Acc: 0.6293, Val L: 0.6514, Val Acc: 0.5556, Val F1: 0.5556 lr: 0.000269\n",
      " Fold 2 Epoch 152/230: Tr L: 0.6020, Tr Acc: 0.6121, Val L: 0.6489, Val Acc: 0.5556, Val F1: 0.5556 lr: 0.000263\n",
      " Fold 2 Epoch 153/230: Tr L: 0.6073, Tr Acc: 0.6379, Val L: 0.6634, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000258\n",
      " Fold 2 Epoch 154/230: Tr L: 0.6080, Tr Acc: 0.6379, Val L: 0.6730, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000253\n",
      " Fold 2 Epoch 155/230: Tr L: 0.6120, Tr Acc: 0.6379, Val L: 0.6737, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000247\n",
      " Fold 2 Epoch 156/230: Tr L: 0.6091, Tr Acc: 0.6293, Val L: 0.6597, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000242\n",
      " Fold 2 Epoch 157/230: Tr L: 0.6054, Tr Acc: 0.6121, Val L: 0.6488, Val Acc: 0.5556, Val F1: 0.5000 lr: 0.000236\n",
      " Fold 2 Epoch 158/230: Tr L: 0.6162, Tr Acc: 0.6293, Val L: 0.6384, Val Acc: 0.6667, Val F1: 0.5000 lr: 0.000231\n",
      " Fold 2 Epoch 159/230: Tr L: 0.6136, Tr Acc: 0.6638, Val L: 0.6379, Val Acc: 0.6667, Val F1: 0.5000 lr: 0.000225\n",
      " Fold 2 Epoch 160/230: Tr L: 0.6085, Tr Acc: 0.6207, Val L: 0.6464, Val Acc: 0.5556, Val F1: 0.5000 lr: 0.000220\n",
      " Fold 2 Epoch 161/230: Tr L: 0.6073, Tr Acc: 0.6293, Val L: 0.6555, Val Acc: 0.5556, Val F1: 0.5556 lr: 0.000214\n",
      " Fold 2 Epoch 162/230: Tr L: 0.6086, Tr Acc: 0.6207, Val L: 0.6599, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000209\n",
      " Fold 2 Epoch 163/230: Tr L: 0.6031, Tr Acc: 0.6293, Val L: 0.6580, Val Acc: 0.5556, Val F1: 0.5556 lr: 0.000203\n",
      " Fold 2 Epoch 164/230: Tr L: 0.5958, Tr Acc: 0.6207, Val L: 0.6500, Val Acc: 0.5000, Val F1: 0.4706 lr: 0.000198\n",
      " Fold 2 Epoch 165/230: Tr L: 0.6064, Tr Acc: 0.6293, Val L: 0.6431, Val Acc: 0.5556, Val F1: 0.5000 lr: 0.000192\n",
      " Fold 2 Epoch 166/230: Tr L: 0.6067, Tr Acc: 0.6466, Val L: 0.6437, Val Acc: 0.5556, Val F1: 0.5000 lr: 0.000187\n",
      " Fold 2 Epoch 167/230: Tr L: 0.6020, Tr Acc: 0.6293, Val L: 0.6463, Val Acc: 0.5556, Val F1: 0.5000 lr: 0.000181\n",
      " Fold 2 Epoch 168/230: Tr L: 0.6121, Tr Acc: 0.6293, Val L: 0.6609, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000176\n",
      " Fold 2 Epoch 169/230: Tr L: 0.6025, Tr Acc: 0.6207, Val L: 0.6722, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000170\n",
      " Fold 2 Epoch 170/230: Tr L: 0.6245, Tr Acc: 0.6207, Val L: 0.6750, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000165\n",
      " Fold 2 Epoch 171/230: Tr L: 0.6080, Tr Acc: 0.6293, Val L: 0.6595, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000159\n",
      " Fold 2 Epoch 172/230: Tr L: 0.6040, Tr Acc: 0.6379, Val L: 0.6532, Val Acc: 0.5556, Val F1: 0.5556 lr: 0.000154\n",
      " Fold 2 Epoch 173/230: Tr L: 0.5990, Tr Acc: 0.6121, Val L: 0.6476, Val Acc: 0.5556, Val F1: 0.5000 lr: 0.000148\n",
      " Fold 2 Epoch 174/230: Tr L: 0.6068, Tr Acc: 0.6293, Val L: 0.6448, Val Acc: 0.5556, Val F1: 0.5000 lr: 0.000143\n",
      " Fold 2 Epoch 175/230: Tr L: 0.6070, Tr Acc: 0.6379, Val L: 0.6444, Val Acc: 0.5556, Val F1: 0.5000 lr: 0.000138\n",
      " Fold 2 Epoch 176/230: Tr L: 0.6121, Tr Acc: 0.6379, Val L: 0.6510, Val Acc: 0.5556, Val F1: 0.5556 lr: 0.000133\n",
      " Fold 2 Epoch 177/230: Tr L: 0.6084, Tr Acc: 0.5948, Val L: 0.6595, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000127\n",
      " Fold 2 Epoch 178/230: Tr L: 0.6121, Tr Acc: 0.6293, Val L: 0.6628, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000122\n",
      " Fold 2 Epoch 179/230: Tr L: 0.6059, Tr Acc: 0.6293, Val L: 0.6580, Val Acc: 0.5556, Val F1: 0.5556 lr: 0.000117\n",
      " Fold 2 Epoch 180/230: Tr L: 0.6030, Tr Acc: 0.6207, Val L: 0.6602, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000112\n",
      "Early stopping triggered at epoch 180 for fold 2\n",
      "--- Evaluating Fold 2 on Outer Test Set ---\n",
      "Warning: model_factory used without specific LR, using default/cfg LR: 0.001\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.001\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "Test set class counts for fold 2: {0: 14, 1: 12}\n",
      "percentage of classes in test set: 0    0.538462\n",
      "1    0.461538\n",
      "Name: count, dtype: float64\n",
      " [FOLD 2 FINAL] Test Loss: 0.6267 | Test Acc: 0.3846 | test Balanced Acc: 0.3631 | test F1: 0.1111 | Test AUC: 1.0000\n",
      "model class name: ViTFinetuneModule\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 19:26:15,921] A new study created in memory with name: no-name-3fa7aaee-5830-4f29-9bbb-3eb29daf1cec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== OUTER FOLD 3 / 6 =====\n",
      "Outer Train images: 117 | Outer Test images: 23\n",
      "--- Calculating normalization stats for Fold 3 Training Data ---\n",
      "Fold 3 stats: {'mean': [0.02885417826473713, 0.011161606758832932, 0.07953041791915894], 'std': [0.054783567786216736, 0.017017517238855362, 0.08873949199914932]}\n",
      "--- Generating data transforms for Fold 3 ---\n",
      "Using the train and val transform passed as arguments\n",
      "--- Starting Hyperparameter Tuning for Fold 3 ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=4.715696678089837e-05\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=4.715696678089837e-05\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=4.715696678089837e-05\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=4.715696678089837e-05\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 19:26:20,953] Trial 0 finished with value: 0.7404812425374985 and parameters: {'lr': 4.715696678089837e-05}. Best is trial 0 with value: 0.7404812425374985.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0014886262201211794\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0014886262201211794\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0014886262201211794\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0014886262201211794\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 19:26:26,059] Trial 1 finished with value: 0.7417670488357544 and parameters: {'lr': 0.0014886262201211794}. Best is trial 0 with value: 0.7404812425374985.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0004014783718209777\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0004014783718209777\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0004014783718209777\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0004014783718209777\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 19:26:31,087] Trial 2 finished with value: 0.7149382382631302 and parameters: {'lr': 0.0004014783718209777}. Best is trial 2 with value: 0.7149382382631302.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Best LR from inner CV = 0.000401\n",
      "--- Starting Final Model Training for Fold 3 with LR=0.000401 ---\n",
      "X_train_es: (99,) | X_val_es: (18,)\n",
      "Early stopping split: Train images: 99, Validation images: 18\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0004014783718209777\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "===========================\n",
      "Model Architecture:\n",
      "==================\n",
      "Total parameters: 87,456,770\n",
      "Trainable parameters: 1,538\n",
      "Non-trainable parameters: 87,455,232\n",
      "===========================\n",
      "Using CosineAnnealingWarmRestarts scheduler\n",
      " Fold 3 Epoch 1/230: Tr L: 0.7748, Tr Acc: 0.4655, Val L: 0.7623, Val Acc: 0.5000, Val F1: 0.6087 lr: 0.000401\n",
      " Fold 3 Epoch 2/230: Tr L: 0.6993, Tr Acc: 0.5776, Val L: 0.7123, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000401\n",
      " Fold 3 Epoch 3/230: Tr L: 0.6731, Tr Acc: 0.6121, Val L: 0.6808, Val Acc: 0.4444, Val F1: 0.4444 lr: 0.000382\n",
      " Fold 3 Epoch 4/230: Tr L: 0.6889, Tr Acc: 0.5948, Val L: 0.6999, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000327\n",
      " Fold 3 Epoch 5/230: Tr L: 0.6736, Tr Acc: 0.5948, Val L: 0.7145, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000247\n",
      " Fold 3 Epoch 6/230: Tr L: 0.6778, Tr Acc: 0.5862, Val L: 0.7164, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000159\n",
      " Fold 3 Epoch 7/230: Tr L: 0.6784, Tr Acc: 0.5862, Val L: 0.7130, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000080\n",
      " Fold 3 Epoch 8/230: Tr L: 0.6623, Tr Acc: 0.5862, Val L: 0.7108, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000025\n",
      " Fold 3 Epoch 9/230: Tr L: 0.6644, Tr Acc: 0.5862, Val L: 0.6995, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000401\n",
      " Fold 3 Epoch 10/230: Tr L: 0.6698, Tr Acc: 0.6034, Val L: 0.6804, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000397\n",
      " Fold 3 Epoch 11/230: Tr L: 0.6672, Tr Acc: 0.6379, Val L: 0.6825, Val Acc: 0.4444, Val F1: 0.5000 lr: 0.000382\n",
      " Fold 3 Epoch 12/230: Tr L: 0.6570, Tr Acc: 0.6121, Val L: 0.6897, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000358\n",
      " Fold 3 Epoch 13/230: Tr L: 0.6618, Tr Acc: 0.5862, Val L: 0.6890, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000327\n",
      " Fold 3 Epoch 14/230: Tr L: 0.6570, Tr Acc: 0.5948, Val L: 0.6720, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000289\n",
      " Fold 3 Epoch 15/230: Tr L: 0.6603, Tr Acc: 0.6293, Val L: 0.6796, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000247\n",
      " Fold 3 Epoch 16/230: Tr L: 0.6656, Tr Acc: 0.5948, Val L: 0.6897, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000203\n",
      " Fold 3 Epoch 17/230: Tr L: 0.6639, Tr Acc: 0.5862, Val L: 0.6824, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000159\n",
      " Fold 3 Epoch 18/230: Tr L: 0.6545, Tr Acc: 0.6293, Val L: 0.6739, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000117\n",
      " Fold 3 Epoch 19/230: Tr L: 0.6580, Tr Acc: 0.6379, Val L: 0.6743, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000080\n",
      " Fold 3 Epoch 20/230: Tr L: 0.6590, Tr Acc: 0.6379, Val L: 0.6742, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000048\n",
      " Fold 3 Epoch 21/230: Tr L: 0.6575, Tr Acc: 0.6207, Val L: 0.6763, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000025\n",
      " Fold 3 Epoch 22/230: Tr L: 0.6619, Tr Acc: 0.6121, Val L: 0.6770, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000010\n",
      " Fold 3 Epoch 23/230: Tr L: 0.6534, Tr Acc: 0.6034, Val L: 0.6955, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000401\n",
      " Fold 3 Epoch 24/230: Tr L: 0.6635, Tr Acc: 0.5948, Val L: 0.7092, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000400\n",
      " Fold 3 Epoch 25/230: Tr L: 0.6650, Tr Acc: 0.6121, Val L: 0.6759, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000397\n",
      " Fold 3 Epoch 26/230: Tr L: 0.6584, Tr Acc: 0.5862, Val L: 0.6746, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000390\n",
      " Fold 3 Epoch 27/230: Tr L: 0.6534, Tr Acc: 0.5862, Val L: 0.6761, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000382\n",
      " Fold 3 Epoch 28/230: Tr L: 0.6580, Tr Acc: 0.5862, Val L: 0.6624, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000371\n",
      " Fold 3 Epoch 29/230: Tr L: 0.6582, Tr Acc: 0.5862, Val L: 0.6450, Val Acc: 0.6111, Val F1: 0.0000 lr: 0.000358\n",
      " Fold 3 Epoch 30/230: Tr L: 0.6624, Tr Acc: 0.5517, Val L: 0.6453, Val Acc: 0.6111, Val F1: 0.3636 lr: 0.000343\n",
      " Fold 3 Epoch 31/230: Tr L: 0.6613, Tr Acc: 0.5776, Val L: 0.6706, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000327\n",
      " Fold 3 Epoch 32/230: Tr L: 0.6500, Tr Acc: 0.5862, Val L: 0.6904, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000309\n",
      " Fold 3 Epoch 33/230: Tr L: 0.6538, Tr Acc: 0.5862, Val L: 0.6726, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000289\n",
      " Fold 3 Epoch 34/230: Tr L: 0.6422, Tr Acc: 0.5862, Val L: 0.6563, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000269\n",
      " Fold 3 Epoch 35/230: Tr L: 0.6440, Tr Acc: 0.6293, Val L: 0.6521, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000247\n",
      " Fold 3 Epoch 36/230: Tr L: 0.6480, Tr Acc: 0.6293, Val L: 0.6605, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000225\n",
      " Fold 3 Epoch 37/230: Tr L: 0.6533, Tr Acc: 0.5862, Val L: 0.6776, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000203\n",
      " Fold 3 Epoch 38/230: Tr L: 0.6558, Tr Acc: 0.5862, Val L: 0.6712, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000181\n",
      " Fold 3 Epoch 39/230: Tr L: 0.6525, Tr Acc: 0.5948, Val L: 0.6573, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000159\n",
      " Fold 3 Epoch 40/230: Tr L: 0.6444, Tr Acc: 0.6379, Val L: 0.6525, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000138\n",
      " Fold 3 Epoch 41/230: Tr L: 0.6418, Tr Acc: 0.6466, Val L: 0.6506, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000117\n",
      " Fold 3 Epoch 42/230: Tr L: 0.6467, Tr Acc: 0.6207, Val L: 0.6575, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000098\n",
      " Fold 3 Epoch 43/230: Tr L: 0.6477, Tr Acc: 0.6121, Val L: 0.6600, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000080\n",
      " Fold 3 Epoch 44/230: Tr L: 0.6521, Tr Acc: 0.5862, Val L: 0.6636, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000063\n",
      " Fold 3 Epoch 45/230: Tr L: 0.6448, Tr Acc: 0.5862, Val L: 0.6637, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000048\n",
      " Fold 3 Epoch 46/230: Tr L: 0.6438, Tr Acc: 0.5862, Val L: 0.6606, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000035\n",
      " Fold 3 Epoch 47/230: Tr L: 0.6400, Tr Acc: 0.5862, Val L: 0.6584, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000025\n",
      " Fold 3 Epoch 48/230: Tr L: 0.6466, Tr Acc: 0.5862, Val L: 0.6576, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000016\n",
      " Fold 3 Epoch 49/230: Tr L: 0.6490, Tr Acc: 0.5862, Val L: 0.6574, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000010\n",
      " Fold 3 Epoch 50/230: Tr L: 0.6445, Tr Acc: 0.6034, Val L: 0.6571, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000006\n",
      " Fold 3 Epoch 51/230: Tr L: 0.6381, Tr Acc: 0.5862, Val L: 0.6564, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000401\n",
      " Fold 3 Epoch 52/230: Tr L: 0.6447, Tr Acc: 0.5862, Val L: 0.6675, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000401\n",
      " Fold 3 Epoch 53/230: Tr L: 0.6545, Tr Acc: 0.5862, Val L: 0.6701, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000400\n",
      " Fold 3 Epoch 54/230: Tr L: 0.6486, Tr Acc: 0.6034, Val L: 0.6554, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000399\n",
      " Fold 3 Epoch 55/230: Tr L: 0.6427, Tr Acc: 0.6121, Val L: 0.6534, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000397\n",
      " Fold 3 Epoch 56/230: Tr L: 0.6497, Tr Acc: 0.6379, Val L: 0.6474, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000394\n",
      " Fold 3 Epoch 57/230: Tr L: 0.6472, Tr Acc: 0.6293, Val L: 0.6547, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000390\n",
      " Fold 3 Epoch 58/230: Tr L: 0.6441, Tr Acc: 0.5948, Val L: 0.6597, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000386\n",
      " Fold 3 Epoch 59/230: Tr L: 0.6484, Tr Acc: 0.5862, Val L: 0.6680, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000382\n",
      " Fold 3 Epoch 60/230: Tr L: 0.6457, Tr Acc: 0.5862, Val L: 0.6402, Val Acc: 0.4444, Val F1: 0.5000 lr: 0.000377\n",
      " Fold 3 Epoch 61/230: Tr L: 0.6424, Tr Acc: 0.6121, Val L: 0.6365, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000371\n",
      " Fold 3 Epoch 62/230: Tr L: 0.6446, Tr Acc: 0.6207, Val L: 0.6508, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000365\n",
      " Fold 3 Epoch 63/230: Tr L: 0.6425, Tr Acc: 0.5776, Val L: 0.6551, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000358\n",
      " Fold 3 Epoch 64/230: Tr L: 0.6382, Tr Acc: 0.5862, Val L: 0.6481, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000351\n",
      " Fold 3 Epoch 65/230: Tr L: 0.6426, Tr Acc: 0.5776, Val L: 0.6551, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000343\n",
      " Fold 3 Epoch 66/230: Tr L: 0.6500, Tr Acc: 0.6034, Val L: 0.6698, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000335\n",
      " Fold 3 Epoch 67/230: Tr L: 0.6517, Tr Acc: 0.5948, Val L: 0.6528, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000327\n",
      " Fold 3 Epoch 68/230: Tr L: 0.6404, Tr Acc: 0.5948, Val L: 0.6323, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000318\n",
      " Fold 3 Epoch 69/230: Tr L: 0.6431, Tr Acc: 0.5431, Val L: 0.6227, Val Acc: 0.6667, Val F1: 0.2500 lr: 0.000309\n",
      " Fold 3 Epoch 70/230: Tr L: 0.6455, Tr Acc: 0.5431, Val L: 0.6252, Val Acc: 0.5556, Val F1: 0.4286 lr: 0.000299\n",
      " Fold 3 Epoch 71/230: Tr L: 0.6431, Tr Acc: 0.5345, Val L: 0.6310, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000289\n",
      " Fold 3 Epoch 72/230: Tr L: 0.6358, Tr Acc: 0.6034, Val L: 0.6440, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000279\n",
      " Fold 3 Epoch 73/230: Tr L: 0.6359, Tr Acc: 0.6034, Val L: 0.6682, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000269\n",
      " Fold 3 Epoch 74/230: Tr L: 0.6381, Tr Acc: 0.6034, Val L: 0.6647, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000258\n",
      " Fold 3 Epoch 75/230: Tr L: 0.6373, Tr Acc: 0.6121, Val L: 0.6403, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000247\n",
      " Fold 3 Epoch 76/230: Tr L: 0.6361, Tr Acc: 0.6034, Val L: 0.6318, Val Acc: 0.4444, Val F1: 0.5000 lr: 0.000236\n",
      " Fold 3 Epoch 77/230: Tr L: 0.6339, Tr Acc: 0.5948, Val L: 0.6243, Val Acc: 0.4444, Val F1: 0.4444 lr: 0.000225\n",
      " Fold 3 Epoch 78/230: Tr L: 0.6389, Tr Acc: 0.5086, Val L: 0.6231, Val Acc: 0.3889, Val F1: 0.3529 lr: 0.000214\n",
      " Fold 3 Epoch 79/230: Tr L: 0.6444, Tr Acc: 0.5776, Val L: 0.6344, Val Acc: 0.4444, Val F1: 0.5000 lr: 0.000203\n",
      " Fold 3 Epoch 80/230: Tr L: 0.6427, Tr Acc: 0.6034, Val L: 0.6414, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000192\n",
      " Fold 3 Epoch 81/230: Tr L: 0.6351, Tr Acc: 0.6121, Val L: 0.6338, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000181\n",
      " Fold 3 Epoch 82/230: Tr L: 0.6368, Tr Acc: 0.6121, Val L: 0.6331, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000170\n",
      " Fold 3 Epoch 83/230: Tr L: 0.6279, Tr Acc: 0.6207, Val L: 0.6399, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000159\n",
      " Fold 3 Epoch 84/230: Tr L: 0.6350, Tr Acc: 0.6034, Val L: 0.6428, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000148\n",
      " Fold 3 Epoch 85/230: Tr L: 0.6350, Tr Acc: 0.6121, Val L: 0.6502, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000138\n",
      " Fold 3 Epoch 86/230: Tr L: 0.6389, Tr Acc: 0.6034, Val L: 0.6449, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000127\n",
      " Fold 3 Epoch 87/230: Tr L: 0.6351, Tr Acc: 0.6121, Val L: 0.6362, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000117\n",
      " Fold 3 Epoch 88/230: Tr L: 0.6361, Tr Acc: 0.6034, Val L: 0.6290, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000107\n",
      " Fold 3 Epoch 89/230: Tr L: 0.6287, Tr Acc: 0.6121, Val L: 0.6284, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000098\n",
      " Fold 3 Epoch 90/230: Tr L: 0.6363, Tr Acc: 0.6121, Val L: 0.6288, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000089\n",
      " Fold 3 Epoch 91/230: Tr L: 0.6345, Tr Acc: 0.6207, Val L: 0.6274, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000080\n",
      " Fold 3 Epoch 92/230: Tr L: 0.6373, Tr Acc: 0.6207, Val L: 0.6313, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000071\n",
      " Fold 3 Epoch 93/230: Tr L: 0.6434, Tr Acc: 0.6034, Val L: 0.6380, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000063\n",
      " Fold 3 Epoch 94/230: Tr L: 0.6371, Tr Acc: 0.6034, Val L: 0.6402, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000055\n",
      " Fold 3 Epoch 95/230: Tr L: 0.6385, Tr Acc: 0.6034, Val L: 0.6406, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000048\n",
      " Fold 3 Epoch 96/230: Tr L: 0.6338, Tr Acc: 0.6293, Val L: 0.6405, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000042\n",
      " Fold 3 Epoch 97/230: Tr L: 0.6247, Tr Acc: 0.6293, Val L: 0.6402, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000035\n",
      " Fold 3 Epoch 98/230: Tr L: 0.6405, Tr Acc: 0.6034, Val L: 0.6418, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000030\n",
      " Fold 3 Epoch 99/230: Tr L: 0.6370, Tr Acc: 0.6034, Val L: 0.6413, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000025\n",
      " Fold 3 Epoch 100/230: Tr L: 0.6466, Tr Acc: 0.6034, Val L: 0.6410, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000020\n",
      " Fold 3 Epoch 101/230: Tr L: 0.6305, Tr Acc: 0.6293, Val L: 0.6405, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000016\n",
      " Fold 3 Epoch 102/230: Tr L: 0.6364, Tr Acc: 0.6293, Val L: 0.6400, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000013\n",
      " Fold 3 Epoch 103/230: Tr L: 0.6412, Tr Acc: 0.6121, Val L: 0.6393, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000010\n",
      " Fold 3 Epoch 104/230: Tr L: 0.6351, Tr Acc: 0.6121, Val L: 0.6392, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000008\n",
      " Fold 3 Epoch 105/230: Tr L: 0.6303, Tr Acc: 0.6121, Val L: 0.6390, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000006\n",
      " Fold 3 Epoch 106/230: Tr L: 0.6302, Tr Acc: 0.6121, Val L: 0.6388, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000005\n",
      " Fold 3 Epoch 107/230: Tr L: 0.6463, Tr Acc: 0.5603, Val L: 0.6187, Val Acc: 0.4444, Val F1: 0.4444 lr: 0.000401\n",
      " Fold 3 Epoch 108/230: Tr L: 0.6304, Tr Acc: 0.6034, Val L: 0.6391, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000401\n",
      " Fold 3 Epoch 109/230: Tr L: 0.6367, Tr Acc: 0.6034, Val L: 0.6458, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000401\n",
      " Fold 3 Epoch 110/230: Tr L: 0.6407, Tr Acc: 0.5948, Val L: 0.6542, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000401\n",
      " Fold 3 Epoch 111/230: Tr L: 0.6465, Tr Acc: 0.6034, Val L: 0.6203, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000400\n",
      " Fold 3 Epoch 112/230: Tr L: 0.6426, Tr Acc: 0.5259, Val L: 0.6111, Val Acc: 0.7222, Val F1: 0.4444 lr: 0.000400\n",
      " Fold 3 Epoch 113/230: Tr L: 0.6369, Tr Acc: 0.5345, Val L: 0.6239, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000399\n",
      " Fold 3 Epoch 114/230: Tr L: 0.6384, Tr Acc: 0.6034, Val L: 0.6591, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000398\n",
      " Fold 3 Epoch 115/230: Tr L: 0.6298, Tr Acc: 0.6121, Val L: 0.6413, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000397\n",
      " Fold 3 Epoch 116/230: Tr L: 0.6289, Tr Acc: 0.6034, Val L: 0.6486, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000395\n",
      " Fold 3 Epoch 117/230: Tr L: 0.6409, Tr Acc: 0.6121, Val L: 0.6463, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000394\n",
      " Fold 3 Epoch 118/230: Tr L: 0.6281, Tr Acc: 0.6034, Val L: 0.6319, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000392\n",
      " Fold 3 Epoch 119/230: Tr L: 0.6408, Tr Acc: 0.5259, Val L: 0.6086, Val Acc: 0.7222, Val F1: 0.4444 lr: 0.000390\n",
      " Fold 3 Epoch 120/230: Tr L: 0.6433, Tr Acc: 0.4828, Val L: 0.6154, Val Acc: 0.4444, Val F1: 0.5000 lr: 0.000388\n",
      " Fold 3 Epoch 121/230: Tr L: 0.6287, Tr Acc: 0.6121, Val L: 0.6309, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000386\n",
      " Fold 3 Epoch 122/230: Tr L: 0.6330, Tr Acc: 0.5948, Val L: 0.6479, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000384\n",
      " Fold 3 Epoch 123/230: Tr L: 0.6298, Tr Acc: 0.6034, Val L: 0.6291, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000382\n",
      " Fold 3 Epoch 124/230: Tr L: 0.6426, Tr Acc: 0.5345, Val L: 0.6082, Val Acc: 0.6667, Val F1: 0.4000 lr: 0.000379\n",
      " Fold 3 Epoch 125/230: Tr L: 0.6343, Tr Acc: 0.5862, Val L: 0.6273, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000377\n",
      " Fold 3 Epoch 126/230: Tr L: 0.6280, Tr Acc: 0.6207, Val L: 0.6334, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000374\n",
      " Fold 3 Epoch 127/230: Tr L: 0.6297, Tr Acc: 0.6121, Val L: 0.6202, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000371\n",
      " Fold 3 Epoch 128/230: Tr L: 0.6391, Tr Acc: 0.6207, Val L: 0.6224, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000368\n",
      " Fold 3 Epoch 129/230: Tr L: 0.6305, Tr Acc: 0.6121, Val L: 0.6281, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000365\n",
      " Fold 3 Epoch 130/230: Tr L: 0.6250, Tr Acc: 0.6293, Val L: 0.6278, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000362\n",
      " Fold 3 Epoch 131/230: Tr L: 0.6314, Tr Acc: 0.6034, Val L: 0.6393, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000358\n",
      " Fold 3 Epoch 132/230: Tr L: 0.6220, Tr Acc: 0.6121, Val L: 0.6314, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000355\n",
      " Fold 3 Epoch 133/230: Tr L: 0.6476, Tr Acc: 0.6207, Val L: 0.6220, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000351\n",
      " Fold 3 Epoch 134/230: Tr L: 0.6303, Tr Acc: 0.6293, Val L: 0.6152, Val Acc: 0.4444, Val F1: 0.4444 lr: 0.000347\n",
      " Fold 3 Epoch 135/230: Tr L: 0.6350, Tr Acc: 0.5948, Val L: 0.6161, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000343\n",
      " Fold 3 Epoch 136/230: Tr L: 0.6346, Tr Acc: 0.6207, Val L: 0.6203, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000339\n",
      " Fold 3 Epoch 137/230: Tr L: 0.6296, Tr Acc: 0.6293, Val L: 0.6230, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000335\n",
      " Fold 3 Epoch 138/230: Tr L: 0.6353, Tr Acc: 0.6207, Val L: 0.6428, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000331\n",
      " Fold 3 Epoch 139/230: Tr L: 0.6289, Tr Acc: 0.6034, Val L: 0.6188, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000327\n",
      " Fold 3 Epoch 140/230: Tr L: 0.6462, Tr Acc: 0.4741, Val L: 0.5990, Val Acc: 0.7222, Val F1: 0.4444 lr: 0.000322\n",
      " Fold 3 Epoch 141/230: Tr L: 0.6413, Tr Acc: 0.5431, Val L: 0.6020, Val Acc: 0.7222, Val F1: 0.4444 lr: 0.000318\n",
      " Fold 3 Epoch 142/230: Tr L: 0.6313, Tr Acc: 0.5345, Val L: 0.6123, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000313\n",
      " Fold 3 Epoch 143/230: Tr L: 0.6331, Tr Acc: 0.6121, Val L: 0.6316, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000309\n",
      " Fold 3 Epoch 144/230: Tr L: 0.6281, Tr Acc: 0.6293, Val L: 0.6304, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000304\n",
      " Fold 3 Epoch 145/230: Tr L: 0.6210, Tr Acc: 0.6293, Val L: 0.6160, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000299\n",
      " Fold 3 Epoch 146/230: Tr L: 0.6196, Tr Acc: 0.6207, Val L: 0.6185, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000294\n",
      " Fold 3 Epoch 147/230: Tr L: 0.6327, Tr Acc: 0.6293, Val L: 0.6345, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000289\n",
      " Fold 3 Epoch 148/230: Tr L: 0.6307, Tr Acc: 0.6121, Val L: 0.6252, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000284\n",
      " Fold 3 Epoch 149/230: Tr L: 0.6277, Tr Acc: 0.6207, Val L: 0.6285, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000279\n",
      " Fold 3 Epoch 150/230: Tr L: 0.6310, Tr Acc: 0.6121, Val L: 0.6355, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000274\n",
      " Fold 3 Epoch 151/230: Tr L: 0.6137, Tr Acc: 0.6121, Val L: 0.6141, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000269\n",
      " Fold 3 Epoch 152/230: Tr L: 0.6285, Tr Acc: 0.6034, Val L: 0.6073, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000263\n",
      " Fold 3 Epoch 153/230: Tr L: 0.6304, Tr Acc: 0.5862, Val L: 0.6211, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000258\n",
      " Fold 3 Epoch 154/230: Tr L: 0.6278, Tr Acc: 0.6293, Val L: 0.6312, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000253\n",
      " Fold 3 Epoch 155/230: Tr L: 0.6267, Tr Acc: 0.6034, Val L: 0.6393, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000247\n",
      " Fold 3 Epoch 156/230: Tr L: 0.6246, Tr Acc: 0.6034, Val L: 0.6286, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000242\n",
      " Fold 3 Epoch 157/230: Tr L: 0.6282, Tr Acc: 0.6121, Val L: 0.6231, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000236\n",
      " Fold 3 Epoch 158/230: Tr L: 0.6324, Tr Acc: 0.6121, Val L: 0.6051, Val Acc: 0.5556, Val F1: 0.5556 lr: 0.000231\n",
      " Fold 3 Epoch 159/230: Tr L: 0.6321, Tr Acc: 0.6121, Val L: 0.6060, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000225\n",
      " Fold 3 Epoch 160/230: Tr L: 0.6268, Tr Acc: 0.6034, Val L: 0.6081, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000220\n",
      " Fold 3 Epoch 161/230: Tr L: 0.6247, Tr Acc: 0.6207, Val L: 0.6093, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000214\n",
      " Fold 3 Epoch 162/230: Tr L: 0.6288, Tr Acc: 0.6379, Val L: 0.6263, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000209\n",
      " Fold 3 Epoch 163/230: Tr L: 0.6219, Tr Acc: 0.6207, Val L: 0.6322, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000203\n",
      " Fold 3 Epoch 164/230: Tr L: 0.6308, Tr Acc: 0.6207, Val L: 0.6225, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000198\n",
      " Fold 3 Epoch 165/230: Tr L: 0.6207, Tr Acc: 0.6379, Val L: 0.6149, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000192\n",
      " Fold 3 Epoch 166/230: Tr L: 0.6256, Tr Acc: 0.6293, Val L: 0.6128, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000187\n",
      " Fold 3 Epoch 167/230: Tr L: 0.6216, Tr Acc: 0.6293, Val L: 0.6223, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000181\n",
      " Fold 3 Epoch 168/230: Tr L: 0.6284, Tr Acc: 0.6293, Val L: 0.6248, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000176\n",
      " Fold 3 Epoch 169/230: Tr L: 0.6260, Tr Acc: 0.6293, Val L: 0.6239, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000170\n",
      " Fold 3 Epoch 170/230: Tr L: 0.6314, Tr Acc: 0.6207, Val L: 0.6297, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000165\n",
      " Fold 3 Epoch 171/230: Tr L: 0.6234, Tr Acc: 0.6034, Val L: 0.6302, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000159\n",
      " Fold 3 Epoch 172/230: Tr L: 0.6325, Tr Acc: 0.6034, Val L: 0.6341, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000154\n",
      " Fold 3 Epoch 173/230: Tr L: 0.6297, Tr Acc: 0.6034, Val L: 0.6223, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000148\n",
      " Fold 3 Epoch 174/230: Tr L: 0.6271, Tr Acc: 0.6293, Val L: 0.6195, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000143\n",
      " Fold 3 Epoch 175/230: Tr L: 0.6243, Tr Acc: 0.6379, Val L: 0.6088, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000138\n",
      " Fold 3 Epoch 176/230: Tr L: 0.6251, Tr Acc: 0.6379, Val L: 0.6093, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000133\n",
      " Fold 3 Epoch 177/230: Tr L: 0.6243, Tr Acc: 0.6293, Val L: 0.6138, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000127\n",
      " Fold 3 Epoch 178/230: Tr L: 0.6279, Tr Acc: 0.6293, Val L: 0.6162, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000122\n",
      " Fold 3 Epoch 179/230: Tr L: 0.6217, Tr Acc: 0.6293, Val L: 0.6174, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000117\n",
      " Fold 3 Epoch 180/230: Tr L: 0.6152, Tr Acc: 0.6293, Val L: 0.6152, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000112\n",
      "Early stopping triggered at epoch 180 for fold 3\n",
      "--- Evaluating Fold 3 on Outer Test Set ---\n",
      "Warning: model_factory used without specific LR, using default/cfg LR: 0.001\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.001\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "Test set class counts for fold 3: {0: 14, 1: 9}\n",
      "percentage of classes in test set: 0    0.608696\n",
      "1    0.391304\n",
      "Name: count, dtype: float64\n",
      " [FOLD 3 FINAL] Test Loss: 0.5484 | Test Acc: 0.5652 | test Balanced Acc: 0.4643 | test F1: 0.0000 | Test AUC: 1.0000\n",
      "model class name: ViTFinetuneModule\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 19:27:24,624] A new study created in memory with name: no-name-dc938d8e-9036-4301-aba3-a7dd800a0512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== OUTER FOLD 4 / 6 =====\n",
      "Outer Train images: 117 | Outer Test images: 23\n",
      "--- Calculating normalization stats for Fold 4 Training Data ---\n",
      "Fold 4 stats: {'mean': [0.029821500182151794, 0.011338564567267895, 0.07835365831851959], 'std': [0.057060033082962036, 0.017478104680776596, 0.09008630365133286]}\n",
      "--- Generating data transforms for Fold 4 ---\n",
      "Using the train and val transform passed as arguments\n",
      "--- Starting Hyperparameter Tuning for Fold 4 ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=4.715696678089837e-05\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=4.715696678089837e-05\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=4.715696678089837e-05\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=4.715696678089837e-05\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 19:27:29,637] Trial 0 finished with value: 0.7549209892749786 and parameters: {'lr': 4.715696678089837e-05}. Best is trial 0 with value: 0.7549209892749786.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0014886262201211794\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0014886262201211794\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0014886262201211794\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0014886262201211794\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 19:27:34,547] Trial 1 finished with value: 0.6607005894184113 and parameters: {'lr': 0.0014886262201211794}. Best is trial 1 with value: 0.6607005894184113.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0004014783718209777\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0004014783718209777\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0004014783718209777\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0004014783718209777\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 19:27:39,612] Trial 2 finished with value: 0.6807122230529785 and parameters: {'lr': 0.0004014783718209777}. Best is trial 1 with value: 0.6607005894184113.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Best LR from inner CV = 0.001489\n",
      "--- Starting Final Model Training for Fold 4 with LR=0.001489 ---\n",
      "X_train_es: (99,) | X_val_es: (18,)\n",
      "Early stopping split: Train images: 99, Validation images: 18\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0014886262201211794\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "===========================\n",
      "Model Architecture:\n",
      "==================\n",
      "Total parameters: 87,456,770\n",
      "Trainable parameters: 1,538\n",
      "Non-trainable parameters: 87,455,232\n",
      "===========================\n",
      "Using CosineAnnealingWarmRestarts scheduler\n",
      " Fold 4 Epoch 1/230: Tr L: 0.6939, Tr Acc: 0.5603, Val L: 0.9409, Val Acc: 0.5556, Val F1: 0.2000 lr: 0.001489\n",
      " Fold 4 Epoch 2/230: Tr L: 0.6896, Tr Acc: 0.5517, Val L: 1.1728, Val Acc: 0.5000, Val F1: 0.4000 lr: 0.001489\n",
      " Fold 4 Epoch 3/230: Tr L: 0.6529, Tr Acc: 0.6466, Val L: 0.8527, Val Acc: 0.6111, Val F1: 0.4615 lr: 0.001415\n",
      " Fold 4 Epoch 4/230: Tr L: 0.6403, Tr Acc: 0.6379, Val L: 0.7526, Val Acc: 0.3889, Val F1: 0.3529 lr: 0.001209\n",
      " Fold 4 Epoch 5/230: Tr L: 0.6258, Tr Acc: 0.6552, Val L: 0.7668, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000912\n",
      " Fold 4 Epoch 6/230: Tr L: 0.6162, Tr Acc: 0.6552, Val L: 0.7608, Val Acc: 0.4444, Val F1: 0.4444 lr: 0.000582\n",
      " Fold 4 Epoch 7/230: Tr L: 0.6130, Tr Acc: 0.6638, Val L: 0.7589, Val Acc: 0.3889, Val F1: 0.3529 lr: 0.000284\n",
      " Fold 4 Epoch 8/230: Tr L: 0.6019, Tr Acc: 0.6638, Val L: 0.7633, Val Acc: 0.3889, Val F1: 0.3529 lr: 0.000078\n",
      " Fold 4 Epoch 9/230: Tr L: 0.5970, Tr Acc: 0.6638, Val L: 0.8758, Val Acc: 0.3889, Val F1: 0.3529 lr: 0.001489\n",
      " Fold 4 Epoch 10/230: Tr L: 0.6134, Tr Acc: 0.6638, Val L: 0.8261, Val Acc: 0.5000, Val F1: 0.4000 lr: 0.001470\n",
      " Fold 4 Epoch 11/230: Tr L: 0.6046, Tr Acc: 0.6552, Val L: 0.8086, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.001415\n",
      " Fold 4 Epoch 12/230: Tr L: 0.5965, Tr Acc: 0.6552, Val L: 0.8048, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.001327\n",
      " Fold 4 Epoch 13/230: Tr L: 0.5926, Tr Acc: 0.6552, Val L: 0.7899, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.001209\n",
      " Fold 4 Epoch 14/230: Tr L: 0.5958, Tr Acc: 0.6552, Val L: 0.8018, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.001069\n",
      " Fold 4 Epoch 15/230: Tr L: 0.6030, Tr Acc: 0.6638, Val L: 0.8127, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000912\n",
      " Fold 4 Epoch 16/230: Tr L: 0.6038, Tr Acc: 0.6552, Val L: 0.8138, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000747\n",
      " Fold 4 Epoch 17/230: Tr L: 0.5828, Tr Acc: 0.6638, Val L: 0.7667, Val Acc: 0.3889, Val F1: 0.1538 lr: 0.000582\n",
      " Fold 4 Epoch 18/230: Tr L: 0.6002, Tr Acc: 0.5776, Val L: 0.7595, Val Acc: 0.4444, Val F1: 0.1667 lr: 0.000425\n",
      " Fold 4 Epoch 19/230: Tr L: 0.5934, Tr Acc: 0.6379, Val L: 0.7768, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000284\n",
      " Fold 4 Epoch 20/230: Tr L: 0.5817, Tr Acc: 0.6724, Val L: 0.7919, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000167\n",
      " Fold 4 Epoch 21/230: Tr L: 0.5824, Tr Acc: 0.6552, Val L: 0.8056, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000078\n",
      " Fold 4 Epoch 22/230: Tr L: 0.5866, Tr Acc: 0.6552, Val L: 0.8084, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000024\n",
      " Fold 4 Epoch 23/230: Tr L: 0.5760, Tr Acc: 0.6552, Val L: 0.8737, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.001489\n",
      " Fold 4 Epoch 24/230: Tr L: 0.6048, Tr Acc: 0.6552, Val L: 0.8846, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.001484\n",
      " Fold 4 Epoch 25/230: Tr L: 0.5790, Tr Acc: 0.6983, Val L: 0.7767, Val Acc: 0.4444, Val F1: 0.0000 lr: 0.001470\n",
      " Fold 4 Epoch 26/230: Tr L: 0.6516, Tr Acc: 0.5345, Val L: 0.7659, Val Acc: 0.4444, Val F1: 0.0000 lr: 0.001447\n",
      " Fold 4 Epoch 27/230: Tr L: 0.5931, Tr Acc: 0.5948, Val L: 0.8346, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.001415\n",
      " Fold 4 Epoch 28/230: Tr L: 0.6066, Tr Acc: 0.6552, Val L: 0.9078, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.001375\n",
      " Fold 4 Epoch 29/230: Tr L: 0.5886, Tr Acc: 0.6552, Val L: 0.7624, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.001327\n",
      " Fold 4 Epoch 30/230: Tr L: 0.5897, Tr Acc: 0.6379, Val L: 0.7679, Val Acc: 0.5000, Val F1: 0.4000 lr: 0.001271\n",
      " Fold 4 Epoch 31/230: Tr L: 0.5940, Tr Acc: 0.6810, Val L: 0.8847, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.001209\n",
      " Fold 4 Epoch 32/230: Tr L: 0.5812, Tr Acc: 0.6552, Val L: 0.8069, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.001141\n",
      " Fold 4 Epoch 33/230: Tr L: 0.5879, Tr Acc: 0.5862, Val L: 0.7259, Val Acc: 0.5556, Val F1: 0.3333 lr: 0.001069\n",
      " Fold 4 Epoch 34/230: Tr L: 0.6005, Tr Acc: 0.5431, Val L: 0.7838, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000992\n",
      " Fold 4 Epoch 35/230: Tr L: 0.5826, Tr Acc: 0.6552, Val L: 0.8500, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000912\n",
      " Fold 4 Epoch 36/230: Tr L: 0.5736, Tr Acc: 0.6552, Val L: 0.7687, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000830\n",
      " Fold 4 Epoch 37/230: Tr L: 0.5751, Tr Acc: 0.6897, Val L: 0.7606, Val Acc: 0.6111, Val F1: 0.4615 lr: 0.000747\n",
      " Fold 4 Epoch 38/230: Tr L: 0.5706, Tr Acc: 0.6638, Val L: 0.8464, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000664\n",
      " Fold 4 Epoch 39/230: Tr L: 0.5819, Tr Acc: 0.6552, Val L: 0.8692, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000582\n",
      " Fold 4 Epoch 40/230: Tr L: 0.5828, Tr Acc: 0.6638, Val L: 0.8753, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000502\n",
      " Fold 4 Epoch 41/230: Tr L: 0.5709, Tr Acc: 0.6552, Val L: 0.8238, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000425\n",
      " Fold 4 Epoch 42/230: Tr L: 0.5630, Tr Acc: 0.6638, Val L: 0.8042, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000352\n",
      " Fold 4 Epoch 43/230: Tr L: 0.5628, Tr Acc: 0.6552, Val L: 0.7944, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000284\n",
      " Fold 4 Epoch 44/230: Tr L: 0.5672, Tr Acc: 0.6552, Val L: 0.7997, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000222\n",
      " Fold 4 Epoch 45/230: Tr L: 0.5706, Tr Acc: 0.6552, Val L: 0.8191, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000167\n",
      " Fold 4 Epoch 46/230: Tr L: 0.5719, Tr Acc: 0.6552, Val L: 0.8176, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000119\n",
      " Fold 4 Epoch 47/230: Tr L: 0.5675, Tr Acc: 0.6552, Val L: 0.8121, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000078\n",
      " Fold 4 Epoch 48/230: Tr L: 0.5722, Tr Acc: 0.6552, Val L: 0.8093, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000047\n",
      " Fold 4 Epoch 49/230: Tr L: 0.5766, Tr Acc: 0.6552, Val L: 0.8090, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000024\n",
      " Fold 4 Epoch 50/230: Tr L: 0.5797, Tr Acc: 0.6552, Val L: 0.8085, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000010\n",
      " Fold 4 Epoch 51/230: Tr L: 0.5715, Tr Acc: 0.6552, Val L: 0.7892, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.001489\n",
      " Fold 4 Epoch 52/230: Tr L: 0.5985, Tr Acc: 0.6552, Val L: 0.9099, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.001487\n",
      " Fold 4 Epoch 53/230: Tr L: 0.5863, Tr Acc: 0.6638, Val L: 0.7569, Val Acc: 0.5000, Val F1: 0.1818 lr: 0.001484\n",
      " Fold 4 Epoch 54/230: Tr L: 0.6379, Tr Acc: 0.5431, Val L: 0.7433, Val Acc: 0.4444, Val F1: 0.0000 lr: 0.001478\n",
      " Fold 4 Epoch 55/230: Tr L: 0.5953, Tr Acc: 0.5948, Val L: 0.9061, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.001470\n",
      " Fold 4 Epoch 56/230: Tr L: 0.5814, Tr Acc: 0.6552, Val L: 0.8637, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.001460\n",
      " Fold 4 Epoch 57/230: Tr L: 0.5920, Tr Acc: 0.6034, Val L: 0.7794, Val Acc: 0.5556, Val F1: 0.3333 lr: 0.001447\n",
      " Fold 4 Epoch 58/230: Tr L: 0.5982, Tr Acc: 0.5690, Val L: 0.8658, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.001432\n",
      " Fold 4 Epoch 59/230: Tr L: 0.6132, Tr Acc: 0.6552, Val L: 0.9718, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.001415\n",
      " Fold 4 Epoch 60/230: Tr L: 0.5734, Tr Acc: 0.6552, Val L: 0.7634, Val Acc: 0.5556, Val F1: 0.4286 lr: 0.001396\n",
      " Fold 4 Epoch 61/230: Tr L: 0.5960, Tr Acc: 0.5259, Val L: 0.8026, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.001375\n",
      " Fold 4 Epoch 62/230: Tr L: 0.5730, Tr Acc: 0.6552, Val L: 0.8834, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.001352\n",
      " Fold 4 Epoch 63/230: Tr L: 0.5910, Tr Acc: 0.5690, Val L: 0.7794, Val Acc: 0.6111, Val F1: 0.4615 lr: 0.001327\n",
      " Fold 4 Epoch 64/230: Tr L: 0.6139, Tr Acc: 0.5431, Val L: 0.7839, Val Acc: 0.6111, Val F1: 0.4615 lr: 0.001300\n",
      " Fold 4 Epoch 65/230: Tr L: 0.5937, Tr Acc: 0.6466, Val L: 0.9684, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.001271\n",
      " Fold 4 Epoch 66/230: Tr L: 0.5935, Tr Acc: 0.6552, Val L: 0.7878, Val Acc: 0.5556, Val F1: 0.5556 lr: 0.001241\n",
      " Fold 4 Epoch 67/230: Tr L: 0.5734, Tr Acc: 0.6379, Val L: 0.7866, Val Acc: 0.5000, Val F1: 0.4706 lr: 0.001209\n",
      " Fold 4 Epoch 68/230: Tr L: 0.5735, Tr Acc: 0.6466, Val L: 0.9215, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.001176\n",
      " Fold 4 Epoch 69/230: Tr L: 0.5738, Tr Acc: 0.6466, Val L: 0.8083, Val Acc: 0.6111, Val F1: 0.4615 lr: 0.001141\n",
      " Fold 4 Epoch 70/230: Tr L: 0.5718, Tr Acc: 0.6810, Val L: 0.7523, Val Acc: 0.6111, Val F1: 0.4615 lr: 0.001106\n",
      " Fold 4 Epoch 71/230: Tr L: 0.5634, Tr Acc: 0.6638, Val L: 0.8511, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.001069\n",
      " Fold 4 Epoch 72/230: Tr L: 0.5653, Tr Acc: 0.6724, Val L: 0.8563, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.001031\n",
      " Fold 4 Epoch 73/230: Tr L: 0.5722, Tr Acc: 0.6638, Val L: 0.8429, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000992\n",
      "Early stopping triggered at epoch 73 for fold 4\n",
      "--- Evaluating Fold 4 on Outer Test Set ---\n",
      "Warning: model_factory used without specific LR, using default/cfg LR: 0.001\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.001\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "Test set class counts for fold 4: {0: 14, 1: 9}\n",
      "percentage of classes in test set: 0    0.608696\n",
      "1    0.391304\n",
      "Name: count, dtype: float64\n",
      " [FOLD 4 FINAL] Test Loss: 0.6347 | Test Acc: 0.5652 | test Balanced Acc: 0.4841 | test F1: 0.1667 | Test AUC: 1.0000\n",
      "model class name: ViTFinetuneModule\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 19:28:00,727] A new study created in memory with name: no-name-d67771dd-0a67-4b9b-90e7-cf7ccd846d20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== OUTER FOLD 5 / 6 =====\n",
      "Outer Train images: 119 | Outer Test images: 21\n",
      "--- Calculating normalization stats for Fold 5 Training Data ---\n",
      "Fold 5 stats: {'mean': [0.029491955414414406, 0.011217240244150162, 0.0780130922794342], 'std': [0.05655994638800621, 0.017444733530282974, 0.08970917761325836]}\n",
      "--- Generating data transforms for Fold 5 ---\n",
      "Using the train and val transform passed as arguments\n",
      "--- Starting Hyperparameter Tuning for Fold 5 ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=4.715696678089837e-05\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=4.715696678089837e-05\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=4.715696678089837e-05\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=4.715696678089837e-05\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 19:28:05,889] Trial 0 finished with value: 0.7482298612594604 and parameters: {'lr': 4.715696678089837e-05}. Best is trial 0 with value: 0.7482298612594604.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0014886262201211794\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0014886262201211794\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0014886262201211794\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0014886262201211794\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 19:28:11,048] Trial 1 finished with value: 0.6585778445005417 and parameters: {'lr': 0.0014886262201211794}. Best is trial 1 with value: 0.6585778445005417.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0004014783718209777\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0004014783718209777\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0004014783718209777\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0004014783718209777\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 19:28:16,196] Trial 2 finished with value: 0.667703703045845 and parameters: {'lr': 0.0004014783718209777}. Best is trial 1 with value: 0.6585778445005417.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Best LR from inner CV = 0.001489\n",
      "--- Starting Final Model Training for Fold 5 with LR=0.001489 ---\n",
      "X_train_es: (101,) | X_val_es: (18,)\n",
      "Early stopping split: Train images: 101, Validation images: 18\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0014886262201211794\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "===========================\n",
      "Model Architecture:\n",
      "==================\n",
      "Total parameters: 87,456,770\n",
      "Trainable parameters: 1,538\n",
      "Non-trainable parameters: 87,455,232\n",
      "===========================\n",
      "Using CosineAnnealingWarmRestarts scheduler\n",
      " Fold 5 Epoch 1/230: Tr L: 0.8240, Tr Acc: 0.4917, Val L: 1.1005, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.001489\n",
      " Fold 5 Epoch 2/230: Tr L: 0.6296, Tr Acc: 0.7000, Val L: 0.7332, Val Acc: 0.6111, Val F1: 0.0000 lr: 0.001489\n",
      " Fold 5 Epoch 3/230: Tr L: 0.7072, Tr Acc: 0.5083, Val L: 0.7157, Val Acc: 0.5000, Val F1: 0.4000 lr: 0.001415\n",
      " Fold 5 Epoch 4/230: Tr L: 0.6115, Tr Acc: 0.6833, Val L: 0.7476, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.001209\n",
      " Fold 5 Epoch 5/230: Tr L: 0.5943, Tr Acc: 0.6833, Val L: 0.7188, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000912\n",
      " Fold 5 Epoch 6/230: Tr L: 0.5986, Tr Acc: 0.6833, Val L: 0.7154, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000582\n",
      " Fold 5 Epoch 7/230: Tr L: 0.5885, Tr Acc: 0.6833, Val L: 0.7122, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000284\n",
      " Fold 5 Epoch 8/230: Tr L: 0.5816, Tr Acc: 0.6833, Val L: 0.7094, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000078\n",
      " Fold 5 Epoch 9/230: Tr L: 0.5870, Tr Acc: 0.7083, Val L: 0.7383, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.001489\n",
      " Fold 5 Epoch 10/230: Tr L: 0.6249, Tr Acc: 0.7333, Val L: 0.7613, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.001470\n",
      " Fold 5 Epoch 11/230: Tr L: 0.6046, Tr Acc: 0.7250, Val L: 0.7289, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.001415\n",
      " Fold 5 Epoch 12/230: Tr L: 0.5867, Tr Acc: 0.7000, Val L: 0.7165, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.001327\n",
      " Fold 5 Epoch 13/230: Tr L: 0.5984, Tr Acc: 0.6917, Val L: 0.6739, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.001209\n",
      " Fold 5 Epoch 14/230: Tr L: 0.5913, Tr Acc: 0.6833, Val L: 0.7186, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.001069\n",
      " Fold 5 Epoch 15/230: Tr L: 0.5908, Tr Acc: 0.6833, Val L: 0.7573, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000912\n",
      " Fold 5 Epoch 16/230: Tr L: 0.5802, Tr Acc: 0.7167, Val L: 0.6732, Val Acc: 0.5556, Val F1: 0.5000 lr: 0.000747\n",
      " Fold 5 Epoch 17/230: Tr L: 0.6019, Tr Acc: 0.6750, Val L: 0.6734, Val Acc: 0.5556, Val F1: 0.5556 lr: 0.000582\n",
      " Fold 5 Epoch 18/230: Tr L: 0.5909, Tr Acc: 0.6917, Val L: 0.7305, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000425\n",
      " Fold 5 Epoch 19/230: Tr L: 0.5746, Tr Acc: 0.6833, Val L: 0.7560, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000284\n",
      " Fold 5 Epoch 20/230: Tr L: 0.5757, Tr Acc: 0.6833, Val L: 0.7533, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000167\n",
      " Fold 5 Epoch 21/230: Tr L: 0.5781, Tr Acc: 0.6833, Val L: 0.7428, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000078\n",
      " Fold 5 Epoch 22/230: Tr L: 0.5764, Tr Acc: 0.6833, Val L: 0.7378, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000024\n",
      " Fold 5 Epoch 23/230: Tr L: 0.5859, Tr Acc: 0.7250, Val L: 0.6893, Val Acc: 0.5556, Val F1: 0.5556 lr: 0.001489\n",
      " Fold 5 Epoch 24/230: Tr L: 0.5775, Tr Acc: 0.7167, Val L: 0.8094, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.001484\n",
      " Fold 5 Epoch 25/230: Tr L: 0.6089, Tr Acc: 0.6833, Val L: 0.7389, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.001470\n",
      " Fold 5 Epoch 26/230: Tr L: 0.5800, Tr Acc: 0.6833, Val L: 0.6650, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.001447\n",
      " Fold 5 Epoch 27/230: Tr L: 0.5788, Tr Acc: 0.7167, Val L: 0.7077, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.001415\n",
      " Fold 5 Epoch 28/230: Tr L: 0.5905, Tr Acc: 0.6833, Val L: 0.7800, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.001375\n",
      " Fold 5 Epoch 29/230: Tr L: 0.5774, Tr Acc: 0.7000, Val L: 0.6731, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.001327\n",
      " Fold 5 Epoch 30/230: Tr L: 0.5713, Tr Acc: 0.6917, Val L: 0.7718, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.001271\n",
      " Fold 5 Epoch 31/230: Tr L: 0.6056, Tr Acc: 0.7083, Val L: 0.8051, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.001209\n",
      " Fold 5 Epoch 32/230: Tr L: 0.5824, Tr Acc: 0.7000, Val L: 0.6749, Val Acc: 0.5000, Val F1: 0.4706 lr: 0.001141\n",
      " Fold 5 Epoch 33/230: Tr L: 0.5806, Tr Acc: 0.7167, Val L: 0.7235, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.001069\n",
      " Fold 5 Epoch 34/230: Tr L: 0.5920, Tr Acc: 0.6833, Val L: 0.8066, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000992\n",
      " Fold 5 Epoch 35/230: Tr L: 0.5780, Tr Acc: 0.6833, Val L: 0.6947, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000912\n",
      " Fold 5 Epoch 36/230: Tr L: 0.5685, Tr Acc: 0.6833, Val L: 0.6602, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000830\n",
      " Fold 5 Epoch 37/230: Tr L: 0.5761, Tr Acc: 0.6833, Val L: 0.7408, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000747\n",
      " Fold 5 Epoch 38/230: Tr L: 0.5726, Tr Acc: 0.6833, Val L: 0.7187, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000664\n",
      " Fold 5 Epoch 39/230: Tr L: 0.5733, Tr Acc: 0.6833, Val L: 0.7129, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000582\n",
      " Fold 5 Epoch 40/230: Tr L: 0.5603, Tr Acc: 0.6833, Val L: 0.7207, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000502\n",
      " Fold 5 Epoch 41/230: Tr L: 0.5657, Tr Acc: 0.6833, Val L: 0.7113, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000425\n",
      " Fold 5 Epoch 42/230: Tr L: 0.5734, Tr Acc: 0.6833, Val L: 0.7107, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000352\n",
      " Fold 5 Epoch 43/230: Tr L: 0.5718, Tr Acc: 0.6833, Val L: 0.6858, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000284\n",
      " Fold 5 Epoch 44/230: Tr L: 0.5596, Tr Acc: 0.6833, Val L: 0.6775, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000222\n",
      " Fold 5 Epoch 45/230: Tr L: 0.5656, Tr Acc: 0.6833, Val L: 0.6857, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000167\n",
      " Fold 5 Epoch 46/230: Tr L: 0.5676, Tr Acc: 0.6833, Val L: 0.6975, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000119\n",
      " Fold 5 Epoch 47/230: Tr L: 0.5623, Tr Acc: 0.6833, Val L: 0.7068, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000078\n",
      " Fold 5 Epoch 48/230: Tr L: 0.5689, Tr Acc: 0.6833, Val L: 0.7060, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000047\n",
      " Fold 5 Epoch 49/230: Tr L: 0.5593, Tr Acc: 0.6833, Val L: 0.7061, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000024\n",
      " Fold 5 Epoch 50/230: Tr L: 0.5632, Tr Acc: 0.6833, Val L: 0.7063, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000010\n",
      " Fold 5 Epoch 51/230: Tr L: 0.5845, Tr Acc: 0.7000, Val L: 0.6834, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.001489\n",
      " Fold 5 Epoch 52/230: Tr L: 0.5855, Tr Acc: 0.6833, Val L: 0.7362, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.001487\n",
      " Fold 5 Epoch 53/230: Tr L: 0.5892, Tr Acc: 0.7083, Val L: 0.6425, Val Acc: 0.5556, Val F1: 0.3333 lr: 0.001484\n",
      " Fold 5 Epoch 54/230: Tr L: 0.5916, Tr Acc: 0.7250, Val L: 0.6926, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.001478\n",
      " Fold 5 Epoch 55/230: Tr L: 0.5914, Tr Acc: 0.6833, Val L: 0.9301, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.001470\n",
      " Fold 5 Epoch 56/230: Tr L: 0.6081, Tr Acc: 0.6833, Val L: 0.6666, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.001460\n",
      " Fold 5 Epoch 57/230: Tr L: 0.6479, Tr Acc: 0.5333, Val L: 0.6315, Val Acc: 0.6111, Val F1: 0.0000 lr: 0.001447\n",
      " Fold 5 Epoch 58/230: Tr L: 0.6508, Tr Acc: 0.4833, Val L: 0.6610, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.001432\n",
      " Fold 5 Epoch 59/230: Tr L: 0.5552, Tr Acc: 0.6833, Val L: 0.7222, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.001415\n",
      " Fold 5 Epoch 60/230: Tr L: 0.5741, Tr Acc: 0.6833, Val L: 0.7137, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.001396\n",
      " Fold 5 Epoch 61/230: Tr L: 0.5832, Tr Acc: 0.6500, Val L: 0.6335, Val Acc: 0.6111, Val F1: 0.0000 lr: 0.001375\n",
      " Fold 5 Epoch 62/230: Tr L: 0.5764, Tr Acc: 0.6583, Val L: 0.7052, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.001352\n",
      " Fold 5 Epoch 63/230: Tr L: 0.6032, Tr Acc: 0.6917, Val L: 0.7672, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.001327\n",
      " Fold 5 Epoch 64/230: Tr L: 0.5830, Tr Acc: 0.6833, Val L: 0.6447, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.001300\n",
      " Fold 5 Epoch 65/230: Tr L: 0.5792, Tr Acc: 0.6917, Val L: 0.7054, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.001271\n",
      " Fold 5 Epoch 66/230: Tr L: 0.5761, Tr Acc: 0.6917, Val L: 0.6917, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.001241\n",
      " Fold 5 Epoch 67/230: Tr L: 0.5986, Tr Acc: 0.6833, Val L: 0.8266, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.001209\n",
      " Fold 5 Epoch 68/230: Tr L: 0.5856, Tr Acc: 0.6833, Val L: 0.6969, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.001176\n",
      " Fold 5 Epoch 69/230: Tr L: 0.5639, Tr Acc: 0.6917, Val L: 0.6779, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.001141\n",
      " Fold 5 Epoch 70/230: Tr L: 0.5657, Tr Acc: 0.6833, Val L: 0.7621, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.001106\n",
      " Fold 5 Epoch 71/230: Tr L: 0.5636, Tr Acc: 0.6833, Val L: 0.6716, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.001069\n",
      " Fold 5 Epoch 72/230: Tr L: 0.5688, Tr Acc: 0.7083, Val L: 0.6454, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.001031\n",
      " Fold 5 Epoch 73/230: Tr L: 0.5643, Tr Acc: 0.7000, Val L: 0.7373, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000992\n",
      " Fold 5 Epoch 74/230: Tr L: 0.5606, Tr Acc: 0.7083, Val L: 0.7478, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000952\n",
      " Fold 5 Epoch 75/230: Tr L: 0.5724, Tr Acc: 0.7083, Val L: 0.7197, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000912\n",
      " Fold 5 Epoch 76/230: Tr L: 0.5572, Tr Acc: 0.6917, Val L: 0.6558, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000871\n",
      " Fold 5 Epoch 77/230: Tr L: 0.5689, Tr Acc: 0.7167, Val L: 0.6636, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000830\n",
      " Fold 5 Epoch 78/230: Tr L: 0.5535, Tr Acc: 0.7000, Val L: 0.7533, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000788\n",
      " Fold 5 Epoch 79/230: Tr L: 0.5986, Tr Acc: 0.6833, Val L: 0.8395, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000747\n",
      " Fold 5 Epoch 80/230: Tr L: 0.5823, Tr Acc: 0.7000, Val L: 0.6960, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000705\n",
      " Fold 5 Epoch 81/230: Tr L: 0.5573, Tr Acc: 0.6833, Val L: 0.6371, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000664\n",
      " Fold 5 Epoch 82/230: Tr L: 0.5725, Tr Acc: 0.7250, Val L: 0.6532, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000623\n",
      " Fold 5 Epoch 83/230: Tr L: 0.5625, Tr Acc: 0.6833, Val L: 0.6837, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000582\n",
      " Fold 5 Epoch 84/230: Tr L: 0.5579, Tr Acc: 0.6833, Val L: 0.6753, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000541\n",
      " Fold 5 Epoch 85/230: Tr L: 0.5596, Tr Acc: 0.6833, Val L: 0.6820, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000502\n",
      " Fold 5 Epoch 86/230: Tr L: 0.5564, Tr Acc: 0.6917, Val L: 0.6835, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000463\n",
      " Fold 5 Epoch 87/230: Tr L: 0.5506, Tr Acc: 0.6917, Val L: 0.6844, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000425\n",
      " Fold 5 Epoch 88/230: Tr L: 0.5627, Tr Acc: 0.6917, Val L: 0.6880, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000388\n",
      " Fold 5 Epoch 89/230: Tr L: 0.5521, Tr Acc: 0.6833, Val L: 0.7073, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000352\n",
      " Fold 5 Epoch 90/230: Tr L: 0.5548, Tr Acc: 0.6833, Val L: 0.7049, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000318\n",
      " Fold 5 Epoch 91/230: Tr L: 0.5532, Tr Acc: 0.6833, Val L: 0.7034, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000284\n",
      " Fold 5 Epoch 92/230: Tr L: 0.5579, Tr Acc: 0.6833, Val L: 0.6959, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000253\n",
      " Fold 5 Epoch 93/230: Tr L: 0.5588, Tr Acc: 0.6833, Val L: 0.7002, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000222\n",
      " Fold 5 Epoch 94/230: Tr L: 0.5653, Tr Acc: 0.6833, Val L: 0.7060, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000194\n",
      " Fold 5 Epoch 95/230: Tr L: 0.5578, Tr Acc: 0.6833, Val L: 0.6797, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000167\n",
      " Fold 5 Epoch 96/230: Tr L: 0.5546, Tr Acc: 0.6833, Val L: 0.6709, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000142\n",
      " Fold 5 Epoch 97/230: Tr L: 0.5598, Tr Acc: 0.6833, Val L: 0.6823, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000119\n",
      "Early stopping triggered at epoch 97 for fold 5\n",
      "--- Evaluating Fold 5 on Outer Test Set ---\n",
      "Warning: model_factory used without specific LR, using default/cfg LR: 0.001\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.001\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "Test set class counts for fold 5: {0: 12, 1: 9}\n",
      "percentage of classes in test set: 0    0.571429\n",
      "1    0.428571\n",
      "Name: count, dtype: float64\n",
      " [FOLD 5 FINAL] Test Loss: 0.6894 | Test Acc: 0.5714 | test Balanced Acc: 0.5000 | test F1: 0.0000 | Test AUC: 1.0000\n",
      "model class name: ViTFinetuneModule\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "[I 2025-05-29 19:28:45,928] A new study created in memory with name: no-name-660ece40-4192-4fe0-95e4-3be5351180fb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== OUTER FOLD 6 / 6 =====\n",
      "Outer Train images: 118 | Outer Test images: 22\n",
      "--- Calculating normalization stats for Fold 6 Training Data ---\n",
      "Fold 6 stats: {'mean': [0.028899148106575012, 0.01048550009727478, 0.07865447551012039], 'std': [0.05506647750735283, 0.015474601648747921, 0.08890823274850845]}\n",
      "--- Generating data transforms for Fold 6 ---\n",
      "Using the train and val transform passed as arguments\n",
      "--- Starting Hyperparameter Tuning for Fold 6 ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=4.715696678089837e-05\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=4.715696678089837e-05\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=4.715696678089837e-05\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=4.715696678089837e-05\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 19:28:51,006] Trial 0 finished with value: 0.7272355556488037 and parameters: {'lr': 4.715696678089837e-05}. Best is trial 0 with value: 0.7272355556488037.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0014886262201211794\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0014886262201211794\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0014886262201211794\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0014886262201211794\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 19:28:56,116] Trial 1 finished with value: 0.7972967177629471 and parameters: {'lr': 0.0014886262201211794}. Best is trial 0 with value: 0.7272355556488037.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0004014783718209777\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0004014783718209777\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0004014783718209777\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0004014783718209777\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 19:29:01,122] Trial 2 finished with value: 0.7098043411970139 and parameters: {'lr': 0.0004014783718209777}. Best is trial 2 with value: 0.7098043411970139.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Best LR from inner CV = 0.000401\n",
      "--- Starting Final Model Training for Fold 6 with LR=0.000401 ---\n",
      "X_train_es: (100,) | X_val_es: (18,)\n",
      "Early stopping split: Train images: 100, Validation images: 18\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0004014783718209777\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "===========================\n",
      "Model Architecture:\n",
      "==================\n",
      "Total parameters: 87,456,770\n",
      "Trainable parameters: 1,538\n",
      "Non-trainable parameters: 87,455,232\n",
      "===========================\n",
      "Using CosineAnnealingWarmRestarts scheduler\n",
      " Fold 6 Epoch 1/230: Tr L: 0.7287, Tr Acc: 0.4831, Val L: 0.7732, Val Acc: 0.4444, Val F1: 0.5833 lr: 0.000401\n",
      " Fold 6 Epoch 2/230: Tr L: 0.7010, Tr Acc: 0.6102, Val L: 0.7887, Val Acc: 0.4444, Val F1: 0.5455 lr: 0.000401\n",
      " Fold 6 Epoch 3/230: Tr L: 0.6787, Tr Acc: 0.5932, Val L: 0.7094, Val Acc: 0.3889, Val F1: 0.2667 lr: 0.000382\n",
      " Fold 6 Epoch 4/230: Tr L: 0.7164, Tr Acc: 0.5000, Val L: 0.6897, Val Acc: 0.6111, Val F1: 0.0000 lr: 0.000327\n",
      " Fold 6 Epoch 5/230: Tr L: 0.6925, Tr Acc: 0.5000, Val L: 0.6889, Val Acc: 0.4444, Val F1: 0.1667 lr: 0.000247\n",
      " Fold 6 Epoch 6/230: Tr L: 0.6711, Tr Acc: 0.5847, Val L: 0.7114, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000159\n",
      " Fold 6 Epoch 7/230: Tr L: 0.6665, Tr Acc: 0.5847, Val L: 0.7253, Val Acc: 0.4444, Val F1: 0.5455 lr: 0.000080\n",
      " Fold 6 Epoch 8/230: Tr L: 0.6707, Tr Acc: 0.5763, Val L: 0.7276, Val Acc: 0.4444, Val F1: 0.5455 lr: 0.000025\n",
      " Fold 6 Epoch 9/230: Tr L: 0.6625, Tr Acc: 0.5847, Val L: 0.7008, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000401\n",
      " Fold 6 Epoch 10/230: Tr L: 0.6649, Tr Acc: 0.6017, Val L: 0.6872, Val Acc: 0.3889, Val F1: 0.4211 lr: 0.000397\n",
      " Fold 6 Epoch 11/230: Tr L: 0.6799, Tr Acc: 0.5847, Val L: 0.7331, Val Acc: 0.4444, Val F1: 0.5455 lr: 0.000382\n",
      " Fold 6 Epoch 12/230: Tr L: 0.6562, Tr Acc: 0.5763, Val L: 0.7273, Val Acc: 0.4444, Val F1: 0.5455 lr: 0.000358\n",
      " Fold 6 Epoch 13/230: Tr L: 0.6614, Tr Acc: 0.5932, Val L: 0.7098, Val Acc: 0.4444, Val F1: 0.5000 lr: 0.000327\n",
      " Fold 6 Epoch 14/230: Tr L: 0.6546, Tr Acc: 0.6102, Val L: 0.6875, Val Acc: 0.3889, Val F1: 0.4211 lr: 0.000289\n",
      " Fold 6 Epoch 15/230: Tr L: 0.6541, Tr Acc: 0.5763, Val L: 0.6740, Val Acc: 0.4444, Val F1: 0.3750 lr: 0.000247\n",
      " Fold 6 Epoch 16/230: Tr L: 0.6538, Tr Acc: 0.6102, Val L: 0.6878, Val Acc: 0.4444, Val F1: 0.5455 lr: 0.000203\n",
      " Fold 6 Epoch 17/230: Tr L: 0.6490, Tr Acc: 0.6102, Val L: 0.6994, Val Acc: 0.4444, Val F1: 0.5455 lr: 0.000159\n",
      " Fold 6 Epoch 18/230: Tr L: 0.6481, Tr Acc: 0.6017, Val L: 0.6932, Val Acc: 0.4444, Val F1: 0.5455 lr: 0.000117\n",
      " Fold 6 Epoch 19/230: Tr L: 0.6468, Tr Acc: 0.6017, Val L: 0.6868, Val Acc: 0.4444, Val F1: 0.5455 lr: 0.000080\n",
      " Fold 6 Epoch 20/230: Tr L: 0.6447, Tr Acc: 0.6186, Val L: 0.6822, Val Acc: 0.4444, Val F1: 0.5000 lr: 0.000048\n",
      " Fold 6 Epoch 21/230: Tr L: 0.6455, Tr Acc: 0.6271, Val L: 0.6810, Val Acc: 0.3889, Val F1: 0.4211 lr: 0.000025\n",
      " Fold 6 Epoch 22/230: Tr L: 0.6431, Tr Acc: 0.6271, Val L: 0.6817, Val Acc: 0.4444, Val F1: 0.5000 lr: 0.000010\n",
      " Fold 6 Epoch 23/230: Tr L: 0.6423, Tr Acc: 0.6186, Val L: 0.7058, Val Acc: 0.4444, Val F1: 0.5455 lr: 0.000401\n",
      " Fold 6 Epoch 24/230: Tr L: 0.6399, Tr Acc: 0.5763, Val L: 0.7177, Val Acc: 0.4444, Val F1: 0.5455 lr: 0.000400\n",
      " Fold 6 Epoch 25/230: Tr L: 0.6404, Tr Acc: 0.5847, Val L: 0.7044, Val Acc: 0.4444, Val F1: 0.5455 lr: 0.000397\n",
      " Fold 6 Epoch 26/230: Tr L: 0.6485, Tr Acc: 0.5847, Val L: 0.7152, Val Acc: 0.5000, Val F1: 0.6087 lr: 0.000390\n",
      " Fold 6 Epoch 27/230: Tr L: 0.6333, Tr Acc: 0.5932, Val L: 0.6608, Val Acc: 0.3889, Val F1: 0.4762 lr: 0.000382\n",
      " Fold 6 Epoch 28/230: Tr L: 0.6474, Tr Acc: 0.5508, Val L: 0.6468, Val Acc: 0.7222, Val F1: 0.4444 lr: 0.000371\n",
      " Fold 6 Epoch 29/230: Tr L: 0.6298, Tr Acc: 0.6186, Val L: 0.6737, Val Acc: 0.4444, Val F1: 0.5455 lr: 0.000358\n",
      " Fold 6 Epoch 30/230: Tr L: 0.6220, Tr Acc: 0.6356, Val L: 0.6763, Val Acc: 0.4444, Val F1: 0.5455 lr: 0.000343\n",
      " Fold 6 Epoch 31/230: Tr L: 0.6305, Tr Acc: 0.5932, Val L: 0.7084, Val Acc: 0.5000, Val F1: 0.6087 lr: 0.000327\n",
      " Fold 6 Epoch 32/230: Tr L: 0.6231, Tr Acc: 0.5932, Val L: 0.7003, Val Acc: 0.5000, Val F1: 0.6087 lr: 0.000309\n",
      " Fold 6 Epoch 33/230: Tr L: 0.6217, Tr Acc: 0.5932, Val L: 0.6764, Val Acc: 0.4444, Val F1: 0.5455 lr: 0.000289\n",
      " Fold 6 Epoch 34/230: Tr L: 0.6292, Tr Acc: 0.5932, Val L: 0.6871, Val Acc: 0.5000, Val F1: 0.6087 lr: 0.000269\n",
      " Fold 6 Epoch 35/230: Tr L: 0.6206, Tr Acc: 0.6017, Val L: 0.6788, Val Acc: 0.5000, Val F1: 0.6087 lr: 0.000247\n",
      " Fold 6 Epoch 36/230: Tr L: 0.6171, Tr Acc: 0.6271, Val L: 0.6720, Val Acc: 0.4444, Val F1: 0.5455 lr: 0.000225\n",
      " Fold 6 Epoch 37/230: Tr L: 0.6152, Tr Acc: 0.6271, Val L: 0.6740, Val Acc: 0.5000, Val F1: 0.6087 lr: 0.000203\n",
      " Fold 6 Epoch 38/230: Tr L: 0.6216, Tr Acc: 0.6271, Val L: 0.6785, Val Acc: 0.5000, Val F1: 0.6087 lr: 0.000181\n",
      " Fold 6 Epoch 39/230: Tr L: 0.6182, Tr Acc: 0.6271, Val L: 0.6745, Val Acc: 0.5000, Val F1: 0.6087 lr: 0.000159\n",
      " Fold 6 Epoch 40/230: Tr L: 0.6139, Tr Acc: 0.6271, Val L: 0.6750, Val Acc: 0.5000, Val F1: 0.6087 lr: 0.000138\n",
      " Fold 6 Epoch 41/230: Tr L: 0.6164, Tr Acc: 0.6271, Val L: 0.6788, Val Acc: 0.5000, Val F1: 0.6087 lr: 0.000117\n",
      " Fold 6 Epoch 42/230: Tr L: 0.6103, Tr Acc: 0.6271, Val L: 0.6699, Val Acc: 0.4444, Val F1: 0.5455 lr: 0.000098\n",
      " Fold 6 Epoch 43/230: Tr L: 0.6110, Tr Acc: 0.6271, Val L: 0.6728, Val Acc: 0.5000, Val F1: 0.6087 lr: 0.000080\n",
      " Fold 6 Epoch 44/230: Tr L: 0.6129, Tr Acc: 0.6271, Val L: 0.6797, Val Acc: 0.5000, Val F1: 0.6087 lr: 0.000063\n",
      " Fold 6 Epoch 45/230: Tr L: 0.6169, Tr Acc: 0.6271, Val L: 0.6784, Val Acc: 0.5000, Val F1: 0.6087 lr: 0.000048\n",
      " Fold 6 Epoch 46/230: Tr L: 0.6226, Tr Acc: 0.6271, Val L: 0.6744, Val Acc: 0.5000, Val F1: 0.6087 lr: 0.000035\n",
      " Fold 6 Epoch 47/230: Tr L: 0.6177, Tr Acc: 0.6271, Val L: 0.6736, Val Acc: 0.5000, Val F1: 0.6087 lr: 0.000025\n",
      " Fold 6 Epoch 48/230: Tr L: 0.6142, Tr Acc: 0.6271, Val L: 0.6718, Val Acc: 0.4444, Val F1: 0.5455 lr: 0.000016\n",
      " Fold 6 Epoch 49/230: Tr L: 0.6100, Tr Acc: 0.6356, Val L: 0.6709, Val Acc: 0.4444, Val F1: 0.5455 lr: 0.000010\n",
      " Fold 6 Epoch 50/230: Tr L: 0.6148, Tr Acc: 0.6356, Val L: 0.6707, Val Acc: 0.4444, Val F1: 0.5455 lr: 0.000006\n",
      " Fold 6 Epoch 51/230: Tr L: 0.6243, Tr Acc: 0.6271, Val L: 0.6722, Val Acc: 0.4444, Val F1: 0.5455 lr: 0.000401\n",
      " Fold 6 Epoch 52/230: Tr L: 0.6126, Tr Acc: 0.6271, Val L: 0.6648, Val Acc: 0.4444, Val F1: 0.5455 lr: 0.000401\n",
      " Fold 6 Epoch 53/230: Tr L: 0.6100, Tr Acc: 0.6271, Val L: 0.6561, Val Acc: 0.3889, Val F1: 0.4762 lr: 0.000400\n",
      " Fold 6 Epoch 54/230: Tr L: 0.6135, Tr Acc: 0.6271, Val L: 0.6552, Val Acc: 0.3889, Val F1: 0.4762 lr: 0.000399\n",
      " Fold 6 Epoch 55/230: Tr L: 0.6069, Tr Acc: 0.6271, Val L: 0.6756, Val Acc: 0.5000, Val F1: 0.6087 lr: 0.000397\n",
      " Fold 6 Epoch 56/230: Tr L: 0.6097, Tr Acc: 0.6271, Val L: 0.6533, Val Acc: 0.3889, Val F1: 0.4762 lr: 0.000394\n",
      " Fold 6 Epoch 57/230: Tr L: 0.6036, Tr Acc: 0.6186, Val L: 0.6533, Val Acc: 0.3889, Val F1: 0.4762 lr: 0.000390\n",
      " Fold 6 Epoch 58/230: Tr L: 0.6001, Tr Acc: 0.6271, Val L: 0.6833, Val Acc: 0.5000, Val F1: 0.6087 lr: 0.000386\n",
      " Fold 6 Epoch 59/230: Tr L: 0.6077, Tr Acc: 0.6186, Val L: 0.7099, Val Acc: 0.5000, Val F1: 0.6087 lr: 0.000382\n",
      " Fold 6 Epoch 60/230: Tr L: 0.6124, Tr Acc: 0.6017, Val L: 0.6672, Val Acc: 0.4444, Val F1: 0.5455 lr: 0.000377\n",
      " Fold 6 Epoch 61/230: Tr L: 0.6017, Tr Acc: 0.5932, Val L: 0.6545, Val Acc: 0.3889, Val F1: 0.4762 lr: 0.000371\n",
      " Fold 6 Epoch 62/230: Tr L: 0.6001, Tr Acc: 0.6186, Val L: 0.6582, Val Acc: 0.4444, Val F1: 0.5455 lr: 0.000365\n",
      " Fold 6 Epoch 63/230: Tr L: 0.6030, Tr Acc: 0.6102, Val L: 0.6382, Val Acc: 0.4444, Val F1: 0.4444 lr: 0.000358\n",
      " Fold 6 Epoch 64/230: Tr L: 0.5965, Tr Acc: 0.6271, Val L: 0.6530, Val Acc: 0.3889, Val F1: 0.4762 lr: 0.000351\n",
      " Fold 6 Epoch 65/230: Tr L: 0.5936, Tr Acc: 0.6271, Val L: 0.6657, Val Acc: 0.4444, Val F1: 0.5455 lr: 0.000343\n",
      " Fold 6 Epoch 66/230: Tr L: 0.6049, Tr Acc: 0.6356, Val L: 0.6726, Val Acc: 0.5000, Val F1: 0.6087 lr: 0.000335\n",
      " Fold 6 Epoch 67/230: Tr L: 0.5935, Tr Acc: 0.6271, Val L: 0.6454, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000327\n",
      " Fold 6 Epoch 68/230: Tr L: 0.6046, Tr Acc: 0.6525, Val L: 0.6253, Val Acc: 0.6667, Val F1: 0.5714 lr: 0.000318\n",
      " Fold 6 Epoch 69/230: Tr L: 0.6039, Tr Acc: 0.6017, Val L: 0.6312, Val Acc: 0.5556, Val F1: 0.5000 lr: 0.000309\n",
      " Fold 6 Epoch 70/230: Tr L: 0.5952, Tr Acc: 0.6186, Val L: 0.6444, Val Acc: 0.3889, Val F1: 0.4762 lr: 0.000299\n",
      " Fold 6 Epoch 71/230: Tr L: 0.5890, Tr Acc: 0.6356, Val L: 0.6740, Val Acc: 0.5000, Val F1: 0.6087 lr: 0.000289\n",
      " Fold 6 Epoch 72/230: Tr L: 0.5901, Tr Acc: 0.6695, Val L: 0.6719, Val Acc: 0.5000, Val F1: 0.6087 lr: 0.000279\n",
      " Fold 6 Epoch 73/230: Tr L: 0.5939, Tr Acc: 0.6695, Val L: 0.6657, Val Acc: 0.4444, Val F1: 0.5455 lr: 0.000269\n",
      " Fold 6 Epoch 74/230: Tr L: 0.5960, Tr Acc: 0.6780, Val L: 0.6876, Val Acc: 0.5000, Val F1: 0.6087 lr: 0.000258\n",
      " Fold 6 Epoch 75/230: Tr L: 0.5937, Tr Acc: 0.6695, Val L: 0.6830, Val Acc: 0.5000, Val F1: 0.6087 lr: 0.000247\n",
      " Fold 6 Epoch 76/230: Tr L: 0.5928, Tr Acc: 0.6610, Val L: 0.6527, Val Acc: 0.3889, Val F1: 0.4762 lr: 0.000236\n",
      " Fold 6 Epoch 77/230: Tr L: 0.5850, Tr Acc: 0.6271, Val L: 0.6405, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000225\n",
      " Fold 6 Epoch 78/230: Tr L: 0.5954, Tr Acc: 0.6186, Val L: 0.6354, Val Acc: 0.4444, Val F1: 0.4444 lr: 0.000214\n",
      " Fold 6 Epoch 79/230: Tr L: 0.5884, Tr Acc: 0.6186, Val L: 0.6535, Val Acc: 0.3889, Val F1: 0.4762 lr: 0.000203\n",
      " Fold 6 Epoch 80/230: Tr L: 0.5929, Tr Acc: 0.6186, Val L: 0.6467, Val Acc: 0.3889, Val F1: 0.4762 lr: 0.000192\n",
      " Fold 6 Epoch 81/230: Tr L: 0.5876, Tr Acc: 0.6271, Val L: 0.6714, Val Acc: 0.4444, Val F1: 0.5455 lr: 0.000181\n",
      " Fold 6 Epoch 82/230: Tr L: 0.5873, Tr Acc: 0.6441, Val L: 0.6764, Val Acc: 0.5000, Val F1: 0.6087 lr: 0.000170\n",
      " Fold 6 Epoch 83/230: Tr L: 0.5761, Tr Acc: 0.6610, Val L: 0.6629, Val Acc: 0.4444, Val F1: 0.5455 lr: 0.000159\n",
      " Fold 6 Epoch 84/230: Tr L: 0.5887, Tr Acc: 0.6356, Val L: 0.6538, Val Acc: 0.4444, Val F1: 0.5455 lr: 0.000148\n",
      " Fold 6 Epoch 85/230: Tr L: 0.5800, Tr Acc: 0.6356, Val L: 0.6434, Val Acc: 0.3889, Val F1: 0.4762 lr: 0.000138\n",
      " Fold 6 Epoch 86/230: Tr L: 0.5873, Tr Acc: 0.6356, Val L: 0.6343, Val Acc: 0.4444, Val F1: 0.4444 lr: 0.000127\n",
      " Fold 6 Epoch 87/230: Tr L: 0.5806, Tr Acc: 0.6271, Val L: 0.6363, Val Acc: 0.4444, Val F1: 0.4444 lr: 0.000117\n",
      " Fold 6 Epoch 88/230: Tr L: 0.5838, Tr Acc: 0.6441, Val L: 0.6431, Val Acc: 0.3889, Val F1: 0.4762 lr: 0.000107\n",
      " Fold 6 Epoch 89/230: Tr L: 0.5808, Tr Acc: 0.6271, Val L: 0.6504, Val Acc: 0.3889, Val F1: 0.4762 lr: 0.000098\n",
      " Fold 6 Epoch 90/230: Tr L: 0.5876, Tr Acc: 0.6186, Val L: 0.6594, Val Acc: 0.4444, Val F1: 0.5455 lr: 0.000089\n",
      " Fold 6 Epoch 91/230: Tr L: 0.5827, Tr Acc: 0.6441, Val L: 0.6701, Val Acc: 0.4444, Val F1: 0.5455 lr: 0.000080\n",
      " Fold 6 Epoch 92/230: Tr L: 0.5839, Tr Acc: 0.6441, Val L: 0.6683, Val Acc: 0.4444, Val F1: 0.5455 lr: 0.000071\n",
      " Fold 6 Epoch 93/230: Tr L: 0.5808, Tr Acc: 0.6441, Val L: 0.6635, Val Acc: 0.4444, Val F1: 0.5455 lr: 0.000063\n",
      " Fold 6 Epoch 94/230: Tr L: 0.5839, Tr Acc: 0.6356, Val L: 0.6574, Val Acc: 0.4444, Val F1: 0.5455 lr: 0.000055\n",
      " Fold 6 Epoch 95/230: Tr L: 0.5816, Tr Acc: 0.6271, Val L: 0.6543, Val Acc: 0.4444, Val F1: 0.5455 lr: 0.000048\n",
      " Fold 6 Epoch 96/230: Tr L: 0.5797, Tr Acc: 0.6271, Val L: 0.6506, Val Acc: 0.3889, Val F1: 0.4762 lr: 0.000042\n",
      " Fold 6 Epoch 97/230: Tr L: 0.5805, Tr Acc: 0.6271, Val L: 0.6499, Val Acc: 0.3889, Val F1: 0.4762 lr: 0.000035\n",
      " Fold 6 Epoch 98/230: Tr L: 0.5808, Tr Acc: 0.6186, Val L: 0.6486, Val Acc: 0.3889, Val F1: 0.4762 lr: 0.000030\n",
      " Fold 6 Epoch 99/230: Tr L: 0.5765, Tr Acc: 0.6271, Val L: 0.6460, Val Acc: 0.3889, Val F1: 0.4762 lr: 0.000025\n",
      " Fold 6 Epoch 100/230: Tr L: 0.5792, Tr Acc: 0.6525, Val L: 0.6464, Val Acc: 0.3889, Val F1: 0.4762 lr: 0.000020\n",
      " Fold 6 Epoch 101/230: Tr L: 0.5745, Tr Acc: 0.6525, Val L: 0.6456, Val Acc: 0.3889, Val F1: 0.4762 lr: 0.000016\n",
      " Fold 6 Epoch 102/230: Tr L: 0.5761, Tr Acc: 0.6525, Val L: 0.6452, Val Acc: 0.3889, Val F1: 0.4762 lr: 0.000013\n",
      " Fold 6 Epoch 103/230: Tr L: 0.5834, Tr Acc: 0.6525, Val L: 0.6454, Val Acc: 0.3889, Val F1: 0.4762 lr: 0.000010\n",
      " Fold 6 Epoch 104/230: Tr L: 0.5849, Tr Acc: 0.6525, Val L: 0.6454, Val Acc: 0.3889, Val F1: 0.4762 lr: 0.000008\n",
      " Fold 6 Epoch 105/230: Tr L: 0.5862, Tr Acc: 0.6525, Val L: 0.6453, Val Acc: 0.3889, Val F1: 0.4762 lr: 0.000006\n",
      " Fold 6 Epoch 106/230: Tr L: 0.5775, Tr Acc: 0.6525, Val L: 0.6456, Val Acc: 0.3889, Val F1: 0.4762 lr: 0.000005\n",
      " Fold 6 Epoch 107/230: Tr L: 0.5833, Tr Acc: 0.6271, Val L: 0.6679, Val Acc: 0.4444, Val F1: 0.5455 lr: 0.000401\n",
      " Fold 6 Epoch 108/230: Tr L: 0.5814, Tr Acc: 0.6695, Val L: 0.6730, Val Acc: 0.5000, Val F1: 0.6087 lr: 0.000401\n",
      "Early stopping triggered at epoch 108 for fold 6\n",
      "--- Evaluating Fold 6 on Outer Test Set ---\n",
      "Warning: model_factory used without specific LR, using default/cfg LR: 0.001\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.001\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "Test set class counts for fold 6: {0: 13, 1: 9}\n",
      "percentage of classes in test set: 0    0.590909\n",
      "1    0.409091\n",
      "Name: count, dtype: float64\n",
      " [FOLD 6 FINAL] Test Loss: 0.7956 | Test Acc: 0.4091 | test Balanced Acc: 0.3803 | test F1: 0.2353 | Test AUC: 1.0000\n",
      "model class name: ViTFinetuneModule\n",
      "\n",
      "-------------------------------------------------\n",
      "Cross-validation results (outer folds):\n",
      "  Fold 1: Test Loss=0.7504, Acc=0.4400, F1=0.4167, Bal Acc=0.4653, AUC=1.0000 (Best LR=0.000401)\n",
      "  Fold 2: Test Loss=0.6267, Acc=0.3846, F1=0.1111, Bal Acc=0.3631, AUC=1.0000 (Best LR=0.000401)\n",
      "  Fold 3: Test Loss=0.5484, Acc=0.5652, F1=0.0000, Bal Acc=0.4643, AUC=1.0000 (Best LR=0.000401)\n",
      "  Fold 4: Test Loss=0.6347, Acc=0.5652, F1=0.1667, Bal Acc=0.4841, AUC=1.0000 (Best LR=0.001489)\n",
      "  Fold 5: Test Loss=0.6894, Acc=0.5714, F1=0.0000, Bal Acc=0.5000, AUC=1.0000 (Best LR=0.001489)\n",
      "  Fold 6: Test Loss=0.7956, Acc=0.4091, F1=0.2353, Bal Acc=0.3803, AUC=1.0000 (Best LR=0.000401)\n",
      "\n",
      "--- Aggregate Results ---\n",
      "Avg Test Accuracy: 0.4893 +/- 0.0797\n",
      "Avg Test F1-Score: 0.1550 +/- 0.1444\n",
      "Avg Test Balanced Acc: 0.4429 +/- 0.0520\n",
      "Avg Test Precision: 0.1806 +/- 0.1396\n",
      "Avg Test Recall: 0.1620 +/- 0.1914\n",
      "-------------------------------------------------\n",
      "[{'fold': 1, 'test_loss': 0.7504040002822876, 'test_acc': 0.44, 'test_f1': 0.4166666666666667, 'test_balanced_acc': 0.4652777777777778, 'test_auc': 1, 'test_precision': 0.3333333333333333, 'test_recall': 0.5555555555555556, 'best_lr': 0.0004014783718209777}, {'fold': 2, 'test_loss': 0.6266624927520752, 'test_acc': 0.38461538461538464, 'test_f1': 0.1111111111111111, 'test_balanced_acc': 0.36309523809523814, 'test_auc': 1, 'test_precision': 0.16666666666666666, 'test_recall': 0.08333333333333333, 'best_lr': 0.0004014783718209777}, {'fold': 3, 'test_loss': 0.5484372973442078, 'test_acc': 0.5652173913043478, 'test_f1': 0.0, 'test_balanced_acc': 0.4642857142857143, 'test_auc': 1, 'test_precision': 0.0, 'test_recall': 0.0, 'best_lr': 0.0004014783718209777}, {'fold': 4, 'test_loss': 0.6347490549087524, 'test_acc': 0.5652173913043478, 'test_f1': 0.16666666666666666, 'test_balanced_acc': 0.4841269841269841, 'test_auc': 1, 'test_precision': 0.3333333333333333, 'test_recall': 0.1111111111111111, 'best_lr': 0.0014886262201211794}, {'fold': 5, 'test_loss': 0.6894221901893616, 'test_acc': 0.5714285714285714, 'test_f1': 0.0, 'test_balanced_acc': 0.5, 'test_auc': 1, 'test_precision': 0.0, 'test_recall': 0.0, 'best_lr': 0.0014886262201211794}, {'fold': 6, 'test_loss': 0.7956209778785706, 'test_acc': 0.4090909090909091, 'test_f1': 0.23529411764705882, 'test_balanced_acc': 0.3803418803418803, 'test_auc': 1, 'test_precision': 0.25, 'test_recall': 0.2222222222222222, 'best_lr': 0.0004014783718209777}]\n",
      "Best test_balanced_acc Fold Result: {'fold': 5, 'test_loss': 0.6894221901893616, 'test_acc': 0.5714285714285714, 'test_f1': 0.0, 'test_balanced_acc': 0.5, 'test_auc': 1, 'test_precision': 0.0, 'test_recall': 0.0, 'best_lr': 0.0014886262201211794}\n",
      "Warning: model_factory used without specific LR, using default/cfg LR: 0.001\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.001\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Malformed experiment '833162958986127285'. Detailed error Yaml file '/home/zano/Documents/TESI/mlruns/833162958986127285/meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/zano/Documents/TESI/TESI/venv/lib/python3.12/site-packages/mlflow/store/tracking/file_store.py\", line 328, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zano/Documents/TESI/TESI/venv/lib/python3.12/site-packages/mlflow/store/tracking/file_store.py\", line 422, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zano/Documents/TESI/TESI/venv/lib/python3.12/site-packages/mlflow/store/tracking/file_store.py\", line 1368, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zano/Documents/TESI/TESI/venv/lib/python3.12/site-packages/mlflow/store/tracking/file_store.py\", line 1361, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zano/Documents/TESI/TESI/venv/lib/python3.12/site-packages/mlflow/utils/file_utils.py\", line 310, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/zano/Documents/TESI/mlruns/833162958986127285/meta.yaml' does not exist.\n",
      "WARNING:root:Malformed experiment '833162958986127285'. Detailed error Yaml file '/home/zano/Documents/TESI/mlruns/833162958986127285/meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/zano/Documents/TESI/TESI/venv/lib/python3.12/site-packages/mlflow/store/tracking/file_store.py\", line 328, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zano/Documents/TESI/TESI/venv/lib/python3.12/site-packages/mlflow/store/tracking/file_store.py\", line 422, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zano/Documents/TESI/TESI/venv/lib/python3.12/site-packages/mlflow/store/tracking/file_store.py\", line 1368, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zano/Documents/TESI/TESI/venv/lib/python3.12/site-packages/mlflow/store/tracking/file_store.py\", line 1361, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zano/Documents/TESI/TESI/venv/lib/python3.12/site-packages/mlflow/utils/file_utils.py\", line 310, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/zano/Documents/TESI/mlruns/833162958986127285/meta.yaml' does not exist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "you are on linux\n",
      "Run name: ViT_oversamp_monai_color_transforms:False_05-29_at:19-29-33\n",
      "Current tracking uri: /home/zano/Documents/TESI/mlruns\n",
      "None\n",
      "Target layer: vit_backbone.patch_embed.proj\n",
      "Target layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2025/05/29 19:29:41 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating GradCAM for test images: cannot access local variable 'test_loader' where it is not associated with a value\n",
      "Processing batch 1, shape: torch.Size([21, 3, 224, 224])\n",
      "Error generating GradCAM with threshold: tuple index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Backward hook for vit_backbone.patch_embed.proj is not triggered; `requires_grad` of vit_backbone.patch_embed.proj should be `True`.\n",
      "[I 2025-05-29 19:29:42,924] A new study created in memory with name: no-name-c4aa2804-f445-471e-97ff-eeee9d0fecc8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory cleared - Before: 1712.23MB -> After: 1712.23MB\n",
      "Peak memory during session: 5245.65MB\n",
      "\n",
      "Starting nested cross-validation...\n",
      "Detected 2 unique classes.\n",
      "\n",
      "===== OUTER FOLD 1 / 6 =====\n",
      "Outer Train images: 115 | Outer Test images: 25\n",
      "--- Calculating normalization stats for Fold 1 Training Data ---\n",
      "Fold 1 stats: {'mean': [0.02819509245455265, 0.010962597094476223, 0.0807294100522995], 'std': [0.053679682314395905, 0.017251143231987953, 0.08971326798200607]}\n",
      "--- Generating data transforms for Fold 1 ---\n",
      "Using the train and val transform passed as arguments\n",
      "--- Starting Hyperparameter Tuning for Fold 1 ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=4.715696678089837e-05\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=4.715696678089837e-05\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=4.715696678089837e-05\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=4.715696678089837e-05\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 19:29:50,816] Trial 0 finished with value: 0.6988713890314102 and parameters: {'lr': 4.715696678089837e-05}. Best is trial 0 with value: 0.6988713890314102.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0014886262201211794\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0014886262201211794\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0014886262201211794\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0014886262201211794\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 19:29:58,414] Trial 1 finished with value: 1.5048176050186157 and parameters: {'lr': 0.0014886262201211794}. Best is trial 0 with value: 0.6988713890314102.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0004014783718209777\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0004014783718209777\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0004014783718209777\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0004014783718209777\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 19:30:06,139] Trial 2 finished with value: 0.9044981598854065 and parameters: {'lr': 0.0004014783718209777}. Best is trial 0 with value: 0.6988713890314102.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Best LR from inner CV = 0.000047\n",
      "--- Starting Final Model Training for Fold 1 with LR=0.000047 ---\n",
      "X_train_es: (97,) | X_val_es: (18,)\n",
      "Early stopping split: Train images: 97, Validation images: 18\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=4.715696678089837e-05\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "===========================\n",
      "Model Architecture:\n",
      "==================\n",
      "Total parameters: 87,456,770\n",
      "Trainable parameters: 87,456,770\n",
      "Non-trainable parameters: 0\n",
      "===========================\n",
      "Using CosineAnnealingWarmRestarts scheduler\n",
      " Fold 1 Epoch 1/230: Tr L: 0.7904, Tr Acc: 0.5000, Val L: 0.6040, Val Acc: 0.6667, Val F1: 0.7273 lr: 0.000047\n",
      " Fold 1 Epoch 2/230: Tr L: 0.5997, Tr Acc: 0.6667, Val L: 0.7326, Val Acc: 0.5556, Val F1: 0.6667 lr: 0.000047\n",
      " Fold 1 Epoch 3/230: Tr L: 0.6114, Tr Acc: 0.6667, Val L: 0.7741, Val Acc: 0.5556, Val F1: 0.6667 lr: 0.000045\n",
      " Fold 1 Epoch 4/230: Tr L: 0.5781, Tr Acc: 0.6754, Val L: 0.6184, Val Acc: 0.6111, Val F1: 0.6957 lr: 0.000039\n",
      " Fold 1 Epoch 5/230: Tr L: 0.6065, Tr Acc: 0.5351, Val L: 0.6131, Val Acc: 0.5556, Val F1: 0.2000 lr: 0.000031\n",
      " Fold 1 Epoch 6/230: Tr L: 0.5687, Tr Acc: 0.6404, Val L: 0.6798, Val Acc: 0.5556, Val F1: 0.6667 lr: 0.000021\n",
      " Fold 1 Epoch 7/230: Tr L: 0.5422, Tr Acc: 0.7105, Val L: 0.6855, Val Acc: 0.5556, Val F1: 0.6667 lr: 0.000013\n",
      " Fold 1 Epoch 8/230: Tr L: 0.5340, Tr Acc: 0.7193, Val L: 0.6629, Val Acc: 0.5556, Val F1: 0.6667 lr: 0.000007\n",
      " Fold 1 Epoch 9/230: Tr L: 0.5239, Tr Acc: 0.7193, Val L: 0.5866, Val Acc: 0.7222, Val F1: 0.7619 lr: 0.000047\n",
      " Fold 1 Epoch 10/230: Tr L: 0.5360, Tr Acc: 0.7368, Val L: 0.6325, Val Acc: 0.6667, Val F1: 0.7273 lr: 0.000047\n",
      " Fold 1 Epoch 11/230: Tr L: 0.4969, Tr Acc: 0.7807, Val L: 0.6149, Val Acc: 0.6667, Val F1: 0.6250 lr: 0.000045\n",
      " Fold 1 Epoch 12/230: Tr L: 0.5117, Tr Acc: 0.7018, Val L: 0.6196, Val Acc: 0.7222, Val F1: 0.7619 lr: 0.000043\n",
      " Fold 1 Epoch 13/230: Tr L: 0.4648, Tr Acc: 0.7807, Val L: 0.6647, Val Acc: 0.6111, Val F1: 0.6957 lr: 0.000039\n",
      " Fold 1 Epoch 14/230: Tr L: 0.4688, Tr Acc: 0.7719, Val L: 0.5616, Val Acc: 0.6111, Val F1: 0.5882 lr: 0.000035\n",
      " Fold 1 Epoch 15/230: Tr L: 0.4962, Tr Acc: 0.6667, Val L: 0.5685, Val Acc: 0.7778, Val F1: 0.7143 lr: 0.000031\n",
      " Fold 1 Epoch 16/230: Tr L: 0.4461, Tr Acc: 0.7018, Val L: 0.6462, Val Acc: 0.6667, Val F1: 0.7273 lr: 0.000026\n",
      " Fold 1 Epoch 17/230: Tr L: 0.4560, Tr Acc: 0.7544, Val L: 0.6924, Val Acc: 0.6111, Val F1: 0.6957 lr: 0.000021\n",
      " Fold 1 Epoch 18/230: Tr L: 0.4225, Tr Acc: 0.7632, Val L: 0.6024, Val Acc: 0.7222, Val F1: 0.7619 lr: 0.000017\n",
      " Fold 1 Epoch 19/230: Tr L: 0.4140, Tr Acc: 0.7105, Val L: 0.5851, Val Acc: 0.6667, Val F1: 0.7000 lr: 0.000013\n",
      " Fold 1 Epoch 20/230: Tr L: 0.4139, Tr Acc: 0.7018, Val L: 0.6090, Val Acc: 0.7222, Val F1: 0.7619 lr: 0.000010\n",
      " Fold 1 Epoch 21/230: Tr L: 0.4069, Tr Acc: 0.8070, Val L: 0.6276, Val Acc: 0.6667, Val F1: 0.7273 lr: 0.000007\n",
      " Fold 1 Epoch 22/230: Tr L: 0.4278, Tr Acc: 0.7807, Val L: 0.6350, Val Acc: 0.6667, Val F1: 0.7273 lr: 0.000006\n",
      " Fold 1 Epoch 23/230: Tr L: 0.3937, Tr Acc: 0.7632, Val L: 0.5792, Val Acc: 0.7222, Val F1: 0.7619 lr: 0.000047\n",
      " Fold 1 Epoch 24/230: Tr L: 0.3790, Tr Acc: 0.8158, Val L: 0.7848, Val Acc: 0.6111, Val F1: 0.6957 lr: 0.000047\n",
      " Fold 1 Epoch 25/230: Tr L: 0.4244, Tr Acc: 0.7719, Val L: 1.0576, Val Acc: 0.6111, Val F1: 0.6957 lr: 0.000047\n",
      " Fold 1 Epoch 26/230: Tr L: 0.4080, Tr Acc: 0.7368, Val L: 0.7106, Val Acc: 0.7222, Val F1: 0.6154 lr: 0.000046\n",
      " Fold 1 Epoch 27/230: Tr L: 0.3979, Tr Acc: 0.7807, Val L: 0.8881, Val Acc: 0.6111, Val F1: 0.6957 lr: 0.000045\n",
      " Fold 1 Epoch 28/230: Tr L: 0.3574, Tr Acc: 0.7807, Val L: 0.7848, Val Acc: 0.7222, Val F1: 0.7059 lr: 0.000044\n",
      " Fold 1 Epoch 29/230: Tr L: 0.3395, Tr Acc: 0.8333, Val L: 0.9627, Val Acc: 0.7222, Val F1: 0.7619 lr: 0.000043\n",
      " Fold 1 Epoch 30/230: Tr L: 0.4635, Tr Acc: 0.7895, Val L: 1.5614, Val Acc: 0.6111, Val F1: 0.6957 lr: 0.000041\n",
      " Fold 1 Epoch 31/230: Tr L: 0.4029, Tr Acc: 0.7807, Val L: 1.1515, Val Acc: 0.6667, Val F1: 0.6667 lr: 0.000039\n",
      " Fold 1 Epoch 32/230: Tr L: 0.3664, Tr Acc: 0.7807, Val L: 1.1668, Val Acc: 0.6111, Val F1: 0.5882 lr: 0.000037\n",
      " Fold 1 Epoch 33/230: Tr L: 0.3042, Tr Acc: 0.8421, Val L: 1.2305, Val Acc: 0.6667, Val F1: 0.7000 lr: 0.000035\n",
      " Fold 1 Epoch 34/230: Tr L: 0.3047, Tr Acc: 0.8333, Val L: 1.1132, Val Acc: 0.6667, Val F1: 0.7000 lr: 0.000033\n",
      " Fold 1 Epoch 35/230: Tr L: 0.2955, Tr Acc: 0.8421, Val L: 0.9896, Val Acc: 0.7778, Val F1: 0.7778 lr: 0.000031\n",
      " Fold 1 Epoch 36/230: Tr L: 0.2838, Tr Acc: 0.8772, Val L: 1.0601, Val Acc: 0.6667, Val F1: 0.7000 lr: 0.000028\n",
      " Fold 1 Epoch 37/230: Tr L: 0.2534, Tr Acc: 0.8684, Val L: 1.1244, Val Acc: 0.7222, Val F1: 0.7368 lr: 0.000026\n",
      " Fold 1 Epoch 38/230: Tr L: 0.2637, Tr Acc: 0.8596, Val L: 1.1569, Val Acc: 0.7222, Val F1: 0.7368 lr: 0.000024\n",
      " Fold 1 Epoch 39/230: Tr L: 0.2547, Tr Acc: 0.9211, Val L: 1.1603, Val Acc: 0.7222, Val F1: 0.7368 lr: 0.000021\n",
      " Fold 1 Epoch 40/230: Tr L: 0.2305, Tr Acc: 0.8772, Val L: 1.2626, Val Acc: 0.6667, Val F1: 0.7000 lr: 0.000019\n",
      " Fold 1 Epoch 41/230: Tr L: 0.2143, Tr Acc: 0.8684, Val L: 1.2229, Val Acc: 0.7222, Val F1: 0.7368 lr: 0.000017\n",
      " Fold 1 Epoch 42/230: Tr L: 0.2442, Tr Acc: 0.8684, Val L: 1.1866, Val Acc: 0.7222, Val F1: 0.7059 lr: 0.000015\n",
      " Fold 1 Epoch 43/230: Tr L: 0.2418, Tr Acc: 0.9123, Val L: 1.4097, Val Acc: 0.6667, Val F1: 0.7000 lr: 0.000013\n",
      " Fold 1 Epoch 44/230: Tr L: 0.2148, Tr Acc: 0.9123, Val L: 1.2855, Val Acc: 0.7222, Val F1: 0.7368 lr: 0.000011\n",
      " Fold 1 Epoch 45/230: Tr L: 0.1969, Tr Acc: 0.9211, Val L: 1.2232, Val Acc: 0.7222, Val F1: 0.7368 lr: 0.000010\n",
      " Fold 1 Epoch 46/230: Tr L: 0.1879, Tr Acc: 0.9211, Val L: 1.2603, Val Acc: 0.7222, Val F1: 0.7368 lr: 0.000008\n",
      " Fold 1 Epoch 47/230: Tr L: 0.1995, Tr Acc: 0.9211, Val L: 1.2560, Val Acc: 0.6667, Val F1: 0.7000 lr: 0.000007\n",
      " Fold 1 Epoch 48/230: Tr L: 0.1765, Tr Acc: 0.9211, Val L: 1.2891, Val Acc: 0.6667, Val F1: 0.7000 lr: 0.000006\n",
      " Fold 1 Epoch 49/230: Tr L: 0.1761, Tr Acc: 0.9211, Val L: 1.2632, Val Acc: 0.7222, Val F1: 0.7368 lr: 0.000006\n",
      " Fold 1 Epoch 50/230: Tr L: 0.1692, Tr Acc: 0.9561, Val L: 1.2607, Val Acc: 0.7222, Val F1: 0.7368 lr: 0.000005\n",
      " Fold 1 Epoch 51/230: Tr L: 0.2283, Tr Acc: 0.9474, Val L: 1.5011, Val Acc: 0.6667, Val F1: 0.7000 lr: 0.000047\n",
      " Fold 1 Epoch 52/230: Tr L: 0.1796, Tr Acc: 0.9298, Val L: 1.5234, Val Acc: 0.7222, Val F1: 0.7619 lr: 0.000047\n",
      " Fold 1 Epoch 53/230: Tr L: 0.1530, Tr Acc: 0.9649, Val L: 1.2022, Val Acc: 0.7778, Val F1: 0.8000 lr: 0.000047\n",
      " Fold 1 Epoch 54/230: Tr L: 0.1163, Tr Acc: 0.9561, Val L: 1.3051, Val Acc: 0.7222, Val F1: 0.7619 lr: 0.000047\n",
      "Early stopping triggered at epoch 54 for fold 1\n",
      "--- Evaluating Fold 1 on Outer Test Set ---\n",
      "Warning: model_factory used without specific LR, using default/cfg LR: 0.001\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.001\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "Test set class counts for fold 1: {0: 16, 1: 9}\n",
      "percentage of classes in test set: 0    0.64\n",
      "1    0.36\n",
      "Name: count, dtype: float64\n",
      " [FOLD 1 FINAL] Test Loss: 0.8673 | Test Acc: 0.6400 | test Balanced Acc: 0.6944 | test F1: 0.6400 | Test AUC: 1.0000\n",
      "model class name: ViTFinetuneModule\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 19:30:36,974] A new study created in memory with name: no-name-6891977b-7973-43da-973f-2bda5be2d214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== OUTER FOLD 2 / 6 =====\n",
      "Outer Train images: 114 | Outer Test images: 26\n",
      "--- Calculating normalization stats for Fold 2 Training Data ---\n",
      "Fold 2 stats: {'mean': [0.028932757675647736, 0.010694378055632114, 0.07824025303125381], 'std': [0.05646703764796257, 0.01608475297689438, 0.08640255033969879]}\n",
      "--- Generating data transforms for Fold 2 ---\n",
      "Using the train and val transform passed as arguments\n",
      "--- Starting Hyperparameter Tuning for Fold 2 ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=4.715696678089837e-05\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=4.715696678089837e-05\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=4.715696678089837e-05\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=4.715696678089837e-05\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 19:30:45,140] Trial 0 finished with value: 0.7909283339977264 and parameters: {'lr': 4.715696678089837e-05}. Best is trial 0 with value: 0.7909283339977264.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0014886262201211794\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0014886262201211794\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0014886262201211794\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0014886262201211794\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 19:30:53,062] Trial 1 finished with value: 1.1078993678092957 and parameters: {'lr': 0.0014886262201211794}. Best is trial 0 with value: 0.7909283339977264.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0004014783718209777\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0004014783718209777\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0004014783718209777\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0004014783718209777\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 19:31:01,083] Trial 2 finished with value: 0.6733226776123047 and parameters: {'lr': 0.0004014783718209777}. Best is trial 2 with value: 0.6733226776123047.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Best LR from inner CV = 0.000401\n",
      "--- Starting Final Model Training for Fold 2 with LR=0.000401 ---\n",
      "X_train_es: (96,) | X_val_es: (18,)\n",
      "Early stopping split: Train images: 96, Validation images: 18\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0004014783718209777\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "===========================\n",
      "Model Architecture:\n",
      "==================\n",
      "Total parameters: 87,456,770\n",
      "Trainable parameters: 87,456,770\n",
      "Non-trainable parameters: 0\n",
      "===========================\n",
      "Using CosineAnnealingWarmRestarts scheduler\n",
      " Fold 2 Epoch 1/230: Tr L: 2.3881, Tr Acc: 0.5431, Val L: 1.3984, Val Acc: 0.6111, Val F1: 0.0000 lr: 0.000401\n",
      " Fold 2 Epoch 2/230: Tr L: 1.4429, Tr Acc: 0.5000, Val L: 0.8815, Val Acc: 0.3889, Val F1: 0.5600 lr: 0.000401\n",
      " Fold 2 Epoch 3/230: Tr L: 0.8933, Tr Acc: 0.5000, Val L: 0.8419, Val Acc: 0.3889, Val F1: 0.5600 lr: 0.000382\n",
      " Fold 2 Epoch 4/230: Tr L: 0.6795, Tr Acc: 0.6034, Val L: 0.6471, Val Acc: 0.6111, Val F1: 0.0000 lr: 0.000327\n",
      " Fold 2 Epoch 5/230: Tr L: 0.7189, Tr Acc: 0.4483, Val L: 0.7186, Val Acc: 0.3889, Val F1: 0.5600 lr: 0.000247\n",
      " Fold 2 Epoch 6/230: Tr L: 0.7314, Tr Acc: 0.5000, Val L: 0.7318, Val Acc: 0.3889, Val F1: 0.5600 lr: 0.000159\n",
      " Fold 2 Epoch 7/230: Tr L: 0.6844, Tr Acc: 0.5517, Val L: 0.6497, Val Acc: 0.5556, Val F1: 0.3333 lr: 0.000080\n",
      " Fold 2 Epoch 8/230: Tr L: 0.6787, Tr Acc: 0.5948, Val L: 0.6461, Val Acc: 0.7222, Val F1: 0.4444 lr: 0.000025\n",
      " Fold 2 Epoch 9/230: Tr L: 0.6933, Tr Acc: 0.5000, Val L: 0.6397, Val Acc: 0.6667, Val F1: 0.5714 lr: 0.000401\n",
      " Fold 2 Epoch 10/230: Tr L: 0.6509, Tr Acc: 0.5862, Val L: 0.8026, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000397\n",
      " Fold 2 Epoch 11/230: Tr L: 0.7749, Tr Acc: 0.5086, Val L: 0.7665, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000382\n",
      " Fold 2 Epoch 12/230: Tr L: 0.6702, Tr Acc: 0.6034, Val L: 0.6246, Val Acc: 0.6667, Val F1: 0.2500 lr: 0.000358\n",
      " Fold 2 Epoch 13/230: Tr L: 0.6865, Tr Acc: 0.5776, Val L: 0.7285, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000327\n",
      " Fold 2 Epoch 14/230: Tr L: 0.6595, Tr Acc: 0.5862, Val L: 0.6137, Val Acc: 0.6667, Val F1: 0.4000 lr: 0.000289\n",
      " Fold 2 Epoch 15/230: Tr L: 0.6619, Tr Acc: 0.5776, Val L: 0.6097, Val Acc: 0.6111, Val F1: 0.3636 lr: 0.000247\n",
      " Fold 2 Epoch 16/230: Tr L: 0.6494, Tr Acc: 0.6207, Val L: 0.7070, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000203\n",
      " Fold 2 Epoch 17/230: Tr L: 0.6523, Tr Acc: 0.6121, Val L: 0.6185, Val Acc: 0.6667, Val F1: 0.5714 lr: 0.000159\n",
      " Fold 2 Epoch 18/230: Tr L: 0.6326, Tr Acc: 0.5690, Val L: 0.6158, Val Acc: 0.6667, Val F1: 0.4000 lr: 0.000117\n",
      " Fold 2 Epoch 19/230: Tr L: 0.6234, Tr Acc: 0.6121, Val L: 0.6207, Val Acc: 0.6111, Val F1: 0.5333 lr: 0.000080\n",
      " Fold 2 Epoch 20/230: Tr L: 0.6030, Tr Acc: 0.6466, Val L: 0.6407, Val Acc: 0.5000, Val F1: 0.4706 lr: 0.000048\n",
      " Fold 2 Epoch 21/230: Tr L: 0.5982, Tr Acc: 0.6293, Val L: 0.6256, Val Acc: 0.6111, Val F1: 0.5333 lr: 0.000025\n",
      " Fold 2 Epoch 22/230: Tr L: 0.5858, Tr Acc: 0.6466, Val L: 0.6182, Val Acc: 0.6667, Val F1: 0.5714 lr: 0.000010\n",
      " Fold 2 Epoch 23/230: Tr L: 0.7030, Tr Acc: 0.5862, Val L: 0.6346, Val Acc: 0.6667, Val F1: 0.4000 lr: 0.000401\n",
      " Fold 2 Epoch 24/230: Tr L: 0.7283, Tr Acc: 0.6379, Val L: 1.2611, Val Acc: 0.4444, Val F1: 0.5833 lr: 0.000400\n",
      " Fold 2 Epoch 25/230: Tr L: 0.9046, Tr Acc: 0.5345, Val L: 0.5965, Val Acc: 0.6667, Val F1: 0.5000 lr: 0.000397\n",
      " Fold 2 Epoch 26/230: Tr L: 0.4822, Tr Acc: 0.7759, Val L: 0.7203, Val Acc: 0.5556, Val F1: 0.5556 lr: 0.000390\n",
      " Fold 2 Epoch 27/230: Tr L: 0.4737, Tr Acc: 0.7500, Val L: 0.5476, Val Acc: 0.6667, Val F1: 0.5000 lr: 0.000382\n",
      " Fold 2 Epoch 28/230: Tr L: 0.3276, Tr Acc: 0.8621, Val L: 1.2294, Val Acc: 0.6111, Val F1: 0.0000 lr: 0.000371\n",
      " Fold 2 Epoch 29/230: Tr L: 0.7660, Tr Acc: 0.7414, Val L: 0.5926, Val Acc: 0.6667, Val F1: 0.5714 lr: 0.000358\n",
      " Fold 2 Epoch 30/230: Tr L: 0.3953, Tr Acc: 0.8017, Val L: 0.6574, Val Acc: 0.6111, Val F1: 0.3636 lr: 0.000343\n",
      " Fold 2 Epoch 31/230: Tr L: 0.1735, Tr Acc: 0.9569, Val L: 1.5373, Val Acc: 0.5556, Val F1: 0.3333 lr: 0.000327\n",
      " Fold 2 Epoch 32/230: Tr L: 0.1638, Tr Acc: 0.9138, Val L: 2.4219, Val Acc: 0.5556, Val F1: 0.5000 lr: 0.000309\n",
      " Fold 2 Epoch 33/230: Tr L: 0.1529, Tr Acc: 0.9655, Val L: 3.2094, Val Acc: 0.6111, Val F1: 0.6316 lr: 0.000289\n",
      " Fold 2 Epoch 34/230: Tr L: 0.4531, Tr Acc: 0.9310, Val L: 2.1798, Val Acc: 0.6111, Val F1: 0.2222 lr: 0.000269\n",
      " Fold 2 Epoch 35/230: Tr L: 0.2238, Tr Acc: 0.9483, Val L: 0.9500, Val Acc: 0.6667, Val F1: 0.6250 lr: 0.000247\n",
      " Fold 2 Epoch 36/230: Tr L: 0.2280, Tr Acc: 0.9310, Val L: 1.9956, Val Acc: 0.6111, Val F1: 0.0000 lr: 0.000225\n",
      " Fold 2 Epoch 37/230: Tr L: 0.1552, Tr Acc: 0.9397, Val L: 1.5966, Val Acc: 0.6111, Val F1: 0.6316 lr: 0.000203\n",
      " Fold 2 Epoch 38/230: Tr L: 0.0888, Tr Acc: 0.9741, Val L: 1.3346, Val Acc: 0.6667, Val F1: 0.4000 lr: 0.000181\n",
      " Fold 2 Epoch 39/230: Tr L: 0.0105, Tr Acc: 1.0000, Val L: 1.5395, Val Acc: 0.6667, Val F1: 0.5000 lr: 0.000159\n",
      " Fold 2 Epoch 40/230: Tr L: 0.0008, Tr Acc: 1.0000, Val L: 1.8138, Val Acc: 0.6111, Val F1: 0.5333 lr: 0.000138\n",
      " Fold 2 Epoch 41/230: Tr L: 0.0013, Tr Acc: 1.0000, Val L: 2.2222, Val Acc: 0.5556, Val F1: 0.5000 lr: 0.000117\n",
      " Fold 2 Epoch 42/230: Tr L: 0.0035, Tr Acc: 1.0000, Val L: 2.3851, Val Acc: 0.5556, Val F1: 0.5000 lr: 0.000098\n",
      " Fold 2 Epoch 43/230: Tr L: 0.0002, Tr Acc: 1.0000, Val L: 2.3072, Val Acc: 0.6111, Val F1: 0.4615 lr: 0.000080\n",
      " Fold 2 Epoch 44/230: Tr L: 0.0001, Tr Acc: 1.0000, Val L: 2.4962, Val Acc: 0.5556, Val F1: 0.3333 lr: 0.000063\n",
      " Fold 2 Epoch 45/230: Tr L: 0.0001, Tr Acc: 1.0000, Val L: 2.6715, Val Acc: 0.5556, Val F1: 0.3333 lr: 0.000048\n",
      " Fold 2 Epoch 46/230: Tr L: 0.0000, Tr Acc: 1.0000, Val L: 2.7308, Val Acc: 0.5556, Val F1: 0.3333 lr: 0.000035\n",
      " Fold 2 Epoch 47/230: Tr L: 0.0000, Tr Acc: 1.0000, Val L: 2.7544, Val Acc: 0.5556, Val F1: 0.3333 lr: 0.000025\n",
      " Fold 2 Epoch 48/230: Tr L: 0.0000, Tr Acc: 1.0000, Val L: 2.7644, Val Acc: 0.5556, Val F1: 0.3333 lr: 0.000016\n",
      " Fold 2 Epoch 49/230: Tr L: 0.0000, Tr Acc: 1.0000, Val L: 2.7685, Val Acc: 0.5556, Val F1: 0.3333 lr: 0.000010\n",
      " Fold 2 Epoch 50/230: Tr L: 0.0000, Tr Acc: 1.0000, Val L: 2.7703, Val Acc: 0.5556, Val F1: 0.3333 lr: 0.000006\n",
      " Fold 2 Epoch 51/230: Tr L: 0.0000, Tr Acc: 1.0000, Val L: 2.8729, Val Acc: 0.5556, Val F1: 0.3333 lr: 0.000401\n",
      " Fold 2 Epoch 52/230: Tr L: 0.0000, Tr Acc: 1.0000, Val L: 2.9962, Val Acc: 0.5000, Val F1: 0.1818 lr: 0.000401\n",
      " Fold 2 Epoch 53/230: Tr L: 0.0000, Tr Acc: 1.0000, Val L: 3.0775, Val Acc: 0.5000, Val F1: 0.1818 lr: 0.000400\n",
      " Fold 2 Epoch 54/230: Tr L: 0.0000, Tr Acc: 1.0000, Val L: 3.1296, Val Acc: 0.5000, Val F1: 0.1818 lr: 0.000399\n",
      " Fold 2 Epoch 55/230: Tr L: 0.0000, Tr Acc: 1.0000, Val L: 3.1654, Val Acc: 0.5000, Val F1: 0.1818 lr: 0.000397\n",
      " Fold 2 Epoch 56/230: Tr L: 0.0000, Tr Acc: 1.0000, Val L: 3.1903, Val Acc: 0.5000, Val F1: 0.1818 lr: 0.000394\n",
      " Fold 2 Epoch 57/230: Tr L: 0.0000, Tr Acc: 1.0000, Val L: 3.2107, Val Acc: 0.5000, Val F1: 0.1818 lr: 0.000390\n",
      " Fold 2 Epoch 58/230: Tr L: 0.0000, Tr Acc: 1.0000, Val L: 3.2273, Val Acc: 0.5000, Val F1: 0.1818 lr: 0.000386\n",
      " Fold 2 Epoch 59/230: Tr L: 0.0000, Tr Acc: 1.0000, Val L: 3.2413, Val Acc: 0.5000, Val F1: 0.1818 lr: 0.000382\n",
      " Fold 2 Epoch 60/230: Tr L: 0.0000, Tr Acc: 1.0000, Val L: 3.2539, Val Acc: 0.5000, Val F1: 0.1818 lr: 0.000377\n",
      " Fold 2 Epoch 61/230: Tr L: 0.0000, Tr Acc: 1.0000, Val L: 3.2660, Val Acc: 0.5000, Val F1: 0.1818 lr: 0.000371\n",
      " Fold 2 Epoch 62/230: Tr L: 0.0000, Tr Acc: 1.0000, Val L: 3.2770, Val Acc: 0.5000, Val F1: 0.1818 lr: 0.000365\n",
      " Fold 2 Epoch 63/230: Tr L: 0.0000, Tr Acc: 1.0000, Val L: 3.2873, Val Acc: 0.5000, Val F1: 0.1818 lr: 0.000358\n",
      " Fold 2 Epoch 64/230: Tr L: 0.0000, Tr Acc: 1.0000, Val L: 3.2972, Val Acc: 0.5000, Val F1: 0.1818 lr: 0.000351\n",
      " Fold 2 Epoch 65/230: Tr L: 0.0000, Tr Acc: 1.0000, Val L: 3.3069, Val Acc: 0.5000, Val F1: 0.1818 lr: 0.000343\n",
      " Fold 2 Epoch 66/230: Tr L: 0.0000, Tr Acc: 1.0000, Val L: 3.3162, Val Acc: 0.5000, Val F1: 0.1818 lr: 0.000335\n",
      " Fold 2 Epoch 67/230: Tr L: 0.0000, Tr Acc: 1.0000, Val L: 3.3247, Val Acc: 0.5000, Val F1: 0.1818 lr: 0.000327\n",
      "Early stopping triggered at epoch 67 for fold 2\n",
      "--- Evaluating Fold 2 on Outer Test Set ---\n",
      "Warning: model_factory used without specific LR, using default/cfg LR: 0.001\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.001\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "Test set class counts for fold 2: {0: 14, 1: 12}\n",
      "percentage of classes in test set: 0    0.538462\n",
      "1    0.461538\n",
      "Name: count, dtype: float64\n",
      " [FOLD 2 FINAL] Test Loss: 0.6313 | Test Acc: 0.5385 | test Balanced Acc: 0.5357 | test F1: 0.5000 | Test AUC: 1.0000\n",
      "model class name: ViTFinetuneModule\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 19:31:40,795] A new study created in memory with name: no-name-8fc41620-31fe-4d9b-bc3c-6d79e618f1cd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== OUTER FOLD 3 / 6 =====\n",
      "Outer Train images: 117 | Outer Test images: 23\n",
      "--- Calculating normalization stats for Fold 3 Training Data ---\n",
      "Fold 3 stats: {'mean': [0.02885417826473713, 0.011161606758832932, 0.07953041791915894], 'std': [0.054783567786216736, 0.017017517238855362, 0.08873949199914932]}\n",
      "--- Generating data transforms for Fold 3 ---\n",
      "Using the train and val transform passed as arguments\n",
      "--- Starting Hyperparameter Tuning for Fold 3 ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=4.715696678089837e-05\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=4.715696678089837e-05\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=4.715696678089837e-05\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=4.715696678089837e-05\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 19:31:48,974] Trial 0 finished with value: 0.7329201996326447 and parameters: {'lr': 4.715696678089837e-05}. Best is trial 0 with value: 0.7329201996326447.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0014886262201211794\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0014886262201211794\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0014886262201211794\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0014886262201211794\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 19:31:56,898] Trial 1 finished with value: 1.342659130692482 and parameters: {'lr': 0.0014886262201211794}. Best is trial 0 with value: 0.7329201996326447.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0004014783718209777\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0004014783718209777\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0004014783718209777\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0004014783718209777\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 19:32:04,883] Trial 2 finished with value: 0.7153236120939255 and parameters: {'lr': 0.0004014783718209777}. Best is trial 2 with value: 0.7153236120939255.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Best LR from inner CV = 0.000401\n",
      "--- Starting Final Model Training for Fold 3 with LR=0.000401 ---\n",
      "X_train_es: (99,) | X_val_es: (18,)\n",
      "Early stopping split: Train images: 99, Validation images: 18\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0004014783718209777\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "===========================\n",
      "Model Architecture:\n",
      "==================\n",
      "Total parameters: 87,456,770\n",
      "Trainable parameters: 87,456,770\n",
      "Non-trainable parameters: 0\n",
      "===========================\n",
      "Using CosineAnnealingWarmRestarts scheduler\n",
      " Fold 3 Epoch 1/230: Tr L: 3.1069, Tr Acc: 0.5086, Val L: 1.2275, Val Acc: 0.3889, Val F1: 0.2667 lr: 0.000401\n",
      " Fold 3 Epoch 2/230: Tr L: 0.8022, Tr Acc: 0.5345, Val L: 0.6647, Val Acc: 0.6111, Val F1: 0.0000 lr: 0.000401\n",
      " Fold 3 Epoch 3/230: Tr L: 0.6975, Tr Acc: 0.5086, Val L: 0.6662, Val Acc: 0.6111, Val F1: 0.0000 lr: 0.000382\n",
      " Fold 3 Epoch 4/230: Tr L: 0.7324, Tr Acc: 0.4138, Val L: 0.7070, Val Acc: 0.3889, Val F1: 0.5600 lr: 0.000327\n",
      " Fold 3 Epoch 5/230: Tr L: 0.7007, Tr Acc: 0.5000, Val L: 0.6875, Val Acc: 0.5000, Val F1: 0.3077 lr: 0.000247\n",
      " Fold 3 Epoch 6/230: Tr L: 0.6843, Tr Acc: 0.5172, Val L: 0.6684, Val Acc: 0.6111, Val F1: 0.0000 lr: 0.000159\n",
      " Fold 3 Epoch 7/230: Tr L: 0.6941, Tr Acc: 0.5000, Val L: 0.6851, Val Acc: 0.6111, Val F1: 0.3636 lr: 0.000080\n",
      " Fold 3 Epoch 8/230: Tr L: 0.6831, Tr Acc: 0.5345, Val L: 0.6989, Val Acc: 0.5000, Val F1: 0.6087 lr: 0.000025\n",
      " Fold 3 Epoch 9/230: Tr L: 0.6815, Tr Acc: 0.5776, Val L: 0.6658, Val Acc: 0.6111, Val F1: 0.0000 lr: 0.000401\n",
      " Fold 3 Epoch 10/230: Tr L: 0.6765, Tr Acc: 0.5603, Val L: 0.9138, Val Acc: 0.3889, Val F1: 0.5600 lr: 0.000397\n",
      " Fold 3 Epoch 11/230: Tr L: 0.8212, Tr Acc: 0.5000, Val L: 0.7696, Val Acc: 0.5000, Val F1: 0.6087 lr: 0.000382\n",
      " Fold 3 Epoch 12/230: Tr L: 0.7126, Tr Acc: 0.5086, Val L: 0.6711, Val Acc: 0.6111, Val F1: 0.0000 lr: 0.000358\n",
      " Fold 3 Epoch 13/230: Tr L: 0.6954, Tr Acc: 0.5345, Val L: 0.9216, Val Acc: 0.3889, Val F1: 0.5600 lr: 0.000327\n",
      " Fold 3 Epoch 14/230: Tr L: 0.6790, Tr Acc: 0.6379, Val L: 0.6678, Val Acc: 0.5556, Val F1: 0.0000 lr: 0.000289\n",
      " Fold 3 Epoch 15/230: Tr L: 0.6610, Tr Acc: 0.5172, Val L: 0.7169, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000247\n",
      " Fold 3 Epoch 16/230: Tr L: 0.6407, Tr Acc: 0.6466, Val L: 0.7117, Val Acc: 0.4444, Val F1: 0.4444 lr: 0.000203\n",
      " Fold 3 Epoch 17/230: Tr L: 0.6163, Tr Acc: 0.6121, Val L: 0.6949, Val Acc: 0.5000, Val F1: 0.4000 lr: 0.000159\n",
      " Fold 3 Epoch 18/230: Tr L: 0.5711, Tr Acc: 0.6293, Val L: 0.7161, Val Acc: 0.3889, Val F1: 0.4762 lr: 0.000117\n",
      " Fold 3 Epoch 19/230: Tr L: 0.5816, Tr Acc: 0.6810, Val L: 0.7536, Val Acc: 0.5000, Val F1: 0.6087 lr: 0.000080\n",
      " Fold 3 Epoch 20/230: Tr L: 0.5527, Tr Acc: 0.6983, Val L: 0.7000, Val Acc: 0.4444, Val F1: 0.5455 lr: 0.000048\n",
      " Fold 3 Epoch 21/230: Tr L: 0.5349, Tr Acc: 0.7069, Val L: 0.7102, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000025\n",
      " Fold 3 Epoch 22/230: Tr L: 0.5501, Tr Acc: 0.7241, Val L: 0.7108, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000010\n",
      " Fold 3 Epoch 23/230: Tr L: 0.5475, Tr Acc: 0.6810, Val L: 1.1899, Val Acc: 0.3889, Val F1: 0.5600 lr: 0.000401\n",
      " Fold 3 Epoch 24/230: Tr L: 0.5693, Tr Acc: 0.6810, Val L: 1.2725, Val Acc: 0.6111, Val F1: 0.0000 lr: 0.000400\n",
      " Fold 3 Epoch 25/230: Tr L: 0.7694, Tr Acc: 0.5862, Val L: 1.0608, Val Acc: 0.6111, Val F1: 0.6667 lr: 0.000397\n",
      " Fold 3 Epoch 26/230: Tr L: 0.4099, Tr Acc: 0.8190, Val L: 1.0900, Val Acc: 0.3889, Val F1: 0.4211 lr: 0.000390\n",
      " Fold 3 Epoch 27/230: Tr L: 0.3524, Tr Acc: 0.7759, Val L: 1.5001, Val Acc: 0.3889, Val F1: 0.1538 lr: 0.000382\n",
      " Fold 3 Epoch 28/230: Tr L: 0.2355, Tr Acc: 0.9138, Val L: 1.1362, Val Acc: 0.6667, Val F1: 0.4000 lr: 0.000371\n",
      " Fold 3 Epoch 29/230: Tr L: 0.2754, Tr Acc: 0.8793, Val L: 1.0602, Val Acc: 0.6667, Val F1: 0.5000 lr: 0.000358\n",
      " Fold 3 Epoch 30/230: Tr L: 0.0793, Tr Acc: 0.9741, Val L: 2.4632, Val Acc: 0.2778, Val F1: 0.3810 lr: 0.000343\n",
      " Fold 3 Epoch 31/230: Tr L: 0.0673, Tr Acc: 0.9655, Val L: 2.2181, Val Acc: 0.3333, Val F1: 0.4000 lr: 0.000327\n",
      " Fold 3 Epoch 32/230: Tr L: 0.0212, Tr Acc: 1.0000, Val L: 1.9500, Val Acc: 0.5000, Val F1: 0.1818 lr: 0.000309\n",
      " Fold 3 Epoch 33/230: Tr L: 0.0372, Tr Acc: 0.9828, Val L: 5.5360, Val Acc: 0.3889, Val F1: 0.5217 lr: 0.000289\n",
      " Fold 3 Epoch 34/230: Tr L: 0.0351, Tr Acc: 0.9828, Val L: 3.2218, Val Acc: 0.4444, Val F1: 0.1667 lr: 0.000269\n",
      " Fold 3 Epoch 35/230: Tr L: 0.0982, Tr Acc: 0.9569, Val L: 5.2330, Val Acc: 0.3889, Val F1: 0.4762 lr: 0.000247\n",
      " Fold 3 Epoch 36/230: Tr L: 0.0234, Tr Acc: 0.9914, Val L: 5.3822, Val Acc: 0.3889, Val F1: 0.4762 lr: 0.000225\n",
      " Fold 3 Epoch 37/230: Tr L: 0.0000, Tr Acc: 1.0000, Val L: 4.2273, Val Acc: 0.5000, Val F1: 0.4706 lr: 0.000203\n",
      " Fold 3 Epoch 38/230: Tr L: 0.0000, Tr Acc: 1.0000, Val L: 4.2332, Val Acc: 0.5556, Val F1: 0.4286 lr: 0.000181\n",
      " Fold 3 Epoch 39/230: Tr L: 0.0120, Tr Acc: 0.9914, Val L: 4.2002, Val Acc: 0.5556, Val F1: 0.4286 lr: 0.000159\n",
      " Fold 3 Epoch 40/230: Tr L: 0.0000, Tr Acc: 1.0000, Val L: 4.6225, Val Acc: 0.3889, Val F1: 0.4211 lr: 0.000138\n",
      " Fold 3 Epoch 41/230: Tr L: 0.0001, Tr Acc: 1.0000, Val L: 5.3573, Val Acc: 0.3889, Val F1: 0.4762 lr: 0.000117\n",
      " Fold 3 Epoch 42/230: Tr L: 0.0030, Tr Acc: 1.0000, Val L: 4.7831, Val Acc: 0.4444, Val F1: 0.4444 lr: 0.000098\n",
      "Early stopping triggered at epoch 42 for fold 3\n",
      "--- Evaluating Fold 3 on Outer Test Set ---\n",
      "Warning: model_factory used without specific LR, using default/cfg LR: 0.001\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.001\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "Test set class counts for fold 3: {0: 14, 1: 9}\n",
      "percentage of classes in test set: 0    0.608696\n",
      "1    0.391304\n",
      "Name: count, dtype: float64\n",
      " [FOLD 3 FINAL] Test Loss: 0.6315 | Test Acc: 0.6087 | test Balanced Acc: 0.5000 | test F1: 0.0000 | Test AUC: 1.0000\n",
      "model class name: ViTFinetuneModule\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "[I 2025-05-29 19:32:29,300] A new study created in memory with name: no-name-f5825f75-2863-4f9e-9804-2480810bdc4c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== OUTER FOLD 4 / 6 =====\n",
      "Outer Train images: 117 | Outer Test images: 23\n",
      "--- Calculating normalization stats for Fold 4 Training Data ---\n",
      "Fold 4 stats: {'mean': [0.029821500182151794, 0.011338564567267895, 0.07835365831851959], 'std': [0.057060033082962036, 0.017478104680776596, 0.09008630365133286]}\n",
      "--- Generating data transforms for Fold 4 ---\n",
      "Using the train and val transform passed as arguments\n",
      "--- Starting Hyperparameter Tuning for Fold 4 ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=4.715696678089837e-05\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=4.715696678089837e-05\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=4.715696678089837e-05\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=4.715696678089837e-05\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 19:32:37,886] Trial 0 finished with value: 0.6672669649124146 and parameters: {'lr': 4.715696678089837e-05}. Best is trial 0 with value: 0.6672669649124146.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0014886262201211794\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0014886262201211794\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0014886262201211794\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0014886262201211794\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 19:32:45,853] Trial 1 finished with value: 0.6988428831100464 and parameters: {'lr': 0.0014886262201211794}. Best is trial 0 with value: 0.6672669649124146.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0004014783718209777\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0004014783718209777\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0004014783718209777\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0004014783718209777\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 19:32:53,871] Trial 2 finished with value: 0.8623181283473969 and parameters: {'lr': 0.0004014783718209777}. Best is trial 0 with value: 0.6672669649124146.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Best LR from inner CV = 0.000047\n",
      "--- Starting Final Model Training for Fold 4 with LR=0.000047 ---\n",
      "X_train_es: (99,) | X_val_es: (18,)\n",
      "Early stopping split: Train images: 99, Validation images: 18\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=4.715696678089837e-05\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "===========================\n",
      "Model Architecture:\n",
      "==================\n",
      "Total parameters: 87,456,770\n",
      "Trainable parameters: 87,456,770\n",
      "Non-trainable parameters: 0\n",
      "===========================\n",
      "Using CosineAnnealingWarmRestarts scheduler\n",
      " Fold 4 Epoch 1/230: Tr L: 0.6859, Tr Acc: 0.5517, Val L: 1.0795, Val Acc: 0.6111, Val F1: 0.4615 lr: 0.000047\n",
      " Fold 4 Epoch 2/230: Tr L: 0.6503, Tr Acc: 0.6379, Val L: 0.9439, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000047\n",
      " Fold 4 Epoch 3/230: Tr L: 0.5854, Tr Acc: 0.6724, Val L: 0.7723, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000045\n",
      " Fold 4 Epoch 4/230: Tr L: 0.6047, Tr Acc: 0.6724, Val L: 0.7601, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000039\n",
      " Fold 4 Epoch 5/230: Tr L: 0.5809, Tr Acc: 0.6724, Val L: 0.8146, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000031\n",
      " Fold 4 Epoch 6/230: Tr L: 0.5621, Tr Acc: 0.6810, Val L: 0.8252, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000021\n",
      " Fold 4 Epoch 7/230: Tr L: 0.5536, Tr Acc: 0.6897, Val L: 0.8124, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000013\n",
      " Fold 4 Epoch 8/230: Tr L: 0.5352, Tr Acc: 0.6897, Val L: 0.8052, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000007\n",
      " Fold 4 Epoch 9/230: Tr L: 0.5344, Tr Acc: 0.7328, Val L: 0.8400, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000047\n",
      " Fold 4 Epoch 10/230: Tr L: 0.5261, Tr Acc: 0.6897, Val L: 0.8969, Val Acc: 0.4444, Val F1: 0.2857 lr: 0.000047\n",
      " Fold 4 Epoch 11/230: Tr L: 0.5030, Tr Acc: 0.6897, Val L: 1.0111, Val Acc: 0.5000, Val F1: 0.4706 lr: 0.000045\n",
      " Fold 4 Epoch 12/230: Tr L: 0.4732, Tr Acc: 0.7069, Val L: 1.1071, Val Acc: 0.4444, Val F1: 0.4444 lr: 0.000043\n",
      " Fold 4 Epoch 13/230: Tr L: 0.4707, Tr Acc: 0.6897, Val L: 1.0796, Val Acc: 0.5556, Val F1: 0.5000 lr: 0.000039\n",
      " Fold 4 Epoch 14/230: Tr L: 0.4617, Tr Acc: 0.6897, Val L: 1.1542, Val Acc: 0.5556, Val F1: 0.5000 lr: 0.000035\n",
      " Fold 4 Epoch 15/230: Tr L: 0.4474, Tr Acc: 0.7414, Val L: 1.2621, Val Acc: 0.5556, Val F1: 0.5000 lr: 0.000031\n",
      " Fold 4 Epoch 16/230: Tr L: 0.4441, Tr Acc: 0.7414, Val L: 1.3533, Val Acc: 0.5556, Val F1: 0.5000 lr: 0.000026\n",
      " Fold 4 Epoch 17/230: Tr L: 0.4293, Tr Acc: 0.7500, Val L: 1.4264, Val Acc: 0.6111, Val F1: 0.5333 lr: 0.000021\n",
      " Fold 4 Epoch 18/230: Tr L: 0.4262, Tr Acc: 0.7586, Val L: 1.4665, Val Acc: 0.5000, Val F1: 0.3077 lr: 0.000017\n",
      " Fold 4 Epoch 19/230: Tr L: 0.4192, Tr Acc: 0.7586, Val L: 1.5476, Val Acc: 0.4444, Val F1: 0.2857 lr: 0.000013\n",
      " Fold 4 Epoch 20/230: Tr L: 0.4047, Tr Acc: 0.7586, Val L: 1.5983, Val Acc: 0.5000, Val F1: 0.3077 lr: 0.000010\n",
      " Fold 4 Epoch 21/230: Tr L: 0.4029, Tr Acc: 0.7155, Val L: 1.7029, Val Acc: 0.5556, Val F1: 0.5000 lr: 0.000007\n",
      " Fold 4 Epoch 22/230: Tr L: 0.4087, Tr Acc: 0.7500, Val L: 1.6788, Val Acc: 0.5556, Val F1: 0.5000 lr: 0.000006\n",
      " Fold 4 Epoch 23/230: Tr L: 0.4190, Tr Acc: 0.7500, Val L: 1.6270, Val Acc: 0.5000, Val F1: 0.3077 lr: 0.000047\n",
      " Fold 4 Epoch 24/230: Tr L: 0.3796, Tr Acc: 0.8017, Val L: 1.7352, Val Acc: 0.5556, Val F1: 0.5000 lr: 0.000047\n",
      " Fold 4 Epoch 25/230: Tr L: 0.3689, Tr Acc: 0.8017, Val L: 1.6771, Val Acc: 0.5556, Val F1: 0.3333 lr: 0.000047\n",
      " Fold 4 Epoch 26/230: Tr L: 0.3895, Tr Acc: 0.7586, Val L: 2.1071, Val Acc: 0.5556, Val F1: 0.5000 lr: 0.000046\n",
      " Fold 4 Epoch 27/230: Tr L: 0.3203, Tr Acc: 0.8276, Val L: 2.2041, Val Acc: 0.5556, Val F1: 0.5000 lr: 0.000045\n",
      " Fold 4 Epoch 28/230: Tr L: 0.2905, Tr Acc: 0.8707, Val L: 2.4880, Val Acc: 0.5000, Val F1: 0.3077 lr: 0.000044\n",
      " Fold 4 Epoch 29/230: Tr L: 0.2515, Tr Acc: 0.8793, Val L: 2.6269, Val Acc: 0.5000, Val F1: 0.4706 lr: 0.000043\n",
      " Fold 4 Epoch 30/230: Tr L: 0.2767, Tr Acc: 0.8362, Val L: 2.6635, Val Acc: 0.5000, Val F1: 0.3077 lr: 0.000041\n",
      " Fold 4 Epoch 31/230: Tr L: 0.2456, Tr Acc: 0.8448, Val L: 2.9768, Val Acc: 0.5556, Val F1: 0.5000 lr: 0.000039\n",
      " Fold 4 Epoch 32/230: Tr L: 0.1884, Tr Acc: 0.8879, Val L: 2.8431, Val Acc: 0.5556, Val F1: 0.4286 lr: 0.000037\n",
      " Fold 4 Epoch 33/230: Tr L: 0.1438, Tr Acc: 0.9569, Val L: 3.1172, Val Acc: 0.5000, Val F1: 0.3077 lr: 0.000035\n",
      " Fold 4 Epoch 34/230: Tr L: 0.1614, Tr Acc: 0.9138, Val L: 3.4111, Val Acc: 0.5556, Val F1: 0.5000 lr: 0.000033\n",
      " Fold 4 Epoch 35/230: Tr L: 0.1109, Tr Acc: 0.9569, Val L: 3.5397, Val Acc: 0.6111, Val F1: 0.5333 lr: 0.000031\n",
      " Fold 4 Epoch 36/230: Tr L: 0.0856, Tr Acc: 0.9741, Val L: 3.6004, Val Acc: 0.5556, Val F1: 0.4286 lr: 0.000028\n",
      " Fold 4 Epoch 37/230: Tr L: 0.0621, Tr Acc: 0.9828, Val L: 3.5469, Val Acc: 0.5556, Val F1: 0.4286 lr: 0.000026\n",
      " Fold 4 Epoch 38/230: Tr L: 0.0718, Tr Acc: 0.9483, Val L: 3.5206, Val Acc: 0.5556, Val F1: 0.5000 lr: 0.000024\n",
      " Fold 4 Epoch 39/230: Tr L: 0.0441, Tr Acc: 0.9828, Val L: 3.4595, Val Acc: 0.6111, Val F1: 0.4615 lr: 0.000021\n",
      " Fold 4 Epoch 40/230: Tr L: 0.1185, Tr Acc: 0.9397, Val L: 3.4335, Val Acc: 0.6111, Val F1: 0.4615 lr: 0.000019\n",
      " Fold 4 Epoch 41/230: Tr L: 0.0550, Tr Acc: 0.9828, Val L: 4.0249, Val Acc: 0.6111, Val F1: 0.5333 lr: 0.000017\n",
      " Fold 4 Epoch 42/230: Tr L: 0.0892, Tr Acc: 0.9655, Val L: 3.5368, Val Acc: 0.6111, Val F1: 0.4615 lr: 0.000015\n",
      " Fold 4 Epoch 43/230: Tr L: 0.2028, Tr Acc: 0.9224, Val L: 3.6459, Val Acc: 0.6111, Val F1: 0.4615 lr: 0.000013\n",
      " Fold 4 Epoch 44/230: Tr L: 0.0671, Tr Acc: 0.9655, Val L: 3.8037, Val Acc: 0.6111, Val F1: 0.5333 lr: 0.000011\n",
      "Early stopping triggered at epoch 44 for fold 4\n",
      "--- Evaluating Fold 4 on Outer Test Set ---\n",
      "Warning: model_factory used without specific LR, using default/cfg LR: 0.001\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.001\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "Test set class counts for fold 4: {0: 14, 1: 9}\n",
      "percentage of classes in test set: 0    0.608696\n",
      "1    0.391304\n",
      "Name: count, dtype: float64\n",
      " [FOLD 4 FINAL] Test Loss: 0.6863 | Test Acc: 0.5217 | test Balanced Acc: 0.6071 | test F1: 0.6207 | Test AUC: 1.0000\n",
      "model class name: ViTFinetuneModule\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 19:33:19,576] A new study created in memory with name: no-name-d1f6f1fd-e0a4-46f5-988b-fe972cc5041d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== OUTER FOLD 5 / 6 =====\n",
      "Outer Train images: 119 | Outer Test images: 21\n",
      "--- Calculating normalization stats for Fold 5 Training Data ---\n",
      "Fold 5 stats: {'mean': [0.029491955414414406, 0.011217240244150162, 0.0780130922794342], 'std': [0.05655994638800621, 0.017444733530282974, 0.08970917761325836]}\n",
      "--- Generating data transforms for Fold 5 ---\n",
      "Using the train and val transform passed as arguments\n",
      "--- Starting Hyperparameter Tuning for Fold 5 ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=4.715696678089837e-05\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=4.715696678089837e-05\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=4.715696678089837e-05\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=4.715696678089837e-05\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 19:33:27,936] Trial 0 finished with value: 0.6338572353124619 and parameters: {'lr': 4.715696678089837e-05}. Best is trial 0 with value: 0.6338572353124619.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0014886262201211794\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0014886262201211794\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0014886262201211794\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0014886262201211794\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 19:33:36,046] Trial 1 finished with value: 1.1605982929468155 and parameters: {'lr': 0.0014886262201211794}. Best is trial 0 with value: 0.6338572353124619.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0004014783718209777\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0004014783718209777\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0004014783718209777\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0004014783718209777\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 19:33:44,272] Trial 2 finished with value: 0.7005098313093185 and parameters: {'lr': 0.0004014783718209777}. Best is trial 0 with value: 0.6338572353124619.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Best LR from inner CV = 0.000047\n",
      "--- Starting Final Model Training for Fold 5 with LR=0.000047 ---\n",
      "X_train_es: (101,) | X_val_es: (18,)\n",
      "Early stopping split: Train images: 101, Validation images: 18\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=4.715696678089837e-05\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "===========================\n",
      "Model Architecture:\n",
      "==================\n",
      "Total parameters: 87,456,770\n",
      "Trainable parameters: 87,456,770\n",
      "Non-trainable parameters: 0\n",
      "===========================\n",
      "Using CosineAnnealingWarmRestarts scheduler\n",
      " Fold 5 Epoch 1/230: Tr L: 0.7991, Tr Acc: 0.5000, Val L: 1.0162, Val Acc: 0.5000, Val F1: 0.5714 lr: 0.000047\n",
      " Fold 5 Epoch 2/230: Tr L: 0.6246, Tr Acc: 0.7167, Val L: 0.7634, Val Acc: 0.5556, Val F1: 0.5000 lr: 0.000047\n",
      " Fold 5 Epoch 3/230: Tr L: 0.6381, Tr Acc: 0.7167, Val L: 0.8571, Val Acc: 0.5556, Val F1: 0.5556 lr: 0.000045\n",
      " Fold 5 Epoch 4/230: Tr L: 0.6014, Tr Acc: 0.7083, Val L: 0.7176, Val Acc: 0.5556, Val F1: 0.5556 lr: 0.000039\n",
      " Fold 5 Epoch 5/230: Tr L: 0.5863, Tr Acc: 0.7167, Val L: 0.6455, Val Acc: 0.6111, Val F1: 0.4615 lr: 0.000031\n",
      " Fold 5 Epoch 6/230: Tr L: 0.5705, Tr Acc: 0.6583, Val L: 0.6794, Val Acc: 0.5556, Val F1: 0.5556 lr: 0.000021\n",
      " Fold 5 Epoch 7/230: Tr L: 0.5434, Tr Acc: 0.7417, Val L: 0.7600, Val Acc: 0.5556, Val F1: 0.5556 lr: 0.000013\n",
      " Fold 5 Epoch 8/230: Tr L: 0.5459, Tr Acc: 0.7333, Val L: 0.7799, Val Acc: 0.5556, Val F1: 0.5556 lr: 0.000007\n",
      " Fold 5 Epoch 9/230: Tr L: 0.5336, Tr Acc: 0.7333, Val L: 0.7283, Val Acc: 0.5556, Val F1: 0.5556 lr: 0.000047\n",
      " Fold 5 Epoch 10/230: Tr L: 0.6010, Tr Acc: 0.7167, Val L: 0.7389, Val Acc: 0.7222, Val F1: 0.5455 lr: 0.000047\n",
      " Fold 5 Epoch 11/230: Tr L: 0.5521, Tr Acc: 0.6833, Val L: 0.7873, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000045\n",
      " Fold 5 Epoch 12/230: Tr L: 0.5389, Tr Acc: 0.7083, Val L: 0.8395, Val Acc: 0.6111, Val F1: 0.6667 lr: 0.000043\n",
      " Fold 5 Epoch 13/230: Tr L: 0.5069, Tr Acc: 0.7000, Val L: 0.6753, Val Acc: 0.6111, Val F1: 0.4615 lr: 0.000039\n",
      " Fold 5 Epoch 14/230: Tr L: 0.5364, Tr Acc: 0.6750, Val L: 0.6650, Val Acc: 0.6111, Val F1: 0.4615 lr: 0.000035\n",
      " Fold 5 Epoch 15/230: Tr L: 0.5097, Tr Acc: 0.7333, Val L: 0.7846, Val Acc: 0.5556, Val F1: 0.5556 lr: 0.000031\n",
      " Fold 5 Epoch 16/230: Tr L: 0.4936, Tr Acc: 0.7500, Val L: 0.8150, Val Acc: 0.6111, Val F1: 0.6667 lr: 0.000026\n",
      " Fold 5 Epoch 17/230: Tr L: 0.4792, Tr Acc: 0.7333, Val L: 0.7446, Val Acc: 0.6667, Val F1: 0.7000 lr: 0.000021\n",
      " Fold 5 Epoch 18/230: Tr L: 0.4585, Tr Acc: 0.7583, Val L: 0.7361, Val Acc: 0.5556, Val F1: 0.5556 lr: 0.000017\n",
      " Fold 5 Epoch 19/230: Tr L: 0.4604, Tr Acc: 0.7167, Val L: 0.7299, Val Acc: 0.6111, Val F1: 0.5882 lr: 0.000013\n",
      " Fold 5 Epoch 20/230: Tr L: 0.4488, Tr Acc: 0.7333, Val L: 0.7627, Val Acc: 0.6111, Val F1: 0.6316 lr: 0.000010\n",
      " Fold 5 Epoch 21/230: Tr L: 0.4440, Tr Acc: 0.7583, Val L: 0.7852, Val Acc: 0.6111, Val F1: 0.6316 lr: 0.000007\n",
      " Fold 5 Epoch 22/230: Tr L: 0.4480, Tr Acc: 0.7750, Val L: 0.7734, Val Acc: 0.6111, Val F1: 0.6316 lr: 0.000006\n",
      " Fold 5 Epoch 23/230: Tr L: 0.4433, Tr Acc: 0.7667, Val L: 0.7343, Val Acc: 0.7222, Val F1: 0.7059 lr: 0.000047\n",
      " Fold 5 Epoch 24/230: Tr L: 0.4444, Tr Acc: 0.6667, Val L: 0.7839, Val Acc: 0.6667, Val F1: 0.6250 lr: 0.000047\n",
      " Fold 5 Epoch 25/230: Tr L: 0.4178, Tr Acc: 0.7750, Val L: 0.9440, Val Acc: 0.6111, Val F1: 0.6667 lr: 0.000047\n",
      " Fold 5 Epoch 26/230: Tr L: 0.4270, Tr Acc: 0.7917, Val L: 0.9154, Val Acc: 0.6111, Val F1: 0.5882 lr: 0.000046\n",
      " Fold 5 Epoch 27/230: Tr L: 0.3906, Tr Acc: 0.7583, Val L: 1.0712, Val Acc: 0.6667, Val F1: 0.6667 lr: 0.000045\n",
      " Fold 5 Epoch 28/230: Tr L: 0.4709, Tr Acc: 0.7500, Val L: 1.0577, Val Acc: 0.6111, Val F1: 0.6316 lr: 0.000044\n",
      " Fold 5 Epoch 29/230: Tr L: 0.3927, Tr Acc: 0.8000, Val L: 1.1524, Val Acc: 0.5556, Val F1: 0.5556 lr: 0.000043\n",
      " Fold 5 Epoch 30/230: Tr L: 0.4138, Tr Acc: 0.8333, Val L: 1.3694, Val Acc: 0.5556, Val F1: 0.6364 lr: 0.000041\n",
      " Fold 5 Epoch 31/230: Tr L: 0.3491, Tr Acc: 0.8333, Val L: 0.9378, Val Acc: 0.6111, Val F1: 0.5882 lr: 0.000039\n",
      " Fold 5 Epoch 32/230: Tr L: 0.3381, Tr Acc: 0.8333, Val L: 1.0896, Val Acc: 0.6667, Val F1: 0.7000 lr: 0.000037\n",
      " Fold 5 Epoch 33/230: Tr L: 0.3357, Tr Acc: 0.8333, Val L: 1.1963, Val Acc: 0.6111, Val F1: 0.6667 lr: 0.000035\n",
      " Fold 5 Epoch 34/230: Tr L: 0.3271, Tr Acc: 0.8500, Val L: 1.1283, Val Acc: 0.6667, Val F1: 0.7000 lr: 0.000033\n",
      " Fold 5 Epoch 35/230: Tr L: 0.3086, Tr Acc: 0.8667, Val L: 1.1641, Val Acc: 0.6111, Val F1: 0.5882 lr: 0.000031\n",
      " Fold 5 Epoch 36/230: Tr L: 0.2855, Tr Acc: 0.8667, Val L: 1.2992, Val Acc: 0.6111, Val F1: 0.6316 lr: 0.000028\n",
      " Fold 5 Epoch 37/230: Tr L: 0.2620, Tr Acc: 0.9167, Val L: 1.2501, Val Acc: 0.6667, Val F1: 0.6667 lr: 0.000026\n",
      " Fold 5 Epoch 38/230: Tr L: 0.2950, Tr Acc: 0.8500, Val L: 1.1613, Val Acc: 0.7222, Val F1: 0.6667 lr: 0.000024\n",
      " Fold 5 Epoch 39/230: Tr L: 0.2541, Tr Acc: 0.9250, Val L: 1.6671, Val Acc: 0.6111, Val F1: 0.6316 lr: 0.000021\n",
      " Fold 5 Epoch 40/230: Tr L: 0.2772, Tr Acc: 0.8750, Val L: 1.3252, Val Acc: 0.6667, Val F1: 0.6250 lr: 0.000019\n",
      " Fold 5 Epoch 41/230: Tr L: 0.2059, Tr Acc: 0.9167, Val L: 1.4001, Val Acc: 0.6667, Val F1: 0.6667 lr: 0.000017\n",
      " Fold 5 Epoch 42/230: Tr L: 0.1848, Tr Acc: 0.9333, Val L: 1.4183, Val Acc: 0.6667, Val F1: 0.6667 lr: 0.000015\n",
      " Fold 5 Epoch 43/230: Tr L: 0.1828, Tr Acc: 0.9333, Val L: 1.1994, Val Acc: 0.6667, Val F1: 0.6250 lr: 0.000013\n",
      " Fold 5 Epoch 44/230: Tr L: 0.1716, Tr Acc: 0.9250, Val L: 1.3338, Val Acc: 0.6667, Val F1: 0.6667 lr: 0.000011\n",
      " Fold 5 Epoch 45/230: Tr L: 0.1559, Tr Acc: 0.9583, Val L: 1.3759, Val Acc: 0.6667, Val F1: 0.6667 lr: 0.000010\n",
      "Early stopping triggered at epoch 45 for fold 5\n",
      "--- Evaluating Fold 5 on Outer Test Set ---\n",
      "Warning: model_factory used without specific LR, using default/cfg LR: 0.001\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.001\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "Test set class counts for fold 5: {0: 12, 1: 9}\n",
      "percentage of classes in test set: 0    0.571429\n",
      "1    0.428571\n",
      "Name: count, dtype: float64\n",
      " [FOLD 5 FINAL] Test Loss: 0.7525 | Test Acc: 0.4286 | test Balanced Acc: 0.3750 | test F1: 0.0000 | Test AUC: 1.0000\n",
      "model class name: ViTFinetuneModule\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 19:34:11,066] A new study created in memory with name: no-name-2e01dc3f-d71d-433f-8ee0-6b40920acc53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== OUTER FOLD 6 / 6 =====\n",
      "Outer Train images: 118 | Outer Test images: 22\n",
      "--- Calculating normalization stats for Fold 6 Training Data ---\n",
      "Fold 6 stats: {'mean': [0.028899148106575012, 0.01048550009727478, 0.07865447551012039], 'std': [0.05506647750735283, 0.015474601648747921, 0.08890823274850845]}\n",
      "--- Generating data transforms for Fold 6 ---\n",
      "Using the train and val transform passed as arguments\n",
      "--- Starting Hyperparameter Tuning for Fold 6 ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=4.715696678089837e-05\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=4.715696678089837e-05\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=4.715696678089837e-05\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=4.715696678089837e-05\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 19:34:19,438] Trial 0 finished with value: 0.7148698717355728 and parameters: {'lr': 4.715696678089837e-05}. Best is trial 0 with value: 0.7148698717355728.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0014886262201211794\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0014886262201211794\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0014886262201211794\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0014886262201211794\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 19:34:27,581] Trial 1 finished with value: 1.0400511771440506 and parameters: {'lr': 0.0014886262201211794}. Best is trial 0 with value: 0.7148698717355728.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0004014783718209777\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0004014783718209777\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0004014783718209777\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.0004014783718209777\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 19:34:35,551] Trial 2 finished with value: 0.7631227076053619 and parameters: {'lr': 0.0004014783718209777}. Best is trial 0 with value: 0.7148698717355728.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Best LR from inner CV = 0.000047\n",
      "--- Starting Final Model Training for Fold 6 with LR=0.000047 ---\n",
      "X_train_es: (100,) | X_val_es: (18,)\n",
      "Early stopping split: Train images: 100, Validation images: 18\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=4.715696678089837e-05\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "===========================\n",
      "Model Architecture:\n",
      "==================\n",
      "Total parameters: 87,456,770\n",
      "Trainable parameters: 87,456,770\n",
      "Non-trainable parameters: 0\n",
      "===========================\n",
      "Using CosineAnnealingWarmRestarts scheduler\n",
      " Fold 6 Epoch 1/230: Tr L: 0.6477, Tr Acc: 0.6271, Val L: 0.8371, Val Acc: 0.4444, Val F1: 0.5455 lr: 0.000047\n",
      " Fold 6 Epoch 2/230: Tr L: 0.6918, Tr Acc: 0.6017, Val L: 0.6444, Val Acc: 0.6111, Val F1: 0.3636 lr: 0.000047\n",
      " Fold 6 Epoch 3/230: Tr L: 0.6004, Tr Acc: 0.5932, Val L: 0.6754, Val Acc: 0.5000, Val F1: 0.6087 lr: 0.000045\n",
      " Fold 6 Epoch 4/230: Tr L: 0.6076, Tr Acc: 0.5847, Val L: 0.6283, Val Acc: 0.5000, Val F1: 0.4000 lr: 0.000039\n",
      " Fold 6 Epoch 5/230: Tr L: 0.5809, Tr Acc: 0.6441, Val L: 0.6418, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000031\n",
      " Fold 6 Epoch 6/230: Tr L: 0.5604, Tr Acc: 0.6441, Val L: 0.7182, Val Acc: 0.5000, Val F1: 0.6087 lr: 0.000021\n",
      " Fold 6 Epoch 7/230: Tr L: 0.5618, Tr Acc: 0.6780, Val L: 0.6814, Val Acc: 0.4444, Val F1: 0.5455 lr: 0.000013\n",
      " Fold 6 Epoch 8/230: Tr L: 0.5462, Tr Acc: 0.6864, Val L: 0.6614, Val Acc: 0.4444, Val F1: 0.5455 lr: 0.000007\n",
      " Fold 6 Epoch 9/230: Tr L: 0.5323, Tr Acc: 0.6441, Val L: 0.6182, Val Acc: 0.5000, Val F1: 0.6087 lr: 0.000047\n",
      " Fold 6 Epoch 10/230: Tr L: 0.5132, Tr Acc: 0.6780, Val L: 0.6582, Val Acc: 0.5000, Val F1: 0.6087 lr: 0.000047\n",
      " Fold 6 Epoch 11/230: Tr L: 0.5274, Tr Acc: 0.6695, Val L: 0.6232, Val Acc: 0.4444, Val F1: 0.5455 lr: 0.000045\n",
      " Fold 6 Epoch 12/230: Tr L: 0.5165, Tr Acc: 0.6017, Val L: 0.5980, Val Acc: 0.6667, Val F1: 0.5000 lr: 0.000043\n",
      " Fold 6 Epoch 13/230: Tr L: 0.5168, Tr Acc: 0.6441, Val L: 0.6528, Val Acc: 0.4444, Val F1: 0.5455 lr: 0.000039\n",
      " Fold 6 Epoch 14/230: Tr L: 0.4643, Tr Acc: 0.7966, Val L: 0.5626, Val Acc: 0.7222, Val F1: 0.5455 lr: 0.000035\n",
      " Fold 6 Epoch 15/230: Tr L: 0.4862, Tr Acc: 0.6186, Val L: 0.6619, Val Acc: 0.4444, Val F1: 0.4444 lr: 0.000031\n",
      " Fold 6 Epoch 16/230: Tr L: 0.5211, Tr Acc: 0.6949, Val L: 0.9058, Val Acc: 0.5000, Val F1: 0.6087 lr: 0.000026\n",
      " Fold 6 Epoch 17/230: Tr L: 0.4755, Tr Acc: 0.6864, Val L: 0.5985, Val Acc: 0.5556, Val F1: 0.5000 lr: 0.000021\n",
      " Fold 6 Epoch 18/230: Tr L: 0.4491, Tr Acc: 0.7203, Val L: 0.5576, Val Acc: 0.7222, Val F1: 0.5455 lr: 0.000017\n",
      " Fold 6 Epoch 19/230: Tr L: 0.4486, Tr Acc: 0.7119, Val L: 0.6265, Val Acc: 0.4444, Val F1: 0.4444 lr: 0.000013\n",
      " Fold 6 Epoch 20/230: Tr L: 0.4362, Tr Acc: 0.7797, Val L: 0.6799, Val Acc: 0.3889, Val F1: 0.4762 lr: 0.000010\n",
      " Fold 6 Epoch 21/230: Tr L: 0.4227, Tr Acc: 0.7712, Val L: 0.6090, Val Acc: 0.5556, Val F1: 0.5000 lr: 0.000007\n",
      " Fold 6 Epoch 22/230: Tr L: 0.4174, Tr Acc: 0.7881, Val L: 0.5863, Val Acc: 0.6111, Val F1: 0.5333 lr: 0.000006\n",
      " Fold 6 Epoch 23/230: Tr L: 0.4236, Tr Acc: 0.8136, Val L: 0.6679, Val Acc: 0.4444, Val F1: 0.4444 lr: 0.000047\n",
      " Fold 6 Epoch 24/230: Tr L: 0.4286, Tr Acc: 0.8051, Val L: 0.6847, Val Acc: 0.6667, Val F1: 0.5714 lr: 0.000047\n",
      " Fold 6 Epoch 25/230: Tr L: 0.3837, Tr Acc: 0.7373, Val L: 1.2926, Val Acc: 0.6111, Val F1: 0.5333 lr: 0.000047\n",
      " Fold 6 Epoch 26/230: Tr L: 0.3704, Tr Acc: 0.8390, Val L: 0.7075, Val Acc: 0.6667, Val F1: 0.5714 lr: 0.000046\n",
      " Fold 6 Epoch 27/230: Tr L: 0.3362, Tr Acc: 0.8390, Val L: 0.8836, Val Acc: 0.5556, Val F1: 0.6000 lr: 0.000045\n",
      " Fold 6 Epoch 28/230: Tr L: 0.3315, Tr Acc: 0.8390, Val L: 0.8143, Val Acc: 0.6667, Val F1: 0.5714 lr: 0.000044\n",
      " Fold 6 Epoch 29/230: Tr L: 0.3166, Tr Acc: 0.8559, Val L: 1.0677, Val Acc: 0.5556, Val F1: 0.5000 lr: 0.000043\n",
      " Fold 6 Epoch 30/230: Tr L: 0.3332, Tr Acc: 0.8559, Val L: 1.0914, Val Acc: 0.6667, Val F1: 0.5714 lr: 0.000041\n",
      " Fold 6 Epoch 31/230: Tr L: 0.3717, Tr Acc: 0.8051, Val L: 1.3153, Val Acc: 0.5000, Val F1: 0.4706 lr: 0.000039\n",
      " Fold 6 Epoch 32/230: Tr L: 0.3118, Tr Acc: 0.8644, Val L: 1.1666, Val Acc: 0.6667, Val F1: 0.5714 lr: 0.000037\n",
      " Fold 6 Epoch 33/230: Tr L: 0.2408, Tr Acc: 0.8898, Val L: 1.2487, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000035\n",
      " Fold 6 Epoch 34/230: Tr L: 0.1968, Tr Acc: 0.9153, Val L: 1.1207, Val Acc: 0.6667, Val F1: 0.5714 lr: 0.000033\n",
      " Fold 6 Epoch 35/230: Tr L: 0.2342, Tr Acc: 0.8644, Val L: 1.8666, Val Acc: 0.4444, Val F1: 0.5455 lr: 0.000031\n",
      " Fold 6 Epoch 36/230: Tr L: 0.2043, Tr Acc: 0.9237, Val L: 1.1096, Val Acc: 0.6667, Val F1: 0.5714 lr: 0.000028\n",
      " Fold 6 Epoch 37/230: Tr L: 0.3204, Tr Acc: 0.8390, Val L: 1.8422, Val Acc: 0.6111, Val F1: 0.5333 lr: 0.000026\n",
      " Fold 6 Epoch 38/230: Tr L: 0.3158, Tr Acc: 0.8475, Val L: 2.4398, Val Acc: 0.4444, Val F1: 0.5455 lr: 0.000024\n",
      " Fold 6 Epoch 39/230: Tr L: 0.3184, Tr Acc: 0.8390, Val L: 1.9260, Val Acc: 0.6111, Val F1: 0.5333 lr: 0.000021\n",
      " Fold 6 Epoch 40/230: Tr L: 0.2177, Tr Acc: 0.8898, Val L: 1.7830, Val Acc: 0.6111, Val F1: 0.5333 lr: 0.000019\n",
      " Fold 6 Epoch 41/230: Tr L: 0.1922, Tr Acc: 0.8814, Val L: 2.0731, Val Acc: 0.5556, Val F1: 0.5556 lr: 0.000017\n",
      " Fold 6 Epoch 42/230: Tr L: 0.2034, Tr Acc: 0.9407, Val L: 2.1443, Val Acc: 0.5000, Val F1: 0.5263 lr: 0.000015\n",
      " Fold 6 Epoch 43/230: Tr L: 0.1911, Tr Acc: 0.9322, Val L: 1.9480, Val Acc: 0.6111, Val F1: 0.5333 lr: 0.000013\n",
      " Fold 6 Epoch 44/230: Tr L: 0.1572, Tr Acc: 0.9237, Val L: 2.0409, Val Acc: 0.6111, Val F1: 0.5333 lr: 0.000011\n",
      " Fold 6 Epoch 45/230: Tr L: 0.1595, Tr Acc: 0.9492, Val L: 2.1298, Val Acc: 0.5556, Val F1: 0.5000 lr: 0.000010\n",
      " Fold 6 Epoch 46/230: Tr L: 0.1673, Tr Acc: 0.9407, Val L: 2.0767, Val Acc: 0.6111, Val F1: 0.5333 lr: 0.000008\n",
      " Fold 6 Epoch 47/230: Tr L: 0.1387, Tr Acc: 0.9237, Val L: 2.0909, Val Acc: 0.6111, Val F1: 0.5333 lr: 0.000007\n",
      " Fold 6 Epoch 48/230: Tr L: 0.1471, Tr Acc: 0.9237, Val L: 2.0941, Val Acc: 0.6111, Val F1: 0.5333 lr: 0.000006\n",
      " Fold 6 Epoch 49/230: Tr L: 0.1367, Tr Acc: 0.9407, Val L: 2.1580, Val Acc: 0.5556, Val F1: 0.5000 lr: 0.000006\n",
      " Fold 6 Epoch 50/230: Tr L: 0.1411, Tr Acc: 0.9407, Val L: 2.2566, Val Acc: 0.6111, Val F1: 0.5882 lr: 0.000005\n",
      " Fold 6 Epoch 51/230: Tr L: 0.2165, Tr Acc: 0.9492, Val L: 1.9197, Val Acc: 0.6667, Val F1: 0.5714 lr: 0.000047\n",
      " Fold 6 Epoch 52/230: Tr L: 0.1629, Tr Acc: 0.9237, Val L: 1.9981, Val Acc: 0.5556, Val F1: 0.5000 lr: 0.000047\n",
      " Fold 6 Epoch 53/230: Tr L: 0.1449, Tr Acc: 0.9661, Val L: 2.3330, Val Acc: 0.6111, Val F1: 0.5333 lr: 0.000047\n",
      " Fold 6 Epoch 54/230: Tr L: 0.1712, Tr Acc: 0.9407, Val L: 2.7635, Val Acc: 0.6111, Val F1: 0.5882 lr: 0.000047\n",
      " Fold 6 Epoch 55/230: Tr L: 0.0887, Tr Acc: 0.9492, Val L: 2.7823, Val Acc: 0.6111, Val F1: 0.5333 lr: 0.000047\n",
      " Fold 6 Epoch 56/230: Tr L: 0.1548, Tr Acc: 0.9492, Val L: 2.9107, Val Acc: 0.5556, Val F1: 0.5556 lr: 0.000046\n",
      " Fold 6 Epoch 57/230: Tr L: 0.0672, Tr Acc: 0.9831, Val L: 2.3241, Val Acc: 0.6111, Val F1: 0.5333 lr: 0.000046\n",
      " Fold 6 Epoch 58/230: Tr L: 0.0860, Tr Acc: 0.9576, Val L: 2.7398, Val Acc: 0.6111, Val F1: 0.5882 lr: 0.000046\n",
      "Early stopping triggered at epoch 58 for fold 6\n",
      "--- Evaluating Fold 6 on Outer Test Set ---\n",
      "Warning: model_factory used without specific LR, using default/cfg LR: 0.001\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.001\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "Test set class counts for fold 6: {0: 13, 1: 9}\n",
      "percentage of classes in test set: 0    0.590909\n",
      "1    0.409091\n",
      "Name: count, dtype: float64\n",
      " [FOLD 6 FINAL] Test Loss: 2.8156 | Test Acc: 0.3636 | test Balanced Acc: 0.3077 | test F1: 0.0000 | Test AUC: 1.0000\n",
      "model class name: ViTFinetuneModule\n",
      "\n",
      "-------------------------------------------------\n",
      "Cross-validation results (outer folds):\n",
      "  Fold 1: Test Loss=0.8673, Acc=0.6400, F1=0.6400, Bal Acc=0.6944, AUC=1.0000 (Best LR=0.000047)\n",
      "  Fold 2: Test Loss=0.6313, Acc=0.5385, F1=0.5000, Bal Acc=0.5357, AUC=1.0000 (Best LR=0.000401)\n",
      "  Fold 3: Test Loss=0.6315, Acc=0.6087, F1=0.0000, Bal Acc=0.5000, AUC=1.0000 (Best LR=0.000401)\n",
      "  Fold 4: Test Loss=0.6863, Acc=0.5217, F1=0.6207, Bal Acc=0.6071, AUC=1.0000 (Best LR=0.000047)\n",
      "  Fold 5: Test Loss=0.7525, Acc=0.4286, F1=0.0000, Bal Acc=0.3750, AUC=1.0000 (Best LR=0.000047)\n",
      "  Fold 6: Test Loss=2.8156, Acc=0.3636, F1=0.0000, Bal Acc=0.3077, AUC=1.0000 (Best LR=0.000047)\n",
      "\n",
      "--- Aggregate Results ---\n",
      "Avg Test Accuracy: 0.5169 +/- 0.0961\n",
      "Avg Test F1-Score: 0.2934 +/- 0.2967\n",
      "Avg Test Balanced Acc: 0.5033 +/- 0.1311\n",
      "Avg Test Precision: 0.2417 +/- 0.2422\n",
      "Avg Test Recall: 0.3981 +/- 0.4260\n",
      "-------------------------------------------------\n",
      "[{'fold': 1, 'test_loss': 0.8673028349876404, 'test_acc': 0.64, 'test_f1': 0.64, 'test_balanced_acc': 0.6944444444444444, 'test_auc': 1, 'test_precision': 0.5, 'test_recall': 0.8888888888888888, 'best_lr': 4.715696678089837e-05}, {'fold': 2, 'test_loss': 0.6313477754592896, 'test_acc': 0.5384615384615384, 'test_f1': 0.5, 'test_balanced_acc': 0.5357142857142857, 'test_auc': 1, 'test_precision': 0.5, 'test_recall': 0.5, 'best_lr': 0.0004014783718209777}, {'fold': 3, 'test_loss': 0.6314552426338196, 'test_acc': 0.6086956521739131, 'test_f1': 0.0, 'test_balanced_acc': 0.5, 'test_auc': 1, 'test_precision': 0.0, 'test_recall': 0.0, 'best_lr': 0.0004014783718209777}, {'fold': 4, 'test_loss': 0.6863318085670471, 'test_acc': 0.5217391304347826, 'test_f1': 0.6206896551724138, 'test_balanced_acc': 0.6071428571428571, 'test_auc': 1, 'test_precision': 0.45, 'test_recall': 1.0, 'best_lr': 4.715696678089837e-05}, {'fold': 5, 'test_loss': 0.752527117729187, 'test_acc': 0.42857142857142855, 'test_f1': 0.0, 'test_balanced_acc': 0.375, 'test_auc': 1, 'test_precision': 0.0, 'test_recall': 0.0, 'best_lr': 4.715696678089837e-05}, {'fold': 6, 'test_loss': 2.8155698776245117, 'test_acc': 0.36363636363636365, 'test_f1': 0.0, 'test_balanced_acc': 0.3076923076923077, 'test_auc': 1, 'test_precision': 0.0, 'test_recall': 0.0, 'best_lr': 4.715696678089837e-05}]\n",
      "Best test_balanced_acc Fold Result: {'fold': 1, 'test_loss': 0.8673028349876404, 'test_acc': 0.64, 'test_f1': 0.64, 'test_balanced_acc': 0.6944444444444444, 'test_auc': 1, 'test_precision': 0.5, 'test_recall': 0.8888888888888888, 'best_lr': 4.715696678089837e-05}\n",
      "Warning: model_factory used without specific LR, using default/cfg LR: 0.001\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.001\n",
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "  model_factory called: Creating ViTFinetuneModule instance with LR=0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Malformed experiment '833162958986127285'. Detailed error Yaml file '/home/zano/Documents/TESI/mlruns/833162958986127285/meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/zano/Documents/TESI/TESI/venv/lib/python3.12/site-packages/mlflow/store/tracking/file_store.py\", line 328, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zano/Documents/TESI/TESI/venv/lib/python3.12/site-packages/mlflow/store/tracking/file_store.py\", line 422, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zano/Documents/TESI/TESI/venv/lib/python3.12/site-packages/mlflow/store/tracking/file_store.py\", line 1368, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zano/Documents/TESI/TESI/venv/lib/python3.12/site-packages/mlflow/store/tracking/file_store.py\", line 1361, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zano/Documents/TESI/TESI/venv/lib/python3.12/site-packages/mlflow/utils/file_utils.py\", line 310, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/zano/Documents/TESI/mlruns/833162958986127285/meta.yaml' does not exist.\n",
      "WARNING:root:Malformed experiment '833162958986127285'. Detailed error Yaml file '/home/zano/Documents/TESI/mlruns/833162958986127285/meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/zano/Documents/TESI/TESI/venv/lib/python3.12/site-packages/mlflow/store/tracking/file_store.py\", line 328, in search_experiments\n",
      "    exp = self._get_experiment(exp_id, view_type)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zano/Documents/TESI/TESI/venv/lib/python3.12/site-packages/mlflow/store/tracking/file_store.py\", line 422, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zano/Documents/TESI/TESI/venv/lib/python3.12/site-packages/mlflow/store/tracking/file_store.py\", line 1368, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zano/Documents/TESI/TESI/venv/lib/python3.12/site-packages/mlflow/store/tracking/file_store.py\", line 1361, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/zano/Documents/TESI/TESI/venv/lib/python3.12/site-packages/mlflow/utils/file_utils.py\", line 310, in read_yaml\n",
      "    raise MissingConfigException(f\"Yaml file '{file_path}' does not exist.\")\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/zano/Documents/TESI/mlruns/833162958986127285/meta.yaml' does not exist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Attempting to load processed state dictionary into the target model...\n",
      "  Successfully loaded processed weights into the target model.\n",
      "--- Finished Loading MAE Backbone Weights ---\n",
      "you are on linux\n",
      "Run name: ViT_oversamp_monai_color_transforms:False_05-29_at:19-35-10\n",
      "Current tracking uri: /home/zano/Documents/TESI/mlruns\n",
      "None\n",
      "Target layer: vit_backbone.patch_embed.proj\n",
      "Target layer type: <class 'torch.nn.modules.conv.Conv2d'>\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_1/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_2/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_3/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_4/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_5/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_6/train_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_1/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_2/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_3/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_4/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_5/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_6/val_loss\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_1/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_2/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_3/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_4/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_5/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_6/train_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_1/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_2/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_3/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_4/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_5/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_6/val_accuracy\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_1/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_2/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_3/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_4/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_5/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_6/val_f1\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_1/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_2/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_3/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_4/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_5/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_6/val_balanced_accuracy\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_1/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_2/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_3/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_4/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_5/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_6/val_precision\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_1/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_2/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_3/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_4/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_5/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_6/val_recall\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_1/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_2/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_3/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_4/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_5/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n",
      "Logging test_fold_6/val_auc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m2025/05/29 19:35:17 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating GradCAM for test images: cannot access local variable 'test_loader' where it is not associated with a value\n",
      "Processing batch 1, shape: torch.Size([21, 3, 224, 224])\n",
      "Saved GRADCAMPP overlay with DAPI-like channel to: /home/zano/Documents/TESI/TESI/gradcampp_outputs/mae_freezed:False_ViT_oversamp_monai_color_transforms:False_05-29_at:19-35-10/batch_0_img_0.png\n",
      "Saved GRADCAMPP overlay with DAPI-like channel to: /home/zano/Documents/TESI/TESI/gradcampp_outputs/mae_freezed:False_ViT_oversamp_monai_color_transforms:False_05-29_at:19-35-10/batch_0_img_1.png\n",
      "Saved GRADCAMPP overlay with DAPI-like channel to: /home/zano/Documents/TESI/TESI/gradcampp_outputs/mae_freezed:False_ViT_oversamp_monai_color_transforms:False_05-29_at:19-35-10/batch_0_img_2.png\n",
      "Saved GRADCAMPP overlay with DAPI-like channel to: /home/zano/Documents/TESI/TESI/gradcampp_outputs/mae_freezed:False_ViT_oversamp_monai_color_transforms:False_05-29_at:19-35-10/batch_0_img_3.png\n",
      "Saved GRADCAMPP overlay with DAPI-like channel to: /home/zano/Documents/TESI/TESI/gradcampp_outputs/mae_freezed:False_ViT_oversamp_monai_color_transforms:False_05-29_at:19-35-10/batch_0_img_4.png\n",
      "Saved GRADCAMPP overlay with DAPI-like channel to: /home/zano/Documents/TESI/TESI/gradcampp_outputs/mae_freezed:False_ViT_oversamp_monai_color_transforms:False_05-29_at:19-35-10/batch_0_img_5.png\n",
      "Saved GRADCAMPP overlay with DAPI-like channel to: /home/zano/Documents/TESI/TESI/gradcampp_outputs/mae_freezed:False_ViT_oversamp_monai_color_transforms:False_05-29_at:19-35-10/batch_0_img_6.png\n",
      "Saved GRADCAMPP overlay with DAPI-like channel to: /home/zano/Documents/TESI/TESI/gradcampp_outputs/mae_freezed:False_ViT_oversamp_monai_color_transforms:False_05-29_at:19-35-10/batch_0_img_7.png\n",
      "Saved GRADCAMPP overlay with DAPI-like channel to: /home/zano/Documents/TESI/TESI/gradcampp_outputs/mae_freezed:False_ViT_oversamp_monai_color_transforms:False_05-29_at:19-35-10/batch_0_img_8.png\n",
      "Saved GRADCAMPP overlay with DAPI-like channel to: /home/zano/Documents/TESI/TESI/gradcampp_outputs/mae_freezed:False_ViT_oversamp_monai_color_transforms:False_05-29_at:19-35-10/batch_0_img_9.png\n",
      "Saved GRADCAMPP overlay with DAPI-like channel to: /home/zano/Documents/TESI/TESI/gradcampp_outputs/mae_freezed:False_ViT_oversamp_monai_color_transforms:False_05-29_at:19-35-10/batch_0_img_10.png\n",
      "Saved GRADCAMPP overlay with DAPI-like channel to: /home/zano/Documents/TESI/TESI/gradcampp_outputs/mae_freezed:False_ViT_oversamp_monai_color_transforms:False_05-29_at:19-35-10/batch_0_img_11.png\n",
      "Saved GRADCAMPP overlay with DAPI-like channel to: /home/zano/Documents/TESI/TESI/gradcampp_outputs/mae_freezed:False_ViT_oversamp_monai_color_transforms:False_05-29_at:19-35-10/batch_0_img_12.png\n",
      "Saved GRADCAMPP overlay with DAPI-like channel to: /home/zano/Documents/TESI/TESI/gradcampp_outputs/mae_freezed:False_ViT_oversamp_monai_color_transforms:False_05-29_at:19-35-10/batch_0_img_13.png\n",
      "Saved GRADCAMPP overlay with DAPI-like channel to: /home/zano/Documents/TESI/TESI/gradcampp_outputs/mae_freezed:False_ViT_oversamp_monai_color_transforms:False_05-29_at:19-35-10/batch_0_img_14.png\n",
      "Saved GRADCAMPP overlay with DAPI-like channel to: /home/zano/Documents/TESI/TESI/gradcampp_outputs/mae_freezed:False_ViT_oversamp_monai_color_transforms:False_05-29_at:19-35-10/batch_0_img_15.png\n",
      "Saved GRADCAMPP overlay with DAPI-like channel to: /home/zano/Documents/TESI/TESI/gradcampp_outputs/mae_freezed:False_ViT_oversamp_monai_color_transforms:False_05-29_at:19-35-10/batch_0_img_16.png\n",
      "Saved GRADCAMPP overlay with DAPI-like channel to: /home/zano/Documents/TESI/TESI/gradcampp_outputs/mae_freezed:False_ViT_oversamp_monai_color_transforms:False_05-29_at:19-35-10/batch_0_img_17.png\n",
      "Saved GRADCAMPP overlay with DAPI-like channel to: /home/zano/Documents/TESI/TESI/gradcampp_outputs/mae_freezed:False_ViT_oversamp_monai_color_transforms:False_05-29_at:19-35-10/batch_0_img_18.png\n",
      "Saved GRADCAMPP overlay with DAPI-like channel to: /home/zano/Documents/TESI/TESI/gradcampp_outputs/mae_freezed:False_ViT_oversamp_monai_color_transforms:False_05-29_at:19-35-10/batch_0_img_19.png\n",
      "Saved GRADCAMPP overlay with DAPI-like channel to: /home/zano/Documents/TESI/TESI/gradcampp_outputs/mae_freezed:False_ViT_oversamp_monai_color_transforms:False_05-29_at:19-35-10/batch_0_img_20.png\n",
      "Memory cleared - Before: 1712.23MB -> After: 1712.23MB\n",
      "Peak memory during session: 3456.05MB\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DOWNSTREAM TASK\n",
    "import collections\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "import timm # Make sure timm is installed\n",
    "import torchvision.transforms as T # Assuming you might use torchvision transforms\n",
    "from utils.train_functions import BaseSSLClassifier, SSLClassifierModule, nested_cv_stratified_by_patient, solve_cuda_oom\n",
    "import copy\n",
    "from utils.train_functions import  SSLClassifierModule, get_best_fold_idx\n",
    "from utils.mlflow_functions import log_SSL_run_to_mlflow\n",
    "\n",
    "# Import your custom utility functions\n",
    "from utils.train_functions import MLPClassifierHead, LinearProbeHead # Assuming this defines your classifier head\n",
    "from utils.train_functions import nested_cv_stratified_by_patient # Your nested CV function\n",
    "from utils.data_visualization_functions import generate_cv_results_figure # Your plotting function\n",
    "\n",
    "# --- Configuration ---\n",
    "MAE_BACKBONE_PATH = SAVE_PATH # Path to your trained MAE weights file\n",
    "FEATURE_DIM = 768                # Expected output dimension of ViT-Base CLS token (embed_dim)\n",
    "NUM_CLASSES = 2                  # Number of downstream classes for your specific task\n",
    "INPUT_SIZE = 224                 # Input image size (HxW) the MAE ViT was pre-trained on and expects\n",
    "\n",
    "# --- Lightning Module for ViT Fine-tuning ---\n",
    "class ViTFinetuneModule(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    PyTorch Lightning module specifically for fine-tuning a MAE pre-trained ViT backbone.\n",
    "    It handles:\n",
    "    1. Loading the ViT architecture (via TIMM).\n",
    "    2. Loading the pre-trained MAE backbone weights.\n",
    "    3. Attaching a classification head (MLP).\n",
    "    4. Freezing the backbone (optional).\n",
    "    5. Defining the forward pass, loss, optimizer, and train/val/test steps.\n",
    "    Designed to be instantiated by the `model_factory` for use with `nested_cv_stratified_by_patient`.\n",
    "    \"\"\"\n",
    "    def __init__(self, mae_weights_path: str,model_name, num_classes: int, freeze_backbone: bool, lr: float):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            mae_weights_path (str): Path to the '.pth' file containing MAE backbone weights.\n",
    "            num_classes (int): Number of target classes for the downstream task.\n",
    "            freeze_backbone (bool): If True, sets `requires_grad=False` for backbone parameters.\n",
    "            lr (float): Learning rate for the optimizer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Save hyperparameters (num_classes, freeze_backbone, lr) to self.hparams\n",
    "        # Makes them accessible later (e.g., self.hparams.lr) and logs them automatically.\n",
    "        # We don't save mae_weights_path as it's used only during init here.\n",
    "        self.save_hyperparameters(\"num_classes\", \"freeze_backbone\", \"lr\")\n",
    "        # 1. Build ViT Backbone Architecture using TIMM\n",
    "        # Create the specified ViT model ('vit_base_patch32_224').\n",
    "        # `pretrained=False`: Do not load ImageNet weights, we'll load MAE weights.\n",
    "        # `num_classes=0`: Remove the default classifier head from the TIMM model.\n",
    "        #                  This typically makes the model output the CLS token embedding.\n",
    "        # print(\"  Initializing ViT backbone architecture (timm)...\")\n",
    "        vit = timm.create_model(\n",
    "            model_name=model_name, # Use the model name passed to the constructor\n",
    "            pretrained=False,\n",
    "            num_classes=0 # No classifier head, we will add our own later\n",
    "        )\n",
    "        self.vit_backbone = vit\n",
    "        # Get the embedding dimension directly from the loaded TIMM model (robust)\n",
    "        self.feature_dim = self.vit_backbone.embed_dim\n",
    "        # print(f\"  ViT backbone embedding dimension: {self.feature_dim}\") # Should be 768\n",
    "\n",
    "        # 2. Load MAE Pre-trained Weights into the Backbone\n",
    "        load_mae_vit_backbone_state_dict(\n",
    "            model=self.vit_backbone,         # Pass the model instance here\n",
    "            checkpoint_path=mae_weights_path # Pass the path to the weights file\n",
    "            # prefix=\"vit.\" # Default is \"vit.\", change if your MAE model used a different internal name\n",
    "        )\n",
    "\n",
    "        # 3. Freeze Backbone (Optional)\n",
    "        # If `freeze_backbone` is True, iterate through backbone parameters and disable gradient calculation.\n",
    "        if self.hparams.freeze_backbone:\n",
    "            # print(\"  Freezing MAE backbone parameters (requires_grad=False).\")\n",
    "            for param in self.vit_backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "            # Set the frozen backbone to evaluation mode (disables dropout, uses running stats in BatchNorm if any)\n",
    "            self.vit_backbone.eval()\n",
    "\n",
    "        # 4. Attach Classification Head\n",
    "        # Use the imported MLPClassifierHead utility function.\n",
    "        # It needs the backbone's output feature dimension (768) and the number of classes.\n",
    "        # print(f\"  Attaching MLPClassifierHead (input_dim={self.feature_dim}, num_classes={self.hparams.num_classes}).\")\n",
    "        #   self.classifier = MLPClassifierHead(self.feature_dim, self.hparams.num_classes)\n",
    "        self.classifier = LinearProbeHead(self.feature_dim, self.hparams.num_classes)\n",
    "\n",
    "        # 5. Define Loss Function\n",
    "        # Standard Cross-Entropy Loss for multi-class classification.\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\" Defines the forward pass of the model. \"\"\"\n",
    "        # Pass input `x` through the ViT backbone.\n",
    "        # For timm ViTs with num_classes=0, this call typically returns the CLS token embedding.\n",
    "        # Expected output shape: (batch_size, embed_dim) -> (B, 768)\n",
    "        features = self.vit_backbone(x)\n",
    "        # Pass the extracted features (CLS token embedding) through the classifier head.\n",
    "        logits = self.classifier(features)\n",
    "        # Return the final logits (raw scores for each class).\n",
    "        return logits\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\" Defines the optimizer and optionally learning rate schedulers. \"\"\"\n",
    "        # Use AdamW optimizer, common for Transformers.\n",
    "        # `filter(lambda p: p.requires_grad, self.parameters())`: Ensures only parameters\n",
    "        # that require gradients (i.e., not frozen) are passed to the optimizer.\n",
    "        print(f\"  Configuring AdamW optimizer with LR={self.hparams.lr}.\")\n",
    "        optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, self.parameters()),\n",
    "                                       lr=self.hparams.lr)\n",
    "        # --- Optional: Add LR Scheduler ---\n",
    "        # Example: Reduce learning rate when validation loss plateaus\n",
    "        # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
    "        # return {\n",
    "        #     \"optimizer\": optimizer,\n",
    "        #     \"lr_scheduler\": {\n",
    "        #         \"scheduler\": scheduler,\n",
    "        #         \"monitor\": \"val_loss\", # Metric to monitor for scheduler step\n",
    "        #         \"interval\": \"epoch\",   # Check metric at the end of each epoch\n",
    "        #         \"frequency\": 1\n",
    "        #     },\n",
    "        # }\n",
    "        # --- End Optional Scheduler ---\n",
    "        return optimizer # Return only optimizer if no scheduler is used\n",
    "\n",
    "    # --- Training, Validation, and Test Steps ---\n",
    "    # These define the logic executed during the respective phases of training.\n",
    "\n",
    "    def training_step(self, batch: dict, batch_idx: int) -> torch.Tensor:\n",
    "        \"\"\" Performs one step of training. \"\"\"\n",
    "        # Assumes batch is a dictionary with 'image' and 'label' keys. Adapt if different.\n",
    "        images, labels = batch[\"image\"], batch[\"label\"]\n",
    "        # Get model output (logits) by calling the forward method\n",
    "        logits = self(images)\n",
    "        # Calculate the loss\n",
    "        loss = self.criterion(logits, labels)\n",
    "        # Calculate accuracy\n",
    "        acc = (logits.argmax(dim=-1) == labels).float().mean()\n",
    "        # Log metrics using Lightning's self.log method\n",
    "        # `on_epoch=True`: aggregates over the epoch. `prog_bar=True`: shows in progress bar.\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"train_acc\", acc, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss # Return the loss tensor\n",
    "\n",
    "    def validation_step(self, batch: dict, batch_idx: int) -> torch.Tensor:\n",
    "        \"\"\" Performs one step of validation. \"\"\"\n",
    "        images, labels = batch[\"image\"], batch[\"label\"]\n",
    "        logits = self(images)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        acc = (logits.argmax(dim=-1) == labels).float().mean()\n",
    "        # `sync_dist=True`: aggregates metrics correctly across multiple GPUs (if used).\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, sync_dist=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_acc\", acc, on_epoch=True, sync_dist=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch: dict, batch_idx: int) -> torch.Tensor:\n",
    "        \"\"\" Performs one step of testing. \"\"\"\n",
    "        images, labels = batch[\"image\"], batch[\"label\"]\n",
    "        logits = self(images)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        acc = (logits.argmax(dim=-1) == labels).float().mean()\n",
    "        self.log(\"test_loss\", loss, on_epoch=True, logger=True)\n",
    "        self.log(\"test_acc\", acc, on_epoch=True, logger=True)\n",
    "        return loss\n",
    "# --- Utility Function to Load MAE Weights ---\n",
    "    \n",
    "def load_mae_vit_backbone_state_dict(\n",
    "    model: nn.Module,\n",
    "    checkpoint_path: str,\n",
    "    prefix: str = \"vit.\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Loads weights from a MAE pre-trained backbone checkpoint into a standard TIMM ViT model.\n",
    "\n",
    "    Handles the common case where MAE training saves weights with a model-specific\n",
    "    prefix (e.g., \"vit.\") which needs to be removed before loading into a base TIMM model.\n",
    "    It also skips weights that don't belong in the base model (like mask_token or head layers\n",
    "    if the target model has num_classes=0).\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The target TIMM ViT model instance (e.g., created with\n",
    "                           timm.create_model(..., num_classes=0)) into which the\n",
    "                           weights should be loaded. This model is modified in-place.\n",
    "        checkpoint_path (str): Path to the .pth file containing the saved state_dict\n",
    "                               from MAE pre-training (typically model.backbone.state_dict()).\n",
    "        prefix (str, optional): The prefix expected in the keys of the checkpoint's\n",
    "                                state_dict that should be removed. Defaults to \"vit.\".\n",
    "    \"\"\"\n",
    "    # print(f\"\\n--- Loading MAE Backbone Weights ---\")\n",
    "    # print(f\"Target Model Type: {type(model).__name__}\")\n",
    "    # print(f\"Loading from Checkpoint: {checkpoint_path}\")\n",
    "    # print(f\"Expecting prefix in checkpoint keys: '{prefix}'\")\n",
    "\n",
    "    # --- Load the state dictionary from the file ---\n",
    "    try:\n",
    "        # Load to CPU first for flexibility and to avoid GPU memory issues during processing\n",
    "        checkpoint_state_dict = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "        # print(f\"  Successfully loaded state dictionary from file.\")\n",
    "    except FileNotFoundError:\n",
    "        # print(f\"  ERROR: Checkpoint file not found at '{checkpoint_path}'. Cannot load weights.\")\n",
    "        # Depending on requirements, you might want to raise the error instead of just printing\n",
    "        # raise FileNotFoundError(f\"Checkpoint file not found: {checkpoint_path}\")\n",
    "        return # Exit the function if file not found\n",
    "    except Exception as e:\n",
    "        # print(f\"  ERROR: Failed to load checkpoint file '{checkpoint_path}': {e}\")\n",
    "        # raise e\n",
    "        return # Exit on other loading errors\n",
    "\n",
    "    # --- Process the loaded state dictionary ---\n",
    "    processed_state_dict = collections.OrderedDict() # Use OrderedDict to maintain key order potentially\n",
    "    target_model_keys = set(model.state_dict().keys()) # Get keys expected by the target model\n",
    "\n",
    "    # print(\"  Processing checkpoint keys...\")\n",
    "    keys_processed_count = 0\n",
    "    keys_skipped_prefix = 0\n",
    "    keys_skipped_mismatch = 0\n",
    "\n",
    "    for k, v in checkpoint_state_dict.items():\n",
    "        # Check if the key starts with the expected prefix\n",
    "        if k.startswith(prefix):\n",
    "            # Remove the prefix to get the corresponding key in the target model\n",
    "            new_key = k[len(prefix):]\n",
    "\n",
    "            # IMPORTANT: Check if this new key actually exists in the target model\n",
    "            if new_key in target_model_keys:\n",
    "                processed_state_dict[new_key] = v\n",
    "                # print(f\"    Mapping '{k}' -> '{new_key}'\") # Uncomment for detailed debug\n",
    "                keys_processed_count += 1\n",
    "            else:\n",
    "                # This key (e.g., vit.head.weight) exists in the checkpoint but not\n",
    "                # in the target model (which has num_classes=0). Skip it.\n",
    "                # print(f\"    Skipping key '{k}': Corresponding key '{new_key}' not found in target model.\") # Uncomment for detailed debug\n",
    "                keys_skipped_mismatch += 1\n",
    "        else:\n",
    "            # This key doesn't have the prefix. Skip it as it likely belongs to\n",
    "            # the MAE wrapper (e.g., 'mask_token') or is unexpected.\n",
    "            # print(f\"    Skipping key '{k}': Does not have expected prefix '{prefix}'.\") # Uncomment for detailed debug\n",
    "            keys_skipped_prefix += 1\n",
    "\n",
    "    # print(f\"  Processing complete:\")\n",
    "    # print(f\"    {keys_processed_count} keys mapped and kept.\")\n",
    "    # print(f\"    {keys_skipped_mismatch} prefixed keys skipped (target mismatch, e.g., head layers).\")\n",
    "    # print(f\"    {keys_skipped_prefix} non-prefixed keys skipped (e.g., mask_token).\")\n",
    "\n",
    "    if keys_processed_count == 0:\n",
    "        # print(\"  ERROR: No keys were successfully mapped from the checkpoint to the target model!\")\n",
    "        # print(\"         Please check the `prefix` argument and the checkpoint file contents.\")\n",
    "        return # Exit if no keys were useful\n",
    "\n",
    "    # --- Load the processed state dictionary into the target model ---\n",
    "    print(\"  Attempting to load processed state dictionary into the target model...\")\n",
    "    try:\n",
    "        # Use strict=True because we expect a perfect match after processing\n",
    "        missing_keys, unexpected_keys = model.load_state_dict(processed_state_dict, strict=True)\n",
    "\n",
    "        # These should ideally be empty if processing was correct and strict=True worked\n",
    "        if missing_keys:\n",
    "             print(f\"  ERROR: Missing keys AFTER processing: {missing_keys}\")\n",
    "        if unexpected_keys:\n",
    "             print(f\"  ERROR: Unexpected keys AFTER processing: {unexpected_keys}\")\n",
    "\n",
    "        if not missing_keys and not unexpected_keys:\n",
    "             print(\"  Successfully loaded processed weights into the target model.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR: Failed to load processed state_dict into model: {e}\")\n",
    "        print(f\"         Mapped keys ({len(processed_state_dict)}): {list(processed_state_dict.keys())[:5]}...\")\n",
    "        print(f\"         Target keys ({len(target_model_keys)}): {list(target_model_keys)[:5]}...\")\n",
    "\n",
    "    print(f\"--- Finished Loading MAE Backbone Weights ---\")\n",
    "\n",
    "\n",
    "# --- Prepare for Nested CV Call ---\n",
    "# print(\"\\nSetup complete for Nested CV using MAE ViT backbone.\")\n",
    "# print(f\"-> Ensure input images are transformed to {INPUT_SIZE}x{INPUT_SIZE}.\")\n",
    "# print(f\"-> Ensure classifier MLP head ({type(MLPClassifierHead).__name__}) is defined correctly.\")\n",
    "# print(f\"-> Classifier head expects input dimension {FEATURE_DIM}.\")\n",
    "\n",
    "# Define required data transformations for the downstream task.\n",
    "\n",
    "normalization_params = {\n",
    "        \"subtrahend\": [0.485, 0.456, 0.406],\n",
    "        \"divisor\": [0.229, 0.224, 0.225],\n",
    "    }\n",
    "\n",
    "# **CRITICAL**: Must resize/crop images to INPUT_SIZE (e.g., 224x224).\n",
    "# **CRITICAL**: Must include normalization (e.g., ImageNet stats) if the ViT expects it.\n",
    "\n",
    "print(f\"Defining example downstream transforms for input size {INPUT_SIZE}x{INPUT_SIZE}...\")\n",
    "from classes.CustomTiffFileReader import CustomTiffFileReader\n",
    "from monai.transforms import (\n",
    "    Compose, EnsureTyped, Resized, ScaleIntensityd, NormalizeIntensityd,\n",
    "    CenterSpatialCropd\n",
    ")\n",
    "\n",
    "train_transforms_224 = Compose([\n",
    "            CustomTiffFileReader(keys=[\"image\"]),\n",
    "            EnsureTyped(keys=['image', 'label'], data_type=\"tensor\", dtype=torch.float32),\n",
    "            Resized(keys=\"image\", spatial_size=(224,224), mode='bilinear', size_mode='all'),\n",
    "            # CenterSpatialCropd(keys=\"image\", roi_size=(224,224)),\n",
    "            ScaleIntensityd(keys=\"image\"),  # Scale to [0.0, 1.0]\n",
    "            NormalizeIntensityd(\n",
    "                keys=[\"image\"],\n",
    "                subtrahend=normalization_params[\"subtrahend\"] if not CUSTOM_NORMALIZATION else custom_normalization_stats[\"mean\"],\n",
    "                divisor=normalization_params[\"divisor\"] if not CUSTOM_NORMALIZATION else custom_normalization_stats[\"std\"],\n",
    "                channel_wise=True\n",
    "            ),\n",
    "        ])\n",
    "\n",
    "val_transforms_224 = Compose([\n",
    "            CustomTiffFileReader(keys=[\"image\"]),\n",
    "            EnsureTyped(keys=['image', 'label'], data_type=\"tensor\", dtype=torch.float32),\n",
    "            Resized(keys=\"image\", spatial_size=(224,224), mode='bilinear', size_mode='all'),\n",
    "            # CenterSpatialCropd(keys=\"image\", roi_size=(224,224)),\n",
    "            ScaleIntensityd(keys=\"image\"),  # Scale to [0.0, 1.0]\n",
    "            NormalizeIntensityd(\n",
    "                keys=[\"image\"],\n",
    "                subtrahend=normalization_params[\"subtrahend\"] if not CUSTOM_NORMALIZATION else custom_normalization_stats[\"mean\"],\n",
    "                divisor=normalization_params[\"divisor\"] if not CUSTOM_NORMALIZATION else custom_normalization_stats[\"std\"],\n",
    "                channel_wise=True\n",
    "            ),\n",
    "            ])\n",
    "\n",
    "# --- Model Factory Function ---\n",
    "# This function is designed to be passed to `nested_cv_stratified_by_patient`.\n",
    "# It creates a fresh instance of the `ViTFinetuneModule` for each call.\n",
    "for FREEZE_ENCODER in [True,False]:\n",
    "    # This allows you to toggle freezing the backbone during fine-tuning.\n",
    "    # Set to False for initial fine-tuning, True for subsequent fine-tuning with frozen backbone.\n",
    "    # Note: Freezing the backbone is a common practice to stabilize training initially.\n",
    "    # print(f\"  Using freeze_backbone={FREEZE_ENCODER} in model factory.\")\n",
    "    # Note: This is a common practice to stabilize training initially.\n",
    "    # freeze_backbone = False # Set to True if you want to freeze the backbone during fine-tuning\n",
    "    def model_factory(lr: float) -> pl.LightningModule:\n",
    "        \"\"\"\n",
    "        Factory function that creates and returns an instance of ViTFinetuneModule.\n",
    "        Called by the nested CV loop, potentially with different learning rates.\n",
    "\n",
    "        Args:\n",
    "            lr (float): The learning rate to use for this specific model instance.\n",
    "\n",
    "        Returns:\n",
    "            pl.LightningModule: An initialized ViTFinetuneModule instance.\n",
    "        \"\"\"\n",
    "        print(f\"  model_factory called: Creating ViTFinetuneModule instance with LR={lr}\")\n",
    "        # Instantiate the Lightning module, passing necessary arguments.\n",
    "        # Weight loading happens inside ViTFinetuneModule's __init__.\n",
    "        model = ViTFinetuneModule(\n",
    "            mae_weights_path=MAE_BACKBONE_PATH,\n",
    "            model_name='vit_base_patch32_224',  # Use the same model name as used in MAE pre-training\n",
    "            num_classes=NUM_CLASSES,\n",
    "            freeze_backbone=FREEZE_ENCODER,  # Standard practice: start fine-tuning with backbone frozen\n",
    "            lr=lr                  # Pass the learning rate from the factory argument\n",
    "        )\n",
    "        # No need for copy.deepcopy since a new object is created on each call.\n",
    "        return model\n",
    "\n",
    "    # --- Execute Nested Cross-Validation ---\n",
    "    print(\"\\nStarting nested cross-validation...\")\n",
    "    # Replace placeholder variables (df, cfg, labels_np, etc.) with your actual data/config.\n",
    "    # Make sure these variables are defined before this call.\n",
    "    # Example placeholder values (replace them!\n",
    "    # Ensure all required variables are loaded/defined before this call\n",
    "    if any(x is None for x in [df, cfg, labels_np, pat_labels, unique_pat_ids]):\n",
    "        print(\"WARNING: One or more placeholder variables (df, cfg, etc.) are None.\")\n",
    "        print(\"Skipping nested_cv_stratified_by_patient call.\")\n",
    "    else:\n",
    "        # per_fold_metrics, fold_results = nested_cv_stratified_by_patient(\n",
    "        #     df=df,                           # DataFrame with image paths, labels, patient IDs\n",
    "        #     cfg=cfg,                         # Configuration object\n",
    "        #     model_factory=model_factory,     # Use the factory returning ViTFinetuneModule\n",
    "        #     labels_np=labels_np,             # Image-level labels\n",
    "        #     pat_labels=pat_labels,           # Patient-level labels for outer fold stratification\n",
    "        #     unique_pat_ids=unique_pat_ids,   # Unique patient IDs corresponding to pat_labels\n",
    "        #     train_transforms=train_transforms_224, # Pass the 224x224 transforms\n",
    "        #     val_transforms=val_transforms_224,     # Pass the 224x224 transforms\n",
    "        #     pretrained_weights=None,         # CRITICAL: Set to None as weights are loaded internally\n",
    "        #     class_names=class_names,         # List of class names for evaluation/plotting\n",
    "        #     model_manager=None               # Set to None if not using a separate model manager\n",
    "        # )\n",
    "        \n",
    "        from classes.NestedCVStratifiedByPatient import NestedCVStratifiedByPatient\n",
    "        \n",
    "        cfg.set_spatial_size((224, 224))  # Set the spatial size in the configuration\n",
    "        experiment = NestedCVStratifiedByPatient(\n",
    "            df=df, cfg=cfg, labels_np=labels_np, pat_labels=pat_labels, unique_pat_ids=unique_pat_ids,\n",
    "            pretrained_weights=pretrained_weights, class_names=class_names, model_factory=model_factory, num_folds=6,\n",
    "            compute_custom_normalization=CUSTOM_NORMALIZATION,\n",
    "            train_transforms=train_transforms_224,\n",
    "            val_transforms=val_transforms_224,\n",
    "        )\n",
    "        # cfg.set_freezed_layer_index(None)\n",
    "        hold_out_cv = True\n",
    "        # using_cosine_scheduler = False\n",
    "\n",
    "        per_fold_training_metrics, outer_fold_test_results = experiment.run_experiment()\n",
    "\n",
    "        #loading the best model for the metric selected, it's then used for computing gradcams during logging\n",
    "        best_fold_idx = get_best_fold_idx(outer_fold_test_results, metric=\"test_balanced_acc\")\n",
    "        model_instance_for_logging, _ = experiment._get_model_and_device()\n",
    "        model_instance_for_logging.eval()\n",
    "        try:\n",
    "            model_instance_for_logging.load_state_dict(torch.load(f\"best_model_fold_{best_fold_idx}.pth\"))\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(\"Could not find best_model_fold_{best_fold_idx}.pth\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error loading model weights: {str(e)}\")\n",
    "\n",
    "        # Now, call the logging function:\n",
    "        train_transforms, val_transforms,_ = experiment.get_current_fold_transforms()\n",
    "        # Now, call the logging function:\n",
    "        # or \"moco\" if using MoCo\n",
    "        log_SSL_run_to_mlflow(\n",
    "            environmentFlags=environment_flags,\n",
    "            cfg=cfg,\n",
    "            model=model_factory(0.001),  # Pass a dummy model to log the run\n",
    "            class_names=class_names,\n",
    "            fold_results=outer_fold_test_results,\n",
    "            per_fold_metrics=per_fold_training_metrics,\n",
    "            hold_out_cv=True,\n",
    "            test_transforms=val_transforms,\n",
    "            all_images_paths_np=images_paths_np,\n",
    "            all_labels_np=labels_np,\n",
    "            test_images_paths_np=test_images_paths_np,\n",
    "            test_true_labels_np=test_true_labels_np,\n",
    "            yaml_path=yaml_path,\n",
    "            color_transforms=color_transforms,\n",
    "            model_library=model_library,\n",
    "            encoder_type= \"mae\",\n",
    "            pretrained_backbone_path = MAE_BACKBONE_PATH,\n",
    "            train_transforms=train_transforms,\n",
    "            # val_transforms=val_transforms,\n",
    "            freeze_encoder= FREEZE_ENCODER,\n",
    "            ssl=True,\n",
    "        )\n",
    "        solve_cuda_oom()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86385aef",
   "metadata": {},
   "source": [
    "### LOADING RANDOM INITIALIZED ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010d8d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.reproducibility_functions import set_global_seed\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_global_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c6bc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Torchvision for model instantiation.\n",
      "pretrained_weights?  pretrained? False\n",
      "Building torchvision ResNet18...\n",
      "Using Torchvision for model instantiation.\n",
      "pretrained_weights?  pretrained? False\n",
      "Building torchvision ResNet18...\n",
      "Are the two encoders equally initialized? True\n"
     ]
    }
   ],
   "source": [
    "from utils.reproducibility_functions import models_equal_hash\n",
    "byol_encoder_notPretrained, device = model_manager.setup_model(num_classes=num_classes, pretrained_weights=pretrained_weights)\n",
    "byol_encoder_notPretrained2, device = model_manager.setup_model(num_classes=num_classes, pretrained_weights=pretrained_weights)\n",
    "are_equally_initialized = models_equal_hash(byol_encoder_notPretrained, byol_encoder_notPretrained2)\n",
    "print(f\"Are the two encoders equally initialized? {are_equally_initialized}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
