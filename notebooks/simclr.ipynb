{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c37d7d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# SELF SUPERVISED LEARNING\n",
    "%cd ~/Documents/TESI/TESI/\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from utils.setup_functions import set_environment_flags\n",
    "# Example usage:\n",
    "environment_flags = set_environment_flags()\n",
    "kaggle,gdrive,linux = environment_flags[\"kaggle\"], environment_flags[\"gdrive\"], environment_flags[\"linux\"]\n",
    "from utils.reproducibility_functions import set_global_seed\n",
    "set_global_seed(42)\n",
    "import os\n",
    "import re\n",
    "import tifffile\n",
    "import glob\n",
    "import random\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "print(torch.__version__)\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from configs.ConfigLoader import ConfigLoader\n",
    "from utils.train_functions import (\n",
    "# train_epoch,\n",
    "# val_epoch,\n",
    "# print_model_summary,\n",
    "# plot_cv_results,\n",
    "# train_epoch_mixUp,\n",
    "# print_layers,\n",
    "# oversample_minority,\n",
    "# undersample_majority,\n",
    "# freeze_layers_up_to,\n",
    "# freeze_layers_up_to_progressive_ft,\n",
    "train_epoch_vit,\n",
    "val_epoch_vit,\n",
    "    )\n",
    "\n",
    "from utils.test_functions import evaluate_model\n",
    "from utils.data_visualization_functions import plot_confusion_matrix, show_misclassified_images, plot_roc_curve\n",
    "\n",
    "import utils.transformations_functions as tf\n",
    "# Removed redundant import: from configs.ConfigLoader import ConfigLoader\n",
    "\n",
    "from classes.ModelManager import ModelManager\n",
    "import monai\n",
    "print(monai.__version__)\n",
    "#import tifffile\n",
    "#from monai.networks.nets import DenseNet121\n",
    "# import torch.nn.functional as F\n",
    "# from monai.visualize.class_activation_maps import GradCAMpp,GradCAM  \n",
    "#kaggle = input(\"Are you on Kaggle? Enter 'T' for True or 'F' for False: \")\n",
    "kaggle = False\n",
    "if gdrive:\n",
    "    import os\n",
    "\n",
    "    # Change to the desired directory:\n",
    "    os.chdir('/content/drive/MyDrive/TESI/TESI/notebooks')\n",
    "\n",
    "    # Verify the current directory:\n",
    "    print(os.getcwd())\n",
    "\n",
    "# start mlflow ui\n",
    "from utils.mlflow_functions import *\n",
    "from utils.directory_functions import *\n",
    "\n",
    "tracking_uri = get_tracking_uri(gdrive,kaggle,linux)\n",
    "mlflow.set_tracking_uri(tracking_uri)\n",
    "start_mlflow_ui(tracking_uri) # start mlflow ui\n",
    "from utils.directory_functions import get_data_directory, get_base_directory\n",
    "num_input_channels = int(input(\"Enter the number of input channels (3 or 4): \"))\n",
    "\n",
    "from utils.directory_functions import get_data_and_base_directory\n",
    "data_dir, base_dir = get_data_and_base_directory(environment_flags[\"kaggle\"], environment_flags[\"gdrive\"], environment_flags[\"linux\"], num_input_channels=num_input_channels)\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "CLASS_NAME_SETS = {\n",
    "    \"MSA vs Control\": [\"MSA\", \"control\"],\n",
    "    \"MSA vs PD\": [\"MSA\", \"PD\"],\n",
    "    \"MSA-P vs MSA-C\": [\"MSA-P\", \"MSA-C\"],\n",
    "    \"MSA-P vs PD\": [\"MSA-P\", \"PD\"],\n",
    "    \"PD vs MSA-P vs MSA-C\": [\"PD\", \"MSA-P\", \"MSA-C\"]\n",
    "}\n",
    "\n",
    "dropdown = widgets.Dropdown(\n",
    "    options=list(CLASS_NAME_SETS.keys()),\n",
    "    value=\"MSA vs PD\",\n",
    "    description='Class Set:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "def on_dropdown_change(change):\n",
    "    \"\"\"\n",
    "    Update the class_names variable when the dropdown selection changes.\n",
    "    \"\"\"\n",
    "    global class_names\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        class_names = CLASS_NAME_SETS[change['new']]\n",
    "        print(f\"class_names set to: {class_names}\")\n",
    "\n",
    "\n",
    "class_names = CLASS_NAME_SETS[dropdown.value]\n",
    "\n",
    "dropdown.observe(on_dropdown_change)\n",
    "\n",
    "display(dropdown)\n",
    "## Paths of ALL images into a numpy array without labels used for SSL\n",
    "all_images_folder_path = os.path.join(data_dir, \"ALL\")\n",
    "all_images_paths = glob.glob(os.path.join(all_images_folder_path, \"*.tif\"))\n",
    "all_images_paths = np.array(all_images_paths)\n",
    "print(\"Number of images in ALL folder:\", len(all_images_paths))\n",
    "# Create a dictionary mapping each class to its directory\n",
    "class_dirs = {}\n",
    "three_classes = (len(class_names) == 3)\n",
    "\n",
    "for class_name in class_names:\n",
    "    class_dirs[class_name] = os.path.join(data_dir, class_name)\n",
    "    \n",
    "print(class_dirs)\n",
    "if three_classes:\n",
    "    class2_name, class1_name, class0_name = class_names\n",
    "    class2_dir, class1_dir, class0_dir = class_dirs.values()\n",
    "else:\n",
    "    class1_name, class0_name = class_names\n",
    "    class1_dir, class0_dir = class_dirs.values()\n",
    "\n",
    "print(\"Class directories:\")\n",
    "print(class_dirs)\n",
    "\n",
    "# Dictionaries to store image paths and counts for each class\n",
    "images_paths_dict = {}\n",
    "counts_dict = {}\n",
    "\n",
    "# Loop over classes to process each folder\n",
    "for class_name in class_names:\n",
    "    class_dir = class_dirs[class_name]\n",
    "    image_paths = sorted(glob.glob(os.path.join(class_dir, \"*.tif\")))\n",
    "    \n",
    "    # Check if images were found; otherwise raise an error\n",
    "    if not image_paths:\n",
    "        raise FileNotFoundError(f\"No TIFF image file found in {class_dir}\")\n",
    "    \n",
    "    # Count occurrences of 'gh' and 'vaso' in the filenames (using .lower() for case insensitivity)\n",
    "    gh_count = sum('gh' in os.path.basename(path).lower() for path in image_paths)\n",
    "    vaso_count = sum('vaso' in os.path.basename(path).lower() for path in image_paths)\n",
    "    print(f\"{class_name} images (before filtering): 'gh' count: {gh_count}, 'vaso' count: {vaso_count}\")\n",
    "    \n",
    "    # Filter out images that contain 'vaso' (if needed)\n",
    "    image_paths = [path for path in image_paths if 'vaso' not in os.path.basename(path).lower()]\n",
    "    gh_count_after = sum('gh' in os.path.basename(path).lower() for path in image_paths)\n",
    "    vaso_count_after = sum('vaso' in os.path.basename(path).lower() for path in image_paths)\n",
    "    print(f\"After removing 'vaso', {class_name} images: 'gh' count: {gh_count_after}, 'vaso' count: {vaso_count_after}\")\n",
    "    \n",
    "    # Store the filtered image paths and counts for later use\n",
    "    images_paths_dict[class_name] = image_paths\n",
    "    counts_dict[class_name] = {\"gh_count\": gh_count_after, \"vaso_count\": vaso_count_after}\n",
    "\n",
    "# Visualize the number of 'gh' counts per class in a bar chart\n",
    "plt.figure()\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Number of 'gh' occurrences\")\n",
    "plt.title(\"Number of 'gh' occurrences per class\")\n",
    "bar_heights = [counts_dict[cn][\"gh_count\"] for cn in class_names]\n",
    "bar_colors = ['red', 'blue', 'lightblue']\n",
    "plt.bar(class_names, bar_heights, color=bar_colors)\n",
    "plt.show()\n",
    "\n",
    "# --- Debug: Check image shapes after initial loading ---\n",
    "print(\"\\nChecking image shapes:\")\n",
    "for class_name, image_paths in images_paths_dict.items():\n",
    "    for path in image_paths:\n",
    "        img = tifffile.imread(path)  # Read image as a numpy array\n",
    "        print(f\"{class_name} image: {os.path.basename(path)}  dtype: {img.dtype}, shape: {img.shape}\")\n",
    "\n",
    "# Combine image paths and labels for the three classes; \n",
    "# the label here is simply the index of the class in class_names (0, 1, 2)\n",
    "combined = [] # List to store tuples of (image_path, label)\n",
    "for label, class_name in enumerate(class_names):\n",
    "    for path in images_paths_dict[class_name]:\n",
    "        combined.append((path, label))\n",
    "# print(\"\\nSample of combined image paths and labels:\", combined[:5])\n",
    "# random.shuffle(combined)  # Shuffle the combined list to mix classes\n",
    "\n",
    "# Optionally, determine the minority label for resampling purposes\n",
    "counts = {label: len(images_paths_dict[class_name]) for label, class_name in enumerate(class_names)}\n",
    "minority_label = min(counts.keys(), key=lambda k: counts[k])\n",
    "print(f\"\\nMinority label for resampling purposes: {minority_label}\")\n",
    "\n",
    "# Unzip the combined list back into separate tuples (if needed)\n",
    "images_paths, labels = zip(*combined)\n",
    "print(\"\\nSample of image paths:\", images_paths[:5])\n",
    "print(\"Total images found:\", len(combined))\n",
    "\n",
    "# Optionally, convert to NumPy arrays (helpful for further processing or k-fold splitting)\n",
    "images_paths_np = np.array(images_paths)\n",
    "labels_np = np.array(labels)\n",
    "print(\"\\nSample of image paths (NumPy):\", images_paths_np[:5])\n",
    "print((labels_np))\n",
    "#print(X)\n",
    "# --- Your Split Logic for 50/50 distribution in test set ---\n",
    "\n",
    "print(\"Original dataset size:\", len(images_paths_np))\n",
    "\n",
    "# Find unique labels and their counts in the original dataset\n",
    "unique_labels, counts = np.unique(labels_np, return_counts=True)\n",
    "original_distribution = dict(zip(unique_labels, counts))\n",
    "print(f\"Original label distribution: {original_distribution}\")\n",
    "\n",
    "# Determine the maximum possible size for a balanced test set per class\n",
    "# This is limited by the count of the smallest class\n",
    "if len(unique_labels) > 1:\n",
    "    min_class_count = min(counts)\n",
    "    # We want a balanced test set, so take 'min_class_count' samples from each class\n",
    "    test_samples_per_class = min_class_count\n",
    "    total_balanced_test_size = test_samples_per_class * len(unique_labels)\n",
    "\n",
    "    print(f\"\\nAiming for a balanced test set with {test_samples_per_class} samples per class.\")\n",
    "    print(f\"Total balanced test set size will be: {total_balanced_test_size}\")\n",
    "\n",
    "    test_indices = []\n",
    "    train_indices = []\n",
    "\n",
    "    # Iterate through each class to split\n",
    "    for label in unique_labels:\n",
    "        # Get the indices in the original array that correspond to the current class\n",
    "        # print ( labels_np == label) # returns a boolean array\n",
    "        class_indices = np.where(labels_np == label)[0]  #use the boolean array to get the indices where cond is true \n",
    "        # print(f\"\\nClass {label} indices: {class_indices}\") #retuns the indices of the class in the original array and the boolarray so we use [0] to get the indices\n",
    "\n",
    "        # Randomly select a fixed number of indices for the test set from this class\n",
    "        # Use np.random.choice with replace=False for sampling without replacement\n",
    "        # Set a random_state for reproducibility if needed\n",
    "        rng = np.random.default_rng(42) # Use new random generator recommended over np.random.seed\n",
    "        test_class_indices = rng.choice(\n",
    "            class_indices,\n",
    "            size=test_samples_per_class,\n",
    "            replace=False\n",
    "        )\n",
    "        test_indices.extend(test_class_indices)\n",
    "\n",
    "    # Convert lists of indices to NumPy arrays\n",
    "    test_indices = np.array(test_indices)\n",
    "    train_indices = np.array(train_indices)\n",
    "\n",
    "    # Shuffle the indices to mix up the classes in the final arrays (optional but good practice)\n",
    "    # rng.shuffle(test_indices)\n",
    "    # rng.shuffle(train_indices)\n",
    "\n",
    "    balanced_test_images_paths = images_paths_np[test_indices]\n",
    "    balanced_test_true_labels = labels_np[test_indices]\n",
    "    print(f\"Test set size: {len(balanced_test_images_paths)}\")\n",
    "\n",
    "    # Verify the test set distribution\n",
    "    test_unique_labels, test_counts = np.unique(balanced_test_true_labels, return_counts=True)\n",
    "    test_distribution = dict(zip(test_unique_labels, test_counts))\n",
    "    print(f\"\\nTest set distribution: {test_distribution}\")\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    # Use test_unique_labels and test_counts for the bar plot\n",
    "    labels_for_plot = [class_names[label] if 'class_names' in locals() else f\"Label {label}\" for label in test_unique_labels]\n",
    "    plt.bar(labels_for_plot, test_counts)\n",
    "    plt.xlabel(\"Class\")\n",
    "    plt.ylabel(\"Number of test images\")\n",
    "    plt.title(\"Test Set Label Distribution (Balanced)\")\n",
    "    plt.show()\n",
    "\n",
    "    # Print counts and percentages for the balanced test set\n",
    "    print(\"\\nTest set counts and percentages:\")\n",
    "    for label, count in zip(test_unique_labels, test_counts):\n",
    "         class_name = class_names[label] if 'class_names' in locals() else f\"Label {label}\"\n",
    "         print(f\"{class_name}: {count} images ({count/len(balanced_test_true_labels):.1%} of test set)\")\n",
    "\n",
    "else:\n",
    "    print(\"Cannot perform a balanced split with less than two unique classes.\")\n",
    "# NB the test set must be splitted BEFORE oversampling to avoid data leakage!\n",
    "# -------------------------------------------------------------------------\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#returns numpy arrays containing the paths to images and the labels\n",
    "train_images_paths, test_images_paths, train_true_labels, test_true_labels = train_test_split(\n",
    "    images_paths_np,\n",
    "    labels_np,\n",
    "    test_size= 0.15,\n",
    "    stratify=labels,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "test_images_paths_np = np.array(test_images_paths)\n",
    "test_true_labels_np = np.array(test_true_labels)\n",
    "# print(\"train images paths:\", train_images_paths)\n",
    "# print(\"true test labels:\", test_true_labels)\n",
    "# # For the cross-validation, we'll use train_images_paths and labels_temp\n",
    "train_images_paths_np = np.array(train_images_paths) #contains the images paths\n",
    "train_labels_np = np.array(train_true_labels) #contains the labels\n",
    "print(f\"{train_images_paths_np.shape[0]} training images\")\n",
    "print(f\"{len(test_images_paths)} test images\")\n",
    "#test_images_paths = [os.path.basename(path) for path in test_images_paths]\n",
    "# print(test_images_paths)\n",
    "print(type(train_images_paths))\n",
    "\n",
    "unique_labels, counts = np.unique(test_true_labels_np, return_counts=True)\n",
    "\n",
    "\n",
    "plt.bar([class_names[label] for label in unique_labels], counts)\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Number of test images\")\n",
    "plt.title(\"Test Set Label Distribution\")\n",
    "plt.show()\n",
    "\n",
    "for label, count in zip(unique_labels, counts):\n",
    "    print(f\"Label {label} ({class_names[label]}): {count} images\")\n",
    "\n",
    "for label, count in zip(unique_labels, counts):\n",
    "    print(f\"Label {label} ({class_names[label]}) is: {count/test_true_labels_np.shape[0]}\")\n",
    "def extract_patient_id(image_path):\n",
    "    # Example: parse from the file name\n",
    "    # In real code, you might have a different pattern\n",
    "    match = re.search(r'(\\d{4})', image_path)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        return \"UNKNOWN\"\n",
    "\n",
    "# Build a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"image_path\": images_paths_np,\n",
    "    \"label\": labels_np\n",
    "})\n",
    "\n",
    "df[\"patient_id\"] = df[\"image_path\"].apply(extract_patient_id)\n",
    "\n",
    "display(df)\n",
    "\n",
    "# Ensure everything is string or int\n",
    "df[\"patient_id\"] = df[\"patient_id\"].astype(str)\n",
    "\n",
    "# Now group by patient to get a single label per patient.\n",
    "# If every patient truly has exactly one label, we can just take .first()\n",
    "patient_label_df = df.groupby(\"patient_id\", as_index=False)[\"label\"].first()\n",
    "\n",
    "unique_pat_ids = patient_label_df[\"patient_id\"].values  # need these to stratify for patient\n",
    "print(f\"Unique patient IDs: {unique_pat_ids}\")\n",
    "print(f\"Number of unique patients: {len(unique_pat_ids)}\")\n",
    "pat_labels = patient_label_df[\"label\"].values\n",
    "print(f\"Unique patient labels: {pat_labels}\")\n",
    "\n",
    "patient_label_df\n",
    "yaml_path = f\"/home/zano/Documents/TESI/TESI/configs/{num_input_channels}c/densenet121.yaml\"\n",
    "cfg = ConfigLoader(yaml_path) \n",
    "cfg.set_freezed_layer_index(None)\n",
    "cfg.set_transfer_learning(False)\n",
    "transfer_learning = cfg.get_transfer_learning()\n",
    "pretrained_weights = None# \"imagenet\" if transfer_learning else None # 'microscopynet' or \"imagenet\"\n",
    "model_library = \"torchvision\" # or \"torchvision\" or \"monai\"\n",
    "color_transforms = False\n",
    "train_transforms, val_transforms, test_transforms = tf.get_transforms(cfg,color_transforms=color_transforms)\n",
    "model_manager = ModelManager(cfg, library=model_library)\n",
    "# Verify the number of unique labels in the dataset\n",
    "num_classes = len(np.unique(train_labels_np))\n",
    "print(f\"Number of classes in the dataset: {num_classes}\")\n",
    "\n",
    "# Ensure the model's output matches the number of classes\n",
    "model, device = model_manager.setup_model(num_classes=num_classes, pretrained_weights=pretrained_weights)\n",
    "\n",
    "print(model)\n",
    "print(cfg.get_model_input_channels())\n",
    "\n",
    "if model.__class__.__name__ == \"ViT\":\n",
    "    print(\"Using ViT hence it requires custom training and validation functions\")\n",
    "    train_epoch = train_epoch_vit\n",
    "    val_epoch = val_epoch_vit\n",
    "    \n",
    "        \n",
    "def print_transform_names(compose):\n",
    "    \"\"\"\n",
    "    Print the class names of the transformations inside a Compose object.\n",
    "\n",
    "    Args:\n",
    "        compose: A torchvision.transforms.Compose object.\n",
    "    \"\"\"\n",
    "    for idx, transform in enumerate(compose.transforms):\n",
    "        print(f\"{idx + 1}. {transform.__class__.__name__}\")\n",
    "\n",
    "print_transform_names(train_transforms)\n",
    "from utils.data_visualization_functions import visualize_tiff\n",
    "# Example usage:\n",
    "for i in range(len(balanced_test_images_paths)):\n",
    "    print(f\" Image{i} : {balanced_test_images_paths[i]}\")\n",
    "    visualize_tiff(balanced_test_images_paths[i])\n",
    "from utils.data_visualization_functions import display_images_by_class\n",
    "# from utils.filtering_functions import filter_paths_by_imageIds, filter_paths_by_classIndex\n",
    "\n",
    "# filtered_paths, filtered_labels = filter_paths_by_classIndex(images_paths_np, labels_np, 0)\n",
    "# indexes_to_exclude = [4121, 5358, 5435, 5745, 5717, 5767,5753,5878,5776,5978,6179,6053]\n",
    "# filtered_paths, filtered_labels = filter_paths_by_imageIds(filtered_paths, filtered_labels, indexes_to_exclude)\n",
    "# filtered_paths_x, filtered_labels_x = filtered_paths[1], filtered_labels[1]\n",
    "# visualize_tiff(filtered_paths_x,channel_wise_norm=True)\n",
    "# Display filtered images\n",
    "class_names = [class0_name,class1_name]\n",
    "display_images_by_class(images_paths_np, labels_np, class_names, ncols=5, normalize=True)\n",
    "# test_msa_ids = [7144,7120,7239,7293]\n",
    "# test_dir = os.path.join(data_dir, \"TEST\")\n",
    "# test_imagesx_paths = list(sorted(glob.glob(os.path.join(test_dir, \"*.tif\"))))\n",
    "# print(f\"class 0:{class0_name} class 1:{class1_name}\")\n",
    "\n",
    "# #nice result if i flip the labels dont know why\n",
    "# truex_test_labels = [get_label(path,test_msa_ids,test_class0_ids) for path in test_imagesx_paths]\n",
    "# truex_test_labels = np.array(truex_test_labels)\n",
    "# display_images_by_class(test_imagesx_paths, truex_test_labels, class_names, ncols=5, normalize=True, channel_wise_norm=True)\n",
    "## simCLR\n",
    "With a limited labeled dataset, achieving great generalization scores is difficult. SimCLR is a Self-Supervised Learning (SSL) approach to learn meaningful features directly from unlabeled data by contrasting augmented image pairs, without requiring explicit disease labels initially.\n",
    "\n",
    "The goals are:\n",
    "\n",
    "*   Learn robust, biologically relevant embeddings from your microscopy images.\n",
    "*   Reduce overfitting by leveraging subtle structural patterns in data.\n",
    "\n",
    "The implementation consists of:\n",
    "\n",
    "*   A SimCLR dataset returning augmented pairs.\n",
    "*   Using MONAI's contrastive loss to minimize the similarity between negative samples and maximize the similarity between positive samples (cosine similarity).\n",
    "*   Training a DenseNet121-based SimCLR model, clearly defined with a modular encoder and projection head (the projection head has been shown to increase performance in the original paper).\n",
    "\n",
    "Once the model is trained to minimize the contrastive loss (i.e., push embeddings of different subjects apart and push closer the ones of the same):\n",
    "\n",
    "*   Replace the projection head with a linear classifier.\n",
    "*   Fine-tune this network (pre-trained encoder + linear classifier) with the labeled data.\n",
    "\n",
    "Hopefully, fine-tuning leverages embeddings previously learned by SSL, improving classification performance compared to standard supervised training or pure ImageNet transfer learning since the images on which it's trained are very different.\n",
    "\n",
    "Before starting the fine-tuning, visualizing extracted features (e.g., via t-SNE) is useful to understand if the decoders learned meaningful features (if clusters can be detected).\n",
    "\n",
    "\n",
    "### Problems of SimClr\n",
    "* requires large batches to have good results --> high vram needed\n",
    "*   Sensitive to the choice of data augmentations.\n",
    "*   Contrastive loss can be computationally expensive.\n",
    "*   May not work well if the unlabeled data is very different from the labeled data.\n",
    "*   The quality of learned representations depends heavily on the quality and diversity of the unlabeled data.\n",
    "\n",
    "yaml_path = f\"/home/zano/Documents/TESI/TESI/configs/{num_input_channels}c/resnet18.yaml\"\n",
    "cfg = ConfigLoader(yaml_path) \n",
    "cfg.set_freezed_layer_index(None)\n",
    "cfg.set_transfer_learning(False)\n",
    "transfer_learning = cfg.get_transfer_learning()\n",
    "pretrained_weights = None #\"imagenet\" if transfer_learning else None # 'microscopynet' or \"imagenet\"\n",
    "model_library = \"torchvision\" # or \"torchvision\" or \"monai\"\n",
    "color_transforms = False\n",
    "train_transforms, val_transforms, test_transforms = tf.get_transforms(cfg,color_transforms=color_transforms)\n",
    "model_manager = ModelManager(cfg, library=model_library)\n",
    "# Verify the number of unique labels in the dataset\n",
    "num_classes = len(np.unique(train_labels_np))\n",
    "print(f\"Number of classes in the dataset: {num_classes}\")\n",
    "import time\n",
    "import torchvision.transforms.v2 as T\n",
    "from lightly.data import LightlyDataset\n",
    "from lightly.data.collate import SimCLRCollateFunction\n",
    "import pytorch_lightning as pl\n",
    "from lightly.loss import NTXentLoss\n",
    "import torch.nn as nn\n",
    "from pytorch_lightning import Trainer\n",
    "from lightly.transforms import SimCLRTransform\n",
    "from lightly.models.modules import SimCLRProjectionHead\n",
    "from utils.train_functions import LinearProbeHead \n",
    "# from lightly.models.modules import SimCLRProjectionHead\n",
    "torch.set_float32_matmul_precision('high')  # or 'high' for better performance to enable tensor cores?\n",
    "pl.seed_everything(42)\n",
    "ssl_input_dir = f\"/home/zano/Documents/TESI/{num_input_channels}c_MIP_new/CONTROL\"\n",
    "# encoder = DenseNet121(\n",
    "#     spatial_dims=2,\n",
    "#     in_channels=3,\n",
    "#     out_channels=128,  # final feature dim for dense layers\n",
    "#     pretrained=False\n",
    "# )\n",
    "\n",
    "# # 1) define the encoder\n",
    "# encoder, device = ModelManager(cfg, library=model_library).setup_model(\n",
    "#     num_classes=128,  # not used in SSL\n",
    "#     # pretrained_weights=\"imagenet\",\n",
    "# )\n",
    "# Ensure the model's output matches the number of classes\n",
    "encoder, device = model_manager.setup_model(num_classes=num_classes, pretrained_weights=pretrained_weights)\n",
    "print(encoder)\n",
    "\n",
    "print(f\"encoder class: {encoder.__class__.__name__}\")\n",
    "\n",
    "class SimCLRModule(pl.LightningModule):\n",
    "    def __init__(self, backbone, emb_dim=64, proj_hidden_dim=512, temperature=0.07, lr=3e-4):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.backbone = backbone\n",
    "        self.temperature = temperature\n",
    "        self.lr = lr\n",
    "        \n",
    "        ##STRIP ENCODER LINEAR PROBE HEAD\n",
    "        backbone_outdim = self._get_projection_head_input_dim()\n",
    "        # print(self.backbone)\n",
    "        self._remove_linear_probe_head()\n",
    "        \n",
    "        self.projection_head = SimCLRProjectionHead(\n",
    "            input_dim=backbone_outdim,\n",
    "            hidden_dim=proj_hidden_dim,\n",
    "            output_dim=emb_dim,\n",
    "        )\n",
    "\n",
    "        self.criterion = NTXentLoss(temperature=self.temperature)\n",
    "    \n",
    "    @property\n",
    "    def encoder(self):\n",
    "        return self.backbone\n",
    "        \n",
    "    def _remove_linear_probe_head(self):\n",
    "        # if isinstance(self.backbone.fc, LinearProbeHead):\n",
    "        # print(self.backbone.fc.__class__.__name__)\n",
    "        if hasattr(self.backbone, \"fc\"):\n",
    "            if self.backbone.fc.__class__.__name__ == \"LinearProbeHead\":\n",
    "                self.backbone.fc = nn.Identity()\n",
    "            \n",
    "        elif hasattr(self.backbone, \"classifier\"):\n",
    "            if self.backbone.classifier.__class__.__name__ == \"LinearProbeHead\":\n",
    "                self.backbone.classifier = nn.Identity()\n",
    "        else:\n",
    "            raise RuntimeError(\n",
    "                    \"Could not remove linear probe head\"\n",
    "                )\n",
    "    \n",
    "    def _get_projection_head_input_dim(self):\n",
    "        \"\"\"\n",
    "        Return the dimensionality of the **feature vector that comes right\n",
    "        before the classifier** (the value you must feed into a projection\n",
    "        or linear-probe head).\n",
    "        \n",
    "        \"\"\"\n",
    "        print(f\"backbone: {self.backbone}\")\n",
    "        print(f\"hasattr(self.backbone, 'fc'): {hasattr(self.backbone, 'fc')}\")\n",
    "        print(f\"hasattr(self.backbone, 'classifier'): {hasattr(self.backbone, 'classifier')}\")\n",
    "        print(f\"hasattr(self.backbone, 'in_features'): {hasattr(self.backbone.fc, 'in_features')}\")\n",
    "        \n",
    "        if hasattr(self.backbone, \"fc\") and hasattr(self.backbone.fc, \"in_dim\"):\n",
    "            print(f\"backbone.fc.in_features: {self.backbone.fc.in_dim}\")\n",
    "            if self.backbone.fc.__class__.__name__ == \"LinearProbeHead\":\n",
    "                return self.backbone.fc.in_dim\n",
    "            # return self.backbone.fc.in_features\n",
    "        elif hasattr(self.backbone, \"classifier\"):\n",
    "            if self.backbone.classifier.__class__.__name__ == \"LinearProbeHead\":\n",
    "                return self.backbone.classifier.in_dim\n",
    "            \n",
    "        print(f\" backbone input dim: {self.backbone.in_dim}\")\n",
    "        return self.backbone.in_dim\n",
    "        \n",
    "    def forward(self, x):\n",
    "        feats = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.projection_head(feats)\n",
    "        return z\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        (x0, x1),_,_ = batch #x0,x1 are tensor of shape (B,C,H,W)\n",
    "        z0 = self.forward(x0)\n",
    "        z1 = self.forward(x1)\n",
    "        loss = self.criterion(z0, z1)\n",
    "        self.log(\"train_loss\", loss, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # optim = torch.optim.SGD(self.parameters(), lr=self.lr) \n",
    "        optim = torch.optim.Adam(self.parameters(), lr=self.lr,weight_decay=1e-6)\n",
    "        return optim\n",
    "    \n",
    "from lightly.transforms.simclr_transform import SimCLRTransform\n",
    "\n",
    "STABLE_BACKBONE_PATH = f\"simclr_encoder_weights_{num_input_channels}c_{encoder.__class__.__name__}.pth\"\n",
    "# 3) Create the SimCLR transform\n",
    "transform = SimCLRTransform(\n",
    "    input_size=256,\n",
    "    gaussian_blur=0.5,  # probability of applying blur\n",
    "    random_gray_scale = 0.2,  # probability of grayscale\n",
    "    cj_strength=0.4, # color jitter strength\n",
    "    vf_prob=0.5,\n",
    "    hf_prob=0.5,\n",
    "    rr_prob=0.5,\n",
    "    rr_degrees=180,\n",
    ")\n",
    "\n",
    "\n",
    "dataset = LightlyDataset(\n",
    "    input_dir=ssl_input_dir,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# collate_fn = SimCLRCollateFunction(input_size=256)\n",
    "print(\"len of dataset\")\n",
    "print(len(dataset)) \n",
    "\n",
    "loader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size = len(dataset),\n",
    "    shuffle=True,\n",
    "    # collate_fn=collate_fn,\n",
    "    drop_last=True,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "# 4) Create the module & trainer\n",
    "model = SimCLRModule(\n",
    "    backbone=encoder,\n",
    "    emb_dim=128,\n",
    "    proj_hidden_dim=512,\n",
    "    temperature=0.07,\n",
    "    lr=3e-4\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs= 120,\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    devices=1,\n",
    "    precision=\"16-mixed\",\n",
    "    enable_progress_bar=True,\n",
    "    log_every_n_steps=1\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "# 5) Fit\n",
    "trainer.fit(model=model, train_dataloaders=loader)\n",
    "end_time = time.time()\n",
    "print(f\"Training completed in {end_time - start_time:.2f} seconds.\")\n",
    "# 6) Save weights\n",
    "STABLE_BACKBONE_PATH = f\"simclr_encoder_weights_{num_input_channels}c_{encoder.__class__.__name__}.pth\"\n",
    "\n",
    "torch.save(\n",
    "    model.backbone.state_dict(),\n",
    "    STABLE_BACKBONE_PATH\n",
    ")\n",
    "\n",
    "print(f\"Saved encoder weights to {STABLE_BACKBONE_PATH}\")\n",
    "import copy\n",
    "# 7) Reload the backbone\n",
    "encoder_type = \"simclr\"\n",
    "simclr_encoder = copy.deepcopy(model.backbone) # Extract the encoder part not taking into consideration the projection head\n",
    "# 3) Load your pre-trained SSL weights\n",
    "state_dict = torch.load(STABLE_BACKBONE_PATH, map_location=device)\n",
    "simclr_encoder.load_state_dict(state_dict)\n",
    "simclr_encoder = simclr_encoder.to(device)\n",
    "# 8) Use encoder for downstream tasks\n",
    "# small test to see the encoder output dimension\n",
    "from utils.train_functions import remove_projection_head\n",
    "import copy\n",
    "# 4) (Optional) Quick check of encoder output dim\n",
    "encoder = copy.deepcopy(simclr_encoder).eval()  # Set the encoder to evaluation mode\n",
    "print(cfg.get_image_shape())\n",
    "with torch.no_grad():\n",
    "    test_input = torch.zeros(1, 3, cfg.get_image_shape()[0], cfg.get_image_shape()[1], device=device)\n",
    "    encoder_out = simclr_encoder(test_input).flatten(start_dim=1)\n",
    "    model_without_projection = remove_projection_head(copy.deepcopy(simclr_encoder))  # Set the encoder to evaluation mode\n",
    "    test_input = torch.zeros(1, 3, cfg.get_image_shape()[0], cfg.get_image_shape()[1], device=device)\n",
    "    feats = model_without_projection(test_input).flatten(start_dim=1)\n",
    "print(f\"Detected encoder output dimension: {encoder_out.shape[1]}\")\n",
    "print(f\"Detected encoder output dimension once removed classification head: {feats.shape[1]}\")\n",
    "\n",
    "print(simclr_encoder)\n",
    "# encoder.train()  # Set the encoder back to training mode\n",
    "# load the model weights\n",
    "#LOAD WITH RANDOMLY INITIALIZED BACKBONE TO TEST IF SSL IS WORKING\n",
    "simclr_encoder, device = model_manager.setup_model(num_classes=num_classes, pretrained_weights=pretrained_weights)\n",
    "encoder_type = \"NOSSL\"\n",
    "\n",
    "def _remove_linear_probe_head(backbone):\n",
    "        # if isinstance(self.backbone.fc, LinearProbeHead):\n",
    "        # print(self.backbone.fc.__class__.__name__)\n",
    "        if hasattr(backbone, \"fc\"):\n",
    "            if  backbone.fc.__class__.__name__ == \"LinearProbeHead\":\n",
    "                print(\"removing linear probe head\")\n",
    "                backbone.fc = nn.Identity()\n",
    "            \n",
    "        elif hasattr(backbone, \"classifier\"):\n",
    "            if backbone.classifier.__class__.__name__ == \"LinearProbeHead\":\n",
    "                backbone.classifier = nn.Identity()\n",
    "        else:\n",
    "            raise RuntimeError(\n",
    "                    \"Could not remove linear probe head\"\n",
    "                )\n",
    "_remove_linear_probe_head(simclr_encoder)\n",
    "print(simclr_encoder)\n",
    "### TRAINING LOOP\n",
    "import copy\n",
    "from utils.data_visualization_functions import generate_cv_results_figure\n",
    "from utils.train_functions import  SSLClassifierModule\n",
    "\n",
    "FREEZE_ENCODER = True  \n",
    "bool_list = [True, False] \n",
    "for FREEZE_ENCODER in bool_list:\n",
    "    def model_factory(lr: float)-> torch.nn.Module:\n",
    "        \"\"\"\n",
    "        Model‚Äêfactory that instantiates a fresh SSLClassifierModule\n",
    "        with the given learning rate.\n",
    "        \"\"\"\n",
    "        fresh_encoder = copy.deepcopy(simclr_encoder)\n",
    "        return SSLClassifierModule(\n",
    "            encoder=fresh_encoder,     # capture this from your outer scope\n",
    "            num_classes=2,\n",
    "            freeze_encoder=FREEZE_ENCODER,\n",
    "            lr=lr,\n",
    "            backbone_output_dim=feats.shape[1],\n",
    "            # input_shape=(1,3,256,256),  # Adjust as needed (B, C, H, W)\n",
    "        )\n",
    "        \n",
    "\n",
    "    from classes.NestedCVStratifiedByPatient import NestedCVStratifiedByPatient\n",
    "    # cfg.set_freezed_layer_index(None)\n",
    "    experiment = NestedCVStratifiedByPatient(\n",
    "        df=df, cfg=cfg, labels_np=labels_np, pat_labels=pat_labels, unique_pat_ids=unique_pat_ids,\n",
    "        pretrained_weights = pretrained_weights,\n",
    "        class_names = class_names, model_factory=model_factory, num_folds=6\n",
    "    )\n",
    "    # cfg.set_freezed_layer_index(None)\n",
    "    # hold_out_cv = True\n",
    "    using_cosine_scheduler = False\n",
    "\n",
    "    per_fold_training_metrics, outer_fold_test_results = experiment.run_experiment()\n",
    "\n",
    "\n",
    "    from utils.mlflow_functions import log_SSL_run_to_mlflow\n",
    "    def get_best_fold_idx(outer_fold_test_results, metric=\"test_balanced_acc\"):\n",
    "        \"\"\"\n",
    "        Get the index of the best fold based on a specified metric.\n",
    "\n",
    "        Args:\n",
    "            outer_fold_test_results (list of dict): List containing test results for \n",
    "                each outer fold. Each element should be a dictionary with metrics \n",
    "                as keys.\n",
    "            metric (str): The metric name to use for selecting the best fold. \n",
    "                Default is \"test_balanced_acc\".\n",
    "\n",
    "        Returns:\n",
    "            int: The index of the fold with the highest value for the specified metric.\n",
    "\n",
    "        Example:\n",
    "            best_idx = get_best_fold_idx(results, metric=\"test_f1\")\n",
    "        \"\"\"\n",
    "\n",
    "        print(outer_fold_test_results)\n",
    "        best_fold_idx = np.argmax([r[metric] for r in outer_fold_test_results])\n",
    "        # print(f\"Best Balanced Accuracy Fold Index: {best_bac_fold_idx}\")\n",
    "        best_fold_result = outer_fold_test_results[best_fold_idx]\n",
    "        print(f\"Best {metric} Fold Result: {best_fold_result}\")\n",
    "        fold_idx = best_fold_result[\"fold\"]\n",
    "        return fold_idx\n",
    "\n",
    "    #loading the best model for the metric selected, it's then used for computing gradcams during logging\n",
    "    best_fold_idx = get_best_fold_idx(outer_fold_test_results, metric=\"test_balanced_acc\")\n",
    "    model_instance_for_logging, _ = experiment._get_model_and_device()\n",
    "    model_instance_for_logging.eval()\n",
    "    try:\n",
    "        model_instance_for_logging.load_state_dict(torch.load(f\"best_model_fold_{best_fold_idx}.pth\"))\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(\"Could not find best_model_fold_{best_fold_idx}.pth\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error loading model weights: {str(e)}\")\n",
    "\n",
    "    # Now, call the logging function:\n",
    "    train_transforms, val_transforms,_ = experiment.get_current_fold_transforms()\n",
    "    # Now, call the logging function:\n",
    "      # or \"moco\" if using MoCo\n",
    "    log_SSL_run_to_mlflow(\n",
    "        environmentFlags=environment_flags,\n",
    "        cfg=cfg,\n",
    "        model=model_factory(0.001),  # Pass a dummy model to log the run\n",
    "        class_names=class_names,\n",
    "        fold_results=outer_fold_test_results,\n",
    "        per_fold_metrics=per_fold_training_metrics,\n",
    "        hold_out_cv=True,\n",
    "        test_transforms=val_transforms,\n",
    "        all_images_paths_np=images_paths_np,\n",
    "        all_labels_np=labels_np,\n",
    "        test_images_paths_np=test_images_paths_np,\n",
    "        test_true_labels_np=test_true_labels_np,\n",
    "        yaml_path=yaml_path,\n",
    "        color_transforms=color_transforms,\n",
    "        model_library=model_library,\n",
    "        encoder_type= encoder_type,\n",
    "        pretrained_backbone_path = STABLE_BACKBONE_PATH,\n",
    "        train_transforms=train_transforms,\n",
    "        # val_transforms=val_transforms,\n",
    "        freeze_encoder=FREEZE_ENCODER,\n",
    "        ssl=True,\n",
    "    )\n",
    "fold_6_val_image_paths = experiment.get_val_image_paths_per_fold(6)\n",
    "print(fold_6_val_image_paths)\n",
    "\n",
    "from utils.data_visualization_functions import visualize_tiff\n",
    "# Example usage:\n",
    "for i in range(len(fold_6_val_image_paths)):\n",
    "    print(f\" Image{i} : {fold_6_val_image_paths[i]}\")\n",
    "    visualize_tiff(fold_6_val_image_paths[i])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
