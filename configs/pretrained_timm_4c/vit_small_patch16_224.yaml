_base_: base.yaml

model:
  model_name: "vit_small_patch16_224"
  library: "timm"
  in_channels: 4  # 4-channel input
  out_channels: 2
  pretrained_weights: "imagenet"
  
  # Channel adaptation for 4-channel pretrained
  channel_adaptation_strategy: "repeat_avg"  # Options: "repeat_avg", "random_init", "zero_init"
  
  # Optional timm-specific parameters
  global_pool: "avg"
  drop_rate: 0.0
  drop_path_rate: 0.0

data_augmentation:
  resize_spatial_size: [224, 224]  # Match ViT input size
  crop_size: [224, 224]

data_loading:
  # batch_size: 32  # Smaller model, can use larger batch

training:
  num_epochs: 200
  transfer_learning: true
  # early_stopping_patience: 30
  oversample: true
  undersample: false
  weighted_loss: false
  mixup_alpha: 0
  # Transfer learning freeze points for ViT-Small (152 layers, 12 blocks):
  #   -1  = No freezing (full fine-tuning)
  #   39  = Freeze first 3 blocks (25%)
  #   75  = Freeze first 6 blocks (50%) ‚Üê RECOMMENDED START
  #   111 = Freeze first 9 blocks (75%)
  #   149 = Linear probe (freeze all except head)
  freezed_layerIndex: 75  # Freeze first 6 blocks, fine-tune last 6

optimizer:
  learning_rate: 5e-5  # Lower LR for fine-tuning pretrained model
  optimizer_name: "AdamW"  # AdamW works well with ViT
  weight_decay: 1e-4  # Higher weight decay for ViT

scheduler:
  scheduler_name: "ReduceLROnPlateau"
  factor: 0.5
  # scheduler_patience: 25
  threshold: 1e-4
  min_lr: 1e-8

